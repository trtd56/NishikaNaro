# -*- coding: utf-8 -*-
"""なろう_特徴量.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X32v8pmLuEm2HMaXRGaVVVEUjSR1gR71

# 小説家になろう ブクマ数予測 \~”伸びる”タイトルとは？\~

## Memo
- 文章ベクトル
- kwごとのtarget

### done
- PCA
- 単純なdate target
- pseudoでtestのtargetを修正
- userごとの修正
- dateのshift
- kw is none
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers unidic-lite fugashi ipadic python-Levenshtein sentencepiece
!pip install -U torch

"""## 共通設定"""

import gc
import os
import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from datetime import datetime as dt
from tqdm.notebook import tqdm
from collections import defaultdict
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
import itertools
import Levenshtein

from gensim.corpora.dictionary import Dictionary
from gensim.models import LdaModel, TfidfModel, CoherenceModel
from collections import defaultdict

# Models
import lightgbm as lgb
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA

import re

# Transformer
import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader

from transformers import get_cosine_schedule_with_warmup
from transformers import AutoConfig
from transformers import AutoTokenizer
from transformers import AutoModel
from transformers import AdamW

from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

device = torch.device("cuda")
scaler = torch.cuda.amp.GradScaler()

class GCF:
    INPUT_PATH = "/content/drive/MyDrive/Study/Nishika"
    FEATURES_PATH = f"{INPUT_PATH}/features"
    RESULT_PATH = f"{INPUT_PATH}/result"

    N_FOLDS = 5
    SEED = 0

    FEATURES = [
        "pc_or_k",
        "org_bin",
        "genre_ohe", "biggenre_ohe",
        "kw_ohe_50_norm",
        "kw_lda_50_norm",
        "yyyymm",
        "date2",
        #"userid_over5",
        "userid_only_test",
        "url_count_3",
        "kikaku_v1",
        "autopost",
        "cumsum_v1",
        "user_target_v1",
        #"user_target_v2_leak_fix",
        #"genre_target_v1",
        "genre_target_v2",
        #"lda_thr00",
        "bin_target",
        "date_target_v1",
        #"date_diff_v1",
        "kw_none",
        #"user_target_pseudo_v1",
        #"user_window_pseudo_v1",
    ]

train_df = pd.read_csv(f"{GCF.INPUT_PATH}/train.csv")
test_df = pd.read_csv(f"{GCF.INPUT_PATH}/test.csv")
sub_df = pd.read_csv(f"{GCF.INPUT_PATH}/sample_submission.csv")

test_df['fav_novel_cnt_bin'] = -1

"""## 特徴抽出

### PCか携帯か
"""

pc_or_k_dic = {0:[0, 0], 1:[1, 0], 2:[0, 1], 3:[1, 1]}
pc_or_k_train = pd.DataFrame(train_df['pc_or_k'].map(pc_or_k_dic).tolist(), columns=['pc', 'keitai'])
pc_or_k_test = pd.DataFrame(test_df['pc_or_k'].map(pc_or_k_dic).tolist(), columns=['pc', 'keitai'])

pc_or_k_train.to_feather(f"{GCF.FEATURES_PATH}/train_pc_or_k.ftr")
pc_or_k_test.to_feather(f"{GCF.FEATURES_PATH}/test_pc_or_k.ftr")

del pc_or_k_test, pc_or_k_train, pc_or_k_dic
gc.collect()

"""### オリジナル2値データ

- 元データそのままの2値データ
- endとisstopはすべての値が0なので無視する
"""

org_bin_feat = ['isr15', 'isbl', 'isgl', 'iszankoku', 'istensei', 'istenni']
org_bin_train = train_df[org_bin_feat]
org_bin_test = test_df[org_bin_feat]

org_bin_train = pd.concat([org_bin_train, train_df[['novel_type']] - 1], axis=1)
org_bin_test = pd.concat([org_bin_test, test_df[['novel_type']] - 1], axis=1)

org_bin_train.to_feather(f"{GCF.FEATURES_PATH}/train_org_bin.ftr")
org_bin_test.to_feather(f"{GCF.FEATURES_PATH}/test_org_bin.ftr")

del org_bin_train, org_bin_test, org_bin_feat
gc.collect()

"""### ジャンル

#### 単純なOne-Hot Encoding

##### ジャンル
"""

oe = OneHotEncoder(sparse=False, dtype = int)
genre_train = oe.fit_transform(train_df[['genre']])
genre_test = oe.transform(test_df[['genre']])

col = [f"genre_{i}" for i in oe.categories_[0]]

genre_train = pd.DataFrame(genre_train, columns=col)
genre_test = pd.DataFrame(genre_test, columns=col)

genre_train.to_feather(f"{GCF.FEATURES_PATH}/train_genre_ohe.ftr")
genre_test.to_feather(f"{GCF.FEATURES_PATH}/test_genre_ohe.ftr")

del genre_train, genre_test, col, oe
gc.collect()

"""##### 大ジャンル"""

oe = OneHotEncoder(sparse=False, dtype = int)
biggenre_train = oe.fit_transform(train_df[['biggenre']])
biggenre_test = oe.transform(test_df[['biggenre']])

col = [f"biggenre_{i}" for i in oe.categories_[0]]

biggenre_train = pd.DataFrame(biggenre_train, columns=col)
biggenre_test = pd.DataFrame(biggenre_test, columns=col)

biggenre_train.to_feather(f"{GCF.FEATURES_PATH}/train_biggenre_ohe.ftr")
biggenre_test.to_feather(f"{GCF.FEATURES_PATH}/test_biggenre_ohe.ftr")

del biggenre_train, biggenre_test, col, oe
gc.collect()

"""#### カテゴリカルencoding"""

genre_dic = {v: i for i, v in enumerate(train_df['genre'].unique())}

train_genre = train_df['genre'].map(genre_dic)
test_genre = test_df['genre'].map(genre_dic)

biggenre_dic = {v: i for i, v in enumerate(train_df['biggenre'].unique())}

train_biggenre = train_df['biggenre'].map(biggenre_dic)
test_biggenre = test_df['biggenre'].map(biggenre_dic)

train_genre_cate = pd.DataFrame(zip(train_genre.values, train_biggenre.values), columns=['genre_cate', 'biggenre_cate'])
test_genre_cate = pd.DataFrame(zip(test_genre.values, test_biggenre.values), columns=['genre_cate', 'biggenre_cate'])

train_genre_cate.to_feather(f"{GCF.FEATURES_PATH}/train_genre_cate.ftr")
test_genre_cate.to_feather(f"{GCF.FEATURES_PATH}/test_genre_cate.ftr")

del genre_dic, biggenre_dic, train_genre, test_genre, train_biggenre, test_biggenre, train_genre_cate, test_genre_cate
gc.collect()

"""### Keyword
- 出現上位100件のonehot
- 出現上位50件のonehot
"""

corpus_keyword = train_df['keyword'].tolist()

d = defaultdict(int)
for words in corpus_keyword:
    if type(words) is float:
        continue
    for w in words.split():
        d[w] += 1

#available_kw = [k for k, v in d.items() if v >= 100]
available_kw = [k for k, v in d.items() if v >= 50]
available_kw_dic = {v: np.eye(len(available_kw))[i] for i, v in enumerate(available_kw)}

def onehot_keywords(kw, dic):
    vec = np.zeros(len(dic))
    if type(kw) is float:
        return vec
    for w in kw.split():
        try:
            ohe = dic[w]
        except KeyError:
            continue
        vec += ohe
    return vec.tolist()

keyword_vecs = train_df['keyword'].map(lambda x: onehot_keywords(x, available_kw_dic)).tolist()
keyword_ohe_train = pd.DataFrame(keyword_vecs, columns=available_kw)
del keyword_vecs
keyword_vecs = test_df['keyword'].map(lambda x: onehot_keywords(x, available_kw_dic)).tolist()
keyword_ohe_test = pd.DataFrame(keyword_vecs, columns=available_kw)

#keyword_ohe_train.to_feather(f"{GCF.FEATURES_PATH}/train_kw_ohe_100.ftr")
#keyword_ohe_test.to_feather(f"{GCF.FEATURES_PATH}/test_kw_ohe_100.ftr")
keyword_ohe_train.to_feather(f"{GCF.FEATURES_PATH}/train_kw_ohe_50.ftr")
keyword_ohe_test.to_feather(f"{GCF.FEATURES_PATH}/test_kw_ohe_50.ftr")

"""#### is Nan"""

pd.DataFrame(train_df['keyword'].isnull().astype(int).values, columns=['kw_is_none']).to_feather(f"{GCF.FEATURES_PATH}/train_kw_none.ftr")
pd.DataFrame(test_df['keyword'].isnull().astype(int).values, columns=['kw_is_none']).to_feather(f"{GCF.FEATURES_PATH}/test_kw_none.ftr")

"""#### 企画っぽいやつ"""

kikaku_1 = [
            "ゲラゲラコンテスト３",
            "新人発掘コンテスト",
            "がうがうコン1",
            "夏の夜の恋物語企画",
            '架空戦記創作大会'
]
kikaku_2 = [
            'HJ2021',
            '夏のホラー2021',
            '冬童話2021',          
]
kikaku_3 = [
            'ネット小説大賞九感想',
            'ネット小説大賞七感想',
            'ネット小説大賞八感想',
            'ネット小説大賞九',
]
kikaku_4 = [
            'OVL大賞7M',
            'OVL大賞7',
            'OVL大賞7F',
]
kikaku_5 = [
            'ESN大賞２',
            'ESN大賞',
            'ESN大賞３',
]
kikaku_6 = [            
            'キネノベ大賞２',
            'キネノベ大賞３',
]
kikaku_7 = [               
            '集英社小説大賞2',
            '集英社WEB小説大賞',
            '集英社小説大賞２',
]
kikaku_8 = [
            '123大賞',
            'MBSラジオ短編賞1',
            'マンガＵＰ！賞１',
            'ネット小説大賞八',
            'なろうラジオ大賞2'
]
kikaku_cols = kikaku_1 + kikaku_2 + kikaku_3 + kikaku_4 + kikaku_5 + kikaku_6 + kikaku_7 + kikaku_8

kikaku_train = pd.concat([
           keyword_ohe_train[kikaku_1].max(1),
           keyword_ohe_train[kikaku_2].max(1),
           keyword_ohe_train[kikaku_3].max(1),
           keyword_ohe_train[kikaku_4].max(1),
           keyword_ohe_train[kikaku_5].max(1),
           keyword_ohe_train[kikaku_6].max(1),
           keyword_ohe_train[kikaku_7].max(1),
           keyword_ohe_train[kikaku_8].max(1),
           keyword_ohe_train[kikaku_cols].max(1),
], axis=1)
kikaku_train = (kikaku_train > 0).astype(int)
kikaku_train.columns = [f"kikaku_{i}"for i in range(8)] + ["kikaku_all"]

kikaku_test = pd.concat([
           keyword_ohe_test[kikaku_1].max(1),
           keyword_ohe_test[kikaku_2].max(1),
           keyword_ohe_test[kikaku_3].max(1),
           keyword_ohe_test[kikaku_4].max(1),
           keyword_ohe_test[kikaku_5].max(1),
           keyword_ohe_test[kikaku_6].max(1),
           keyword_ohe_test[kikaku_7].max(1),
           keyword_ohe_test[kikaku_8].max(1),
           keyword_ohe_test[kikaku_cols].max(1),
], axis=1)
kikaku_test = (kikaku_test > 0).astype(int)
kikaku_test.columns = [f"kikaku_{i}"for i in range(8)] + ["kikaku_all"]

kikaku_train.to_feather(f"{GCF.FEATURES_PATH}/train_kikaku_v1.ftr")
kikaku_test.to_feather(f"{GCF.FEATURES_PATH}/test_kikaku_v1.ftr")

del kikaku_cols, kikaku_1, kikaku_2, kikaku_3, kikaku_4, kikaku_5, kikaku_6, kikaku_7, kikaku_8
gc.collect()

"""#### 編集距離で絞り込み"""

'''
for i in keyword_ohe_train.columns:
    if len(i) <= 2:
        continue
    for j in keyword_ohe_train.columns:
        if i == j:
            continue
        if len(j) <= 2:
            continue
        d = Levenshtein.distance(i, j)
        if d < 2:
            print(i, j, d)
'''
norm_word_dic = {
    'コメディー': ['コメディ'],
    'ミステリー': ['ミステリ'],
    '新選組': ['新撰組'],
    '片思い': ['片想い'],
    'ざまあ': ['ざまぁ'],
}
norm_genre_dic = {
    '超能力': ['異能力'],
    '宇宙船': ['宇宙人'],
    #'異世界転移': ['異世界転生'],
}

for k, v in norm_word_dic.items():
    keyword_ohe_train[k] = keyword_ohe_train[k] + keyword_ohe_train[v].sum(1)
    keyword_ohe_train = keyword_ohe_train.drop(v, axis=1)

for k, v in norm_word_dic.items():
    keyword_ohe_test[k] = keyword_ohe_test[k] + keyword_ohe_test[v].sum(1)
    keyword_ohe_test = keyword_ohe_test.drop(v, axis=1)

keyword_ohe_train = (keyword_ohe_train > 0).astype(int)
keyword_ohe_test = (keyword_ohe_test > 0).astype(int)

keyword_ohe_train.to_feather(f"{GCF.FEATURES_PATH}/train_kw_ohe_50_norm.ftr")
keyword_ohe_test.to_feather(f"{GCF.FEATURES_PATH}/test_kw_ohe_50_norm.ftr")

del keyword_ohe_train, keyword_ohe_test, keyword_vecs, available_kw, available_kw_dic, d, w, corpus_keyword
gc.collect()

"""#### LDA"""

keyword_ohe_train = pd.read_feather(f"{GCF.FEATURES_PATH}/train_kw_ohe_50_norm.ftr")
keyword_ohe_test = pd.read_feather(f"{GCF.FEATURES_PATH}/test_kw_ohe_50_norm.ftr")

all_keywords = []
for _, row in keyword_ohe_train.iterrows():
    kws = row[row > 0].index.tolist()
    all_keywords.append(kws)
for _, row in keyword_ohe_test.iterrows():
    kws = row[row > 0].index.tolist()
    all_keywords.append(kws)

dictionary = Dictionary(all_keywords)
corpus = [dictionary.doc2bow(text) for text in all_keywords]
tfidf = TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]

coherence_vals = []
perplexity_vals = []
topic_lst = [10, 20, 30, 40, 50]
for n_topic in topic_lst:
    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=n_topic, random_state=0)
    perplexity_vals.append(np.exp2(-lda_model.log_perplexity(corpus)))
    coherence_model_lda = CoherenceModel(model=lda_model, texts=all_keywords, dictionary=dictionary, coherence='c_v')
    coherence_vals.append(coherence_model_lda.get_coherence())

# Perplexity は低ければ低い程，Coherence は高ければ高い程、良い
pd.DataFrame({
    'n_topic': topic_lst,
    'coherence': coherence_vals,
    'perplexity': perplexity_vals,
}).set_index('n_topic')#.plot()

NUM_TOPICS = 20
lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=NUM_TOPICS, random_state=0)

lda_model.show_topics()

def _lda_vectorizer(d):
    vec = np.zeros(NUM_TOPICS)
    for i, v in lda_model.get_document_topics(d):
        vec[i] = v
    return vec
    
vecs = []
for c in tqdm(corpus):
    v = _lda_vectorizer(c)
    vecs.append(v)

topic_vecs = pd.DataFrame(np.stack(vecs), columns=[f"lda_topic_{i:02}" for i in range(NUM_TOPICS)])
topic_vecs.head()

topic_vecs.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_kw_lda_50_norm.ftr")
topic_vecs.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_kw_lda_50_norm.ftr")

del keyword_ohe_train, keyword_ohe_test, dictionary, corpus, lda, score_by_topic, topic_vecs, vecs, lda_model
gc.collect()

"""### アップロード日付

#### 年・月
"""

year_train = train_df['general_firstup'].map(lambda x: x.split('-')[0]).astype(int)
month_train = train_df['general_firstup'].map(lambda x: x.split('-')[1]).astype(int)
year_test = test_df['general_firstup'].map(lambda x: x.split('-')[0]).astype(int)
month_test = test_df['general_firstup'].map(lambda x: x.split('-')[1]).astype(int)

date_train = pd.DataFrame(zip(year_train.tolist(), month_train.tolist()), columns=['upload_year', 'upload_month'])
date_test = pd.DataFrame(zip(year_test.tolist(), month_test.tolist()), columns=['upload_year', 'upload_month'])

date_train.to_feather(f"{GCF.FEATURES_PATH}/train_yyyymm.ftr")
date_test.to_feather(f"{GCF.FEATURES_PATH}/test_yyyymm.ftr")

del date_train, date_test, month_test, year_test, month_train, year_train
gc.collect()

"""#### 時間帯・曜日

"""

hour_train = train_df['general_firstup'].map(lambda x: int(x.split()[1].split(":")[0])).values
hour_test = test_df['general_firstup'].map(lambda x: int(x.split()[1].split(":")[0])).values

day_of_week_train = train_df['general_firstup'].map(lambda x: dt.strptime(x, '%Y-%m-%d %H:%M:%S').isoweekday()).values
day_of_week_test = test_df['general_firstup'].map(lambda x: dt.strptime(x, '%Y-%m-%d %H:%M:%S').isoweekday()).values

date2_train = pd.DataFrame(zip(hour_train, day_of_week_train), columns=['hour', 'day_of_week'])
date2_test = pd.DataFrame(zip(hour_test, day_of_week_test), columns=['hour', 'day_of_week'])

date2_train.to_feather(f"{GCF.FEATURES_PATH}/train_date2.ftr")
date2_test.to_feather(f"{GCF.FEATURES_PATH}/test_date2.ftr")

del date2_train, date2_test, hour_train, hour_test, day_of_week_train, day_of_week_test
gc.collect()

"""#### 自動投稿か"""

set_time_train = train_df['general_firstup'].map(lambda x: x.split()[-1][3:] == '00:00').astype(int)
set_time_test = test_df['general_firstup'].map(lambda x: x.split()[-1][3:] == '00:00').astype(int)

pd.DataFrame(set_time_train.tolist(), columns=['auto_post']).to_feather(f"{GCF.FEATURES_PATH}/train_autopost.ftr")
pd.DataFrame(set_time_test.tolist(), columns=['auto_post']).to_feather(f"{GCF.FEATURES_PATH}/test_autopost.ftr")

del set_time_train, set_time_test
gc.collect()

"""### URL"""

removes = ['http://', 'https://', 'www.', '.co', '.ne', '.jp', '.com']
def get_domain(string): 
    url = re.findall('https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+', string)
    _url = []
    for u in url:
        for s in removes:
            u = u.replace(s, '')
        _url.append(u)
    return _url

train_urls = train_df['story'].map(get_domain)
test_urls = test_df['story'].map(get_domain)

all_urls = train_urls.tolist() + test_urls.tolist()
domein_count = pd.DataFrame(list(itertools.chain.from_iterable(all_urls)))[0].value_counts()

domein_over3 = domein_count[domein_count > 3].index.values

def get_count_url(urls_lst, domein_over3):
    lst = []
    for urls in urls_lst:
        count_url = np.zeros(len(domein_over3))
        for url in urls:
            count_url += (domein_over3 == url).astype(int)
        lst.append(count_url)
    return pd.DataFrame(lst, columns=domein_over3)

train_url_count = get_count_url(train_urls, domein_over3)
test_url_count = get_count_url(test_urls, domein_over3)

max_count = max(test_url_count.max().max(), train_url_count.max().max())
train_url_count = train_url_count / max_count
test_url_count = test_url_count / max_count

train_url_count.to_feather(f"{GCF.FEATURES_PATH}/train_url_count_3.ftr")
test_url_count.to_feather(f"{GCF.FEATURES_PATH}/test_url_count_3.ftr")

del train_url_count, test_url_count, max_count, domein_over3, domein_count, all_urls, train_urls, test_urls
del removes, get_domain, get_count_url
gc.collect()

"""### One-Hot Encoding"""

month_train = train_df['general_firstup'].map(lambda x: np.eye(12)[int(x.split('-')[1])-1]).tolist()
month_test = test_df['general_firstup'].map(lambda x: np.eye(12)[int(x.split('-')[1])-1]).tolist()

hour_train = train_df['general_firstup'].map(lambda x: np.eye(24)[int(x.split()[1].split(":")[0])]).tolist()
hour_test = test_df['general_firstup'].map(lambda x: np.eye(24)[int(x.split()[1].split(":")[0])]).tolist()

day_of_week_train = train_df['general_firstup'].map(lambda x: np.eye(7)[dt.strptime(x, '%Y-%m-%d %H:%M:%S').isoweekday()-1]).tolist()
day_of_week_test = test_df['general_firstup'].map(lambda x: np.eye(7)[dt.strptime(x, '%Y-%m-%d %H:%M:%S').isoweekday()-1]).tolist()

cols = [f"month_ohe_{i:02}" for i in range(12)] + [f"hour_ohe_{i:02}" for i in range(24)] + [f"day_of_week_ohe_{i}" for i in range(7)]

date_ohe_train = pd.DataFrame(np.concatenate([np.stack(month_train), np.stack(hour_train), np.stack(day_of_week_train)], axis=1), columns=cols)
date_ohe_test = pd.DataFrame(np.concatenate([np.stack(month_test), np.stack(hour_test), np.stack(day_of_week_test)], axis=1), columns=cols)

date_ohe_train.to_feather(f"{GCF.FEATURES_PATH}/train_date_ohe.ftr")
date_ohe_test.to_feather(f"{GCF.FEATURES_PATH}/test_date_ohe.ftr")

del month_train, month_test, hour_train, hour_test, day_of_week_train, day_of_week_test, date_ohe_train, date_ohe_test, cols
gc.collect()

"""### UserID"""

# 投稿が多いからと言ってスコアが高いわけではなさそう
#lst = []
#for userid, df in train_df.groupby('userid'):
#    d = len(df), df['fav_novel_cnt_bin'].max(), df['fav_novel_cnt_bin'].min(), df['fav_novel_cnt_bin'].mean(), df['fav_novel_cnt_bin'].median()
#    lst.append(d)
#pd.DataFrame(lst).plot.scatter(x=0, y=4)

def get_userid_idx(_id, dic):
    try:
        return dic[_id]
    except KeyError:
        return len(dic)

all_userid = test_df['userid'].tolist() + train_df['userid'].tolist()

dic = {}
idx = 0
for _id in set(all_userid):
    n = all_userid.count(_id)
    if n < 6:
        continue
    dic[_id] = idx
    idx += 1

userid_train = train_df['userid'].map(lambda x: get_userid_idx(x, dic))
userid_test = test_df['userid'].map(lambda x: get_userid_idx(x, dic))

userid_train = pd.DataFrame(userid_train.tolist(), columns=['userid_over5'])
userid_test = pd.DataFrame(userid_test.tolist(), columns=['userid_over5'])

userid_train.to_feather(f"{GCF.FEATURES_PATH}/train_userid_over5.ftr")
userid_test.to_feather(f"{GCF.FEATURES_PATH}/test_userid_over5.ftr")

del userid_train, userid_test, dic, idx, n, all_userid
gc.collect()

"""#### testのみでfix"""

test_userids = set(test_df['userid'].tolist())

dic = {}
idx = 0
for uid in test_userids:
    _df = train_df.query(f"userid=='{uid}'")
    if len(_df) == 0:
        continue
    dic[uid] = idx
    idx += 1

userid_train = train_df['userid'].map(lambda x: get_userid_idx(x, dic))
userid_test = test_df['userid'].map(lambda x: get_userid_idx(x, dic))

userid_train = pd.DataFrame(userid_train.tolist(), columns=['userid_only_test'])
userid_test = pd.DataFrame(userid_test.tolist(), columns=['userid_only_test'])

userid_train.to_feather(f"{GCF.FEATURES_PATH}/train_userid_only_test.ftr")
userid_test.to_feather(f"{GCF.FEATURES_PATH}/test_userid_only_test.ftr")

len(dic)

"""### userごとの特徴"""

def get_date_diff(df):
    _date = df['general_firstup'].map(lambda x: dt.strptime(x, "%Y-%m-%d %H:%M:%S"))
    dt_diff_prev = _date - _date.shift(1)
    dt_diff_next =  _date.shift(-1) - _date
    dt_diff_prev = dt_diff_prev.map(lambda x: x.days)
    dt_diff_next = dt_diff_next.map(lambda x: x.days)
    dt_diff_init = _date - _date.iloc[0]

    dic = {}
    for i in range(12):
        d = (i+1)*30
        dic[f"prev_over_{d}"] = dt_diff_prev > d
        dic[f"next_over_{d}"] = dt_diff_next > d
    date_diff = pd.DataFrame(dic).astype(int)
    date_diff['init_diff'] = dt_diff_init.map(lambda x: x.days)
    return date_diff

all_df = pd.concat([train_df, test_df]).reset_index(drop=True)
_df = get_date_diff(all_df)

_df.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_date_diff_v1.ftr")
_df.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_date_diff_v1.ftr")

"""#### 累積特徴（cumsum）"""

genre_ohe_train = pd.read_feather(f"{GCF.FEATURES_PATH}/train_genre_ohe.ftr")
genre_ohe_test = pd.read_feather(f"{GCF.FEATURES_PATH}/test_genre_ohe.ftr")
all_df = pd.concat([
    pd.concat([train_df, genre_ohe_train], axis=1),
    pd.concat([test_df, genre_ohe_test], axis=1),
], axis=0).reset_index(drop=True)

org_bin_feat = ['isr15', 'isbl', 'isgl', 'iszankoku', 'istensei', 'istenni']
genre_feat = genre_ohe_train.columns.tolist()
del genre_ohe_train, genre_ohe_test

feats = np.zeros((len(all_df), len(org_bin_feat) + len(genre_feat)))
for userid, df in all_df.groupby('userid'):
    feats[df.index, :] = df[org_bin_feat+genre_feat].cumsum().values
cumsum_feat_df = pd.DataFrame(feats, columns=org_bin_feat+genre_feat)
del feats

post_count = pd.DataFrame(zip([1 for _ in range(len(all_df))], all_df['userid'].tolist())).groupby(1).cumsum(0)
post_count.columns = ['post_count']
cumsum_feat_norm_df = pd.DataFrame(cumsum_feat_df.values / post_count.values, columns=[f"cumsum_{c}_norm"for c in org_bin_feat+genre_feat])
post_count_norm = post_count/post_count.max()
post_count_norm.columns = ['post_count_norm']

cumsum_feat_norm_df = pd.concat([post_count_norm, cumsum_feat_norm_df], axis=1)
del post_count_norm, post_count

cumsum_feat_norm_df.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_cumsum_v1.ftr")
cumsum_feat_norm_df.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_cumsum_v1.ftr")

del all_df, org_bin_feat, genre_feat, cumsum_feat_norm_df, cumsum_feat_df
gc.collect()

"""### text"""

def story_feat(df):
    story_len = df['story'].map(lambda x: len(x))/1000
    story_br = df['story'].map(lambda x: len(x.split('\n')))/100
    _df = pd.concat([story_len, story_br], axis=1)
    _df.columns = ['story_len', 'story_br']
    return _df

story_feat(train_df).to_feather(f"{GCF.FEATURES_PATH}/train_story_feat_v1.ftr")
story_feat(test_df).to_feather(f"{GCF.FEATURES_PATH}/test_story_feat_v1.ftr")

#N4771GY...円周率
#N4326HD...遺言

"""### Target encoding"""

all_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)

date_df = pd.DataFrame(all_df['general_firstup'].map(lambda x: x.split()[0].split('-')[:2]).tolist(), columns=['year', 'month'])
date_df.loc[date_df.query('year=="2021" and month=="09"').index, 'month'] = "08"
date_df['yyyymm'] = date_df.apply(lambda x: f"{x.year}_{x.month}", axis=1)

feat = np.zeros((len(all_df), 4)).astype(str)
for year, df in date_df.groupby('year'):
    m2 = pd.cut(df['month'].astype(int), [0, 2, 4, 6, 8, 10, 12], labels=False)
    m3 = pd.cut(df['month'].astype(int), [0, 3, 6, 9, 12], labels=False)
    m4 = pd.cut(df['month'].astype(int), [0, 4, 8, 12], labels=False)
    m6 = pd.cut(df['month'].astype(int), [0, 6, 12], labels=False)

    m2 = m2.map(lambda x: f"bin_{year}_{x}")
    m3 = m3.map(lambda x: f"bin_{year}_{x}")
    m4 = m4.map(lambda x: f"bin_{year}_{x}")
    m6 = m6.map(lambda x: f"bin_{year}_{x}")
    _df = pd.concat([m2, m3, m4, m6], axis=1)

    feat[df.index, :] = _df.values
bin_df = pd.DataFrame(feat, columns=['bin_2month', 'bin_3month', 'bin_4month', 'bin_6month'])

date_all_df = pd.concat([date_df, bin_df, all_df], axis=1)

"""#### KeyWords"""

keyword_ohe_train = pd.read_feather(f"{GCF.FEATURES_PATH}/train_kw_ohe_50_norm.ftr")
keyword_ohe_test = pd.read_feather(f"{GCF.FEATURES_PATH}/test_kw_ohe_50_norm.ftr")
kw_all = pd.concat([keyword_ohe_train, keyword_ohe_test]).reset_index(drop=True)
kw_all['fav_novel_cnt_bin'] = all_df['fav_novel_cnt_bin']

use_kws = kw_all[date_df['year'].map(lambda x: int(x) >= 2020)]
use_kws = use_kws.query('fav_novel_cnt_bin!=-1')
use_kws = use_kws[use_kws.columns[use_kws.sum() > 200].tolist()]

dfs = []
for c in use_kws.columns:
    if c == 'fav_novel_cnt_bin':
        continue
    df = pd.concat([
        kw_all[c].map(use_kws.groupby(c).mean()['fav_novel_cnt_bin'].to_dict()),
        kw_all[c].map(use_kws.groupby(c).median()['fav_novel_cnt_bin'].to_dict()),
        kw_all[c].map(use_kws.groupby(c).var()['fav_novel_cnt_bin'].to_dict()),
        kw_all[c].map(use_kws.groupby(c).std()['fav_novel_cnt_bin'].to_dict()),
        kw_all[c].map(use_kws.groupby(c).max()['fav_novel_cnt_bin'].to_dict())/5,
        kw_all[c].map(use_kws.groupby(c).min()['fav_novel_cnt_bin'].to_dict())/5,
        kw_all[c].map(use_kws.groupby(c).skew()['fav_novel_cnt_bin'].to_dict()),
    ], axis=1)
    df.columns= [f'cur_{c}_target_{i}' for i in range(df.shape[1])]
    dfs.append(df)

cur_kw_target = pd.concat(dfs, axis=1)

cur_kw_target.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_cur_kw_target_v1.ftr")
cur_kw_target.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_cur_kw_target_v1.ftr")

"""#### date"""

date_group_df = pd.concat([date_df, all_df[['fav_novel_cnt_bin']]], axis=1)

_date_group_df = date_group_df.query('fav_novel_cnt_bin!=-1')

dfs = []
for col in ['year', 'yyyymm', 'bin_2month', 'bin_3month', 'bin_4month', 'bin_6month']:
    dic = _date_group_df.groupby(col).mean()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(date_group_df[col].map(dic).values, columns=[f"{col}_mean"])
    dfs.append(_df)

    dic = _date_group_df.groupby(col).std()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(date_group_df[col].map(dic).values, columns=[f"{col}_std"])
    dfs.append(_df)

    dic = _date_group_df.groupby(col).var()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(date_group_df[col].map(dic).values, columns=[f"{col}_var"])
    dfs.append(_df)

    dic = _date_group_df.groupby(col).skew()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(date_group_df[col].map(dic).values, columns=[f"{col}_skew"])
    dfs.append(_df)

    dic = _date_group_df.groupby(col).median()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(date_group_df[col].map(dic).values, columns=[f"{col}_median"])
    dfs.append(_df)

date_target_df = pd.concat(dfs, axis=1).fillna(0)

date_target_df.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_date_target_v1.ftr")
date_target_df.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_date_target_v1.ftr")



"""#### UserID"""

pseudo = pd.read_csv("/content/drive/MyDrive/Study/Nishika/result/exp099_mlp2.csv")
pseudo_test_df = test_df
pseudo_test_df['fav_novel_cnt_bin'] = pseudo.values[:, 1:].argmax(1)
pseudo_all_df = pd.concat([train_df, pseudo_test_df], axis=0).reset_index(drop=True)

def get_cumsum(target):
    res = []
    lst = []
    for i, v in enumerate(target):
        if v == -1:
            max_v, min_v, avg_v, med_v, std_v, var_v = -1, -1, -1, -1, -1, -1
        else:
            lst.append(v)
            max_v = max(lst) / 5
            min_v = min(lst) / 5
            avg_v = sum(lst)/len(lst)
            med_v = np.median(lst)
            std_v = np.std(lst)
            var_v = np.var(lst)
        res.append({
            'max_v': max_v,
            'min_v': min_v,
            'avg_v': avg_v,
            'med_v': med_v,
            'std_v': std_v,
            'var_v': var_v,
        })
    return pd.DataFrame(res)

target_feat = np.zeros((len(all_df), 9))
#for userid, df in tqdm(all_df.groupby('userid'), total=len(all_df['userid'].unique())):
for userid, df in tqdm(pseudo_all_df.groupby('userid'), total=len(pseudo_all_df['userid'].unique())):
    #_df = pd.concat([df['fav_novel_cnt_bin'].shift(1), df['fav_novel_cnt_bin'].shift(2), df['fav_novel_cnt_bin'].shift(3)], axis=1).fillna(-2)
    _df = pd.concat([df['fav_novel_cnt_bin'].shift(1), df['fav_novel_cnt_bin'].shift(2), df['fav_novel_cnt_bin'].shift(3)], axis=1).fillna(-1)
    #_df = _df.applymap(lambda x: None if x == -1 else x).fillna(method='ffill').applymap(lambda x: -1 if x == -2 else x).fillna(method='ffill')
    _df.columns = ['target_1', 'target_2', 'target_3']
    cumcum_df = get_cumsum(_df['target_1'])
    _feat = pd.concat([_df.reset_index(drop=True), cumcum_df], axis=1)
    target_feat[df.index, :] = _feat.values
user_target_feat_df = pd.DataFrame(target_feat, columns=_feat.columns)

#user_target_feat_df.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_user_target_v1.ftr")
#user_target_feat_df.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_user_target_v1.ftr")
user_target_feat_df.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_user_target_pseudo_v1.ftr")
user_target_feat_df.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_user_target_pseudo_v1.ftr")

del user_target_feat_df, target_feat
gc.collect()

"""#### window"""

def get_window_target(_target, w):
    d = {
        f"w{w}_mean": _target.rolling(w).mean().fillna(-1),
        f"w{w}_max": _target.rolling(w).max().fillna(-1),
        f"w{w}_min": _target.rolling(w).min().fillna(-1),
        f"w{w}_median": _target.rolling(w).median().fillna(-1),
        f"w{w}_var": _target.rolling(w).var().fillna(-1),
        f"w{w}_std": _target.rolling(w).std().fillna(-1),
        f"w{w}_skew": _target.rolling(w).skew().fillna(0),
        f"w{w}_kurt": _target.rolling(w).kurt().fillna(0),
    }
    return pd.DataFrame(d)

target_feat = np.zeros((len(all_df), 40))
#for userid, df in tqdm(all_df.groupby('userid'), total=len(all_df['userid'].unique())):
for userid, df in tqdm(pseudo_all_df.groupby('userid'), total=len(pseudo_all_df['userid'].unique())):
    #_target = df['fav_novel_cnt_bin'].map(lambda x: None if x == -1 else x).fillna(method='ffill')
    _target = df['fav_novel_cnt_bin']
    _target = _target.shift(1)
    window_feat = pd.concat([
           get_window_target(_target, 2),
           get_window_target(_target, 3),
           get_window_target(_target, 4),
           get_window_target(_target, 5),
           get_window_target(_target, 6),
    ], axis=1)
    target_feat[df.index, :] = window_feat.values

user_target_feat_df = pd.DataFrame(target_feat, columns=window_feat.columns)

#user_target_feat_df.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_user_target_v2_leak_fix.ftr")
#user_target_feat_df.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_user_target_v2_leak_fix.ftr")
user_target_feat_df.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_user_window_pseudo_v1.ftr")
user_target_feat_df.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_user_window_pseudo_v1.ftr")

"""#### カテゴリ"""

cate_df = pd.concat([date_df, all_df[['genre', 'fav_novel_cnt_bin']]], axis=1)

cate_df['yyyy_genre'] = cate_df.apply(lambda x: f"{x.year}_{x.genre}",axis=1)
cate_df['yyyymm_genre'] = cate_df.apply(lambda x: f"{x.year}{x.month}_{x.genre}",axis=1)
cate_df['bin_2month_genre'] = cate_df.apply(lambda x: f"{x.bin_2month}_{x.genre}",axis=1)
cate_df['bin_3month_genre'] = cate_df.apply(lambda x: f"{x.bin_3month}_{x.genre}",axis=1)
cate_df['bin_4month_genre'] = cate_df.apply(lambda x: f"{x.bin_4month}_{x.genre}",axis=1)
cate_df['bin_6month_genre'] = cate_df.apply(lambda x: f"{x.bin_6month}_{x.genre}",axis=1)

_cate_df = cate_df.query('fav_novel_cnt_bin!=-1')

dfs = []
for col in ['genre', 'yyyy_genre', 'yyyymm_genre', 'bin_2month_genre', 'bin_3month_genre', 'bin_4month_genre', 'bin_6month_genre']:
    dic = _cate_df.groupby(col).mean()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(cate_df[col].map(dic).values, columns=[f"{col}_mean"])
    dfs.append(_df)

    dic = _cate_df.groupby(col).std()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(cate_df[col].map(dic).values, columns=[f"{col}_std"])
    dfs.append(_df)

    dic = _cate_df.groupby(col).var()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(cate_df[col].map(dic).values, columns=[f"{col}_var"])
    dfs.append(_df)

    dic = _cate_df.groupby(col).skew()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(cate_df[col].map(dic).values, columns=[f"{col}_skew"])
    dfs.append(_df)

    dic = _cate_df.groupby(col).median()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(cate_df[col].map(dic).values, columns=[f"{col}_median"])
    dfs.append(_df)

genre_target_df = pd.concat(dfs, axis=1).fillna(0)

genre_target_df.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_genre_target_v2.ftr")
genre_target_df.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_genre_target_v2.ftr")

"""#### キーワード LDA"""

train_kw_lda_50 = pd.read_feather(f"{GCF.FEATURES_PATH}/train_kw_lda_50_norm.ftr")
test_kw_lda_50 = pd.read_feather(f"{GCF.FEATURES_PATH}/test_kw_lda_50_norm.ftr")
lda_all_df = pd.concat([train_kw_lda_50, test_kw_lda_50], axis=0).reset_index(drop=True)

thr = 0.2
lda_thr_df = pd.DataFrame(lda_all_df > thr)
lda_thr_df.columns = [f"{c}_t{thr}" for c in lda_thr_df.columns]

date_lda_df = pd.concat([date_df, lda_thr_df], axis=1)

dfs = []
for col in lda_thr_df.columns:
    dfs.append(date_lda_df.apply(lambda x: f'{x.year}_{x[col]}', axis=1))
    dfs.append(date_lda_df.apply(lambda x: f'{x.yyyymm}_{x[col]}', axis=1))
    dfs.append(date_lda_df.apply(lambda x: f'{x.bin_2month}_{x[col]}', axis=1))
    dfs.append(date_lda_df.apply(lambda x: f'{x.bin_3month}_{x[col]}', axis=1))
    dfs.append(date_lda_df.apply(lambda x: f'{x.bin_4month}_{x[col]}', axis=1))
    dfs.append(date_lda_df.apply(lambda x: f'{x.bin_6month}_{x[col]}', axis=1))

lda_date_for_group = pd.concat(dfs, axis=1)

date_lda_df = pd.concat([date_df, lda_thr_df, all_df[['fav_novel_cnt_bin']]], axis=1)
lda_date_for_group['fav_novel_cnt_bin'] = all_df['fav_novel_cnt_bin']
_lda_date_for_group = lda_date_for_group.query('fav_novel_cnt_bin!=-1')

dfs = []
for col in tqdm(range(120)):
    dic = _lda_date_for_group.groupby(col).mean()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(lda_date_for_group[col].map(dic).values, columns=[f"{col}_mean"])
    dfs.append(_df)

    dic = _lda_date_for_group.groupby(col).std()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(lda_date_for_group[col].map(dic).values, columns=[f"{col}_std"])
    dfs.append(_df)

    dic = _lda_date_for_group.groupby(col).var()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(lda_date_for_group[col].map(dic).values, columns=[f"{col}_var"])
    dfs.append(_df)

    dic = _lda_date_for_group.groupby(col).skew()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(lda_date_for_group[col].map(dic).values, columns=[f"{col}_skew"])
    dfs.append(_df)

    dic = _lda_date_for_group.groupby(col).median()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(lda_date_for_group[col].map(dic).values, columns=[f"{col}_median"])
    dfs.append(_df)
lda_target_feat = pd.concat(dfs, axis=1).fillna(0)
lda_target_feat.columns = [f"lda_thr{thr}_{c}" for c in lda_target_feat.columns]

#lda_target_feat.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_lda_thr00.ftr")
#lda_target_feat.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_lda_thr00.ftr")
lda_target_feat.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_lda_thr02.ftr")
lda_target_feat.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_lda_thr02.ftr")

"""#### bin feat"""

bin_ferat_df = pd.concat([date_df, all_df[['novel_type', 'isr15', 'isbl', 'isgl', 'iszankoku', 'istensei', 'istenni', 'fav_novel_cnt_bin']]], axis=1)

bin_cols = ['novel_type', 'isr15', 'isbl', 'isgl', 'iszankoku', 'istensei', 'istenni',]
dfs = []
for col in bin_cols:
    dfs.append(bin_ferat_df.apply(lambda x: f'{x.year}_{x[col]}', axis=1))
    dfs.append(bin_ferat_df.apply(lambda x: f'{x.yyyymm}_{x[col]}', axis=1))
    dfs.append(bin_ferat_df.apply(lambda x: f'{x.bin_2month}_{x[col]}', axis=1))
    dfs.append(bin_ferat_df.apply(lambda x: f'{x.bin_3month}_{x[col]}', axis=1))
    dfs.append(bin_ferat_df.apply(lambda x: f'{x.bin_4month}_{x[col]}', axis=1))
    dfs.append(bin_ferat_df.apply(lambda x: f'{x.bin_6month}_{x[col]}', axis=1))
bin_feat_for_group = pd.concat(dfs, axis=1)

bin_feat_for_group['fav_novel_cnt_bin'] = all_df['fav_novel_cnt_bin']
_bin_feat_for_group = bin_feat_for_group.query('fav_novel_cnt_bin!=-1')

dfs = []
for col in tqdm(range(42)):
    dic = _bin_feat_for_group.groupby(col).mean()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(bin_feat_for_group[col].map(dic).values, columns=[f"{col}_mean"])
    dfs.append(_df)

    dic = _bin_feat_for_group.groupby(col).std()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(bin_feat_for_group[col].map(dic).values, columns=[f"{col}_std"])
    dfs.append(_df)

    dic = _bin_feat_for_group.groupby(col).var()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(bin_feat_for_group[col].map(dic).values, columns=[f"{col}_var"])
    dfs.append(_df)

    dic = _bin_feat_for_group.groupby(col).skew()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(bin_feat_for_group[col].map(dic).values, columns=[f"{col}_skew"])
    dfs.append(_df)

    dic = _bin_feat_for_group.groupby(col).median()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(bin_feat_for_group[col].map(dic).values, columns=[f"{col}_median"])
    dfs.append(_df)
bin_target_feat = pd.concat(dfs, axis=1).fillna(0)
bin_target_feat.columns = [f"bin_feat_{c}" for c in bin_target_feat.columns]

bin_feat_for_group

bin_target_feat.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_bin_target.ftr")
bin_target_feat.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_bin_target.ftr")

"""#### user v2"""

keyword_ohe_train = pd.read_feather(f"{GCF.FEATURES_PATH}/train_kw_ohe_50_norm.ftr")
keyword_ohe_test = pd.read_feather(f"{GCF.FEATURES_PATH}/test_kw_ohe_50_norm.ftr")
kw_all = pd.concat([keyword_ohe_train, keyword_ohe_test]).reset_index(drop=True)

feat_dfs = []
for col in ['year', 'yyyymm', 'bin_2month', 'bin_3month', 'bin_4month', 'bin_6month']:
    dfs = []
    for _c, df in date_all_df.groupby(col):
        n = len(df) if len(df) < 10 else 10
        pca = PCA(n_components=n)
        pca_feat = pca.fit_transform(kw_all.loc[df.index].values)
        if n != 10:
            pca_feat = np.hstack([pca_feat, np.zeros((len(df), 10-n))])
        _df = pd.DataFrame(pca_feat, columns=[f'{col}_pca{i}' for i in range(10)])
        _df.index = df.index
        dfs.append(_df)
    feat_df = pd.concat(dfs)
    feat_dfs.append(feat_df)
feat_dfs = pd.concat(feat_dfs, axis=1)

feat_dfs.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_pca_kw_date.ftr")
feat_dfs.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_pca_kw_date.ftr")

dfs[0]



"""#### kw"""

keyword_ohe_train = pd.read_feather(f"{GCF.FEATURES_PATH}/train_kw_ohe_50_norm.ftr")
keyword_ohe_test = pd.read_feather(f"{GCF.FEATURES_PATH}/test_kw_ohe_50_norm.ftr")
kw_all = pd.concat([keyword_ohe_train, keyword_ohe_test]).reset_index(drop=True)
#kw_all['fav_novel_cnt_bin'] = all_df['fav_novel_cnt_bin']

from sklearn.cluster import KMeans

#kmeans = KMeans(n_clusters=20, random_state=0).fit(kw_all.values)
kmeans = KMeans(n_clusters=5, random_state=0).fit(keyword_ohe_test.values)
kw_labs = kmeans.transform(kw_all.values).argmax(1)
pd.DataFrame(kw_labs)[0].value_counts()

#kmeans_df = pd.concat([date_all_df, pd.DataFrame(kmeans.labels_, columns=['kmeans20'])], axis=1)
kmeans_df = pd.concat([date_all_df, pd.DataFrame(kw_labs, columns=['kmeans5'])], axis=1)

#kmeans_df['yyyy_kmeans20'] = kmeans_df.apply(lambda x: f"{x.year}_{x.kmeans20}",axis=1)
#kmeans_df['yyyymm_kmeans20'] = kmeans_df.apply(lambda x: f"{x.year}{x.month}_{x.kmeans20}",axis=1)
#kmeans_df['bin_2month_kmeans20'] = kmeans_df.apply(lambda x: f"{x.bin_2month}_{x.kmeans20}",axis=1)
#kmeans_df['bin_3month_kmeans20'] = kmeans_df.apply(lambda x: f"{x.bin_3month}_{x.kmeans20}",axis=1)
#kmeans_df['bin_4month_kmeans20'] = kmeans_df.apply(lambda x: f"{x.bin_4month}_{x.kmeans20}",axis=1)
#kmeans_df['bin_6month_kmeans20'] = kmeans_df.apply(lambda x: f"{x.bin_6month}_{x.kmeans20}",axis=1)
kmeans_df['yyyy_kmeans5'] = kmeans_df.apply(lambda x: f"{x.year}_{x.kmeans5}",axis=1)
kmeans_df['yyyymm_kmeans5'] = kmeans_df.apply(lambda x: f"{x.year}{x.month}_{x.kmeans5}",axis=1)
kmeans_df['bin_2month_kmeans5'] = kmeans_df.apply(lambda x: f"{x.bin_2month}_{x.kmeans5}",axis=1)
kmeans_df['bin_3month_kmeans5'] = kmeans_df.apply(lambda x: f"{x.bin_3month}_{x.kmeans5}",axis=1)
kmeans_df['bin_4month_kmeans5'] = kmeans_df.apply(lambda x: f"{x.bin_4month}_{x.kmeans5}",axis=1)
kmeans_df['bin_6month_kmeans5'] = kmeans_df.apply(lambda x: f"{x.bin_6month}_{x.kmeans5}",axis=1)

_kmeans_df = kmeans_df.query('fav_novel_cnt_bin!=-1')

dfs = []
#for col in ['yyyy_kmeans20', 'yyyymm_kmeans20', 'bin_2month_kmeans20', 'bin_3month_kmeans20', 'bin_4month_kmeans20', 'bin_6month_kmeans20']:
for col in ['yyyy_kmeans5', 'yyyymm_kmeans5', 'bin_2month_kmeans5', 'bin_3month_kmeans5', 'bin_4month_kmeans5', 'bin_6month_kmeans5']:
    _df = pd.DataFrame(kmeans_df[col].map(_kmeans_df.groupby(col).mean()['fav_novel_cnt_bin'].to_dict()).values, columns=[col+"_mean"])
    dfs.append(_df)

    _df = pd.DataFrame(kmeans_df[col].map(_kmeans_df.groupby(col).std()['fav_novel_cnt_bin'].to_dict()).values, columns=[col+"_std"])
    dfs.append(_df)

    _df = pd.DataFrame(kmeans_df[col].map(_kmeans_df.groupby(col).var()['fav_novel_cnt_bin'].to_dict()).values, columns=[col+"_var"])
    dfs.append(_df)

    _df = pd.DataFrame(kmeans_df[col].map(_kmeans_df.groupby(col).skew()['fav_novel_cnt_bin'].to_dict()).values, columns=[col+"_skew"])
    dfs.append(_df)

    _df = pd.DataFrame(kmeans_df[col].map(_kmeans_df.groupby(col).median()['fav_novel_cnt_bin'].to_dict()).values, columns=[col+"_median"])
    dfs.append(_df)

kmeans_target_df = pd.concat(dfs, axis=1).fillna(0)

#kmeans_target_df.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_kmeans20.ftr")
#kmeans_target_df.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_kmeans20.ftr")
kmeans_target_df.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_kmeans5.ftr")
kmeans_target_df.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_kmeans5.ftr")

kmeans_target_df

"""#### kmeans story or title"""

bert_vector_train = np.load(f"{GCF.FEATURES_PATH}/bert_vector_train.npy")
bert_vector_test = np.load(f"{GCF.FEATURES_PATH}/bert_vector_test.npy")

bert_vecs = np.vstack([bert_vector_train, bert_vector_test])

#kmeans_bert = KMeans(n_clusters=20, random_state=0).fit(bert_vecs)
kmeans = KMeans(n_clusters=3, random_state=0).fit(bert_vector_test)
bert_labs = kmeans.transform(bert_vecs).argmax(1)
pd.DataFrame(bert_labs)[0].value_counts()

#kmeans_df = pd.concat([date_all_df, pd.DataFrame(kmeans_bert.labels_, columns=['kmeans_bert20'])], axis=1)
kmeans_df = pd.concat([date_all_df, pd.DataFrame(bert_labs, columns=['kmeans_bert3'])], axis=1)

kmeans_df.query('fav_novel_cnt_bin!=-1')['kmeans_bert3'].hist(bins=20)
kmeans_df.query('fav_novel_cnt_bin==-1')['kmeans_bert3'].hist(bins=20)

#kmeans_df['yyyy_kmeans_bert20'] = kmeans_df.apply(lambda x: f"{x.year}_{x.kmeans_bert20}",axis=1)
#kmeans_df['yyyymm_kmeans_bert20'] = kmeans_df.apply(lambda x: f"{x.year}{x.month}_{x.kmeans_bert20}",axis=1)
#kmeans_df['bin_2month_kmeans_bert20'] = kmeans_df.apply(lambda x: f"{x.bin_2month}_{x.kmeans_bert20}",axis=1)
#kmeans_df['bin_3month_kmeans_bert20'] = kmeans_df.apply(lambda x: f"{x.bin_3month}_{x.kmeans_bert20}",axis=1)
#kmeans_df['bin_4month_kmeans_bert20'] = kmeans_df.apply(lambda x: f"{x.bin_4month}_{x.kmeans_bert20}",axis=1)
#kmeans_df['bin_6month_kmeans_bert20'] = kmeans_df.apply(lambda x: f"{x.bin_6month}_{x.kmeans_bert20}",axis=1)
kmeans_df['yyyy_kmeans_bert3'] = kmeans_df.apply(lambda x: f"{x.year}_{x.kmeans_bert3}",axis=1)
kmeans_df['yyyymm_kmeans_bert3'] = kmeans_df.apply(lambda x: f"{x.year}{x.month}_{x.kmeans_bert3}",axis=1)
kmeans_df['bin_2month_kmeans_bert3'] = kmeans_df.apply(lambda x: f"{x.bin_2month}_{x.kmeans_bert3}",axis=1)
kmeans_df['bin_3month_kmeans_bert3'] = kmeans_df.apply(lambda x: f"{x.bin_3month}_{x.kmeans_bert3}",axis=1)
kmeans_df['bin_4month_kmeans_bert3'] = kmeans_df.apply(lambda x: f"{x.bin_4month}_{x.kmeans_bert3}",axis=1)
kmeans_df['bin_6month_kmeans_bert3'] = kmeans_df.apply(lambda x: f"{x.bin_6month}_{x.kmeans_bert3}",axis=1)

_kmeans_df = kmeans_df.query('fav_novel_cnt_bin!=-1')

dfs = []
#for col in ['yyyy_kmeans_bert20', 'yyyymm_kmeans_bert20', 'bin_2month_kmeans_bert20', 'bin_3month_kmeans_bert20', 'bin_4month_kmeans_bert20', 'bin_6month_kmeans_bert20']:
for col in ['yyyy_kmeans_bert3', 'yyyymm_kmeans_bert3', 'bin_2month_kmeans_bert3', 'bin_3month_kmeans_bert3', 'bin_4month_kmeans_bert3', 'bin_6month_kmeans_bert3']:
    _df = pd.DataFrame(kmeans_df[col].map(_kmeans_df.groupby(col).mean()['fav_novel_cnt_bin'].to_dict()).values, columns=[col+"_mean"])
    dfs.append(_df)

    _df = pd.DataFrame(kmeans_df[col].map(_kmeans_df.groupby(col).std()['fav_novel_cnt_bin'].to_dict()).values, columns=[col+"_std"])
    dfs.append(_df)

    _df = pd.DataFrame(kmeans_df[col].map(_kmeans_df.groupby(col).var()['fav_novel_cnt_bin'].to_dict()).values, columns=[col+"_var"])
    dfs.append(_df)

    #_df = pd.DataFrame(kmeans_df[col].map(_kmeans_df.groupby(col).skew()['fav_novel_cnt_bin'].to_dict()).values, columns=[col+"_skew"])
    #dfs.append(_df)

    _df = pd.DataFrame(kmeans_df[col].map(_kmeans_df.groupby(col).median()['fav_novel_cnt_bin'].to_dict()).values, columns=[col+"_median"])
    dfs.append(_df)

kmeans_target_df = pd.concat(dfs, axis=1).fillna(0)

#kmeans_target_df.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_kmeans_bert20.ftr")
#kmeans_target_df.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_kmeans_bert20.ftr")
kmeans_target_df.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_kmeans_bert3.ftr")
kmeans_target_df.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_kmeans_bert3.ftr")

kmeans_target_df

"""## 0~4のonehotでTarget"""

onehot_target_df = pd.concat([
                       date_all_df,
                       #pd.DataFrame(kmeans.labels_, columns=['kmeans20']),
                       #pd.DataFrame(kmeans_bert.labels_, columns=['kmeans_bert20']),
                       pd.DataFrame(kw_labs, columns=['kmeans20']),
                       pd.DataFrame(bert_labs, columns=['kmeans_bert20']),
], axis=1)

onehot_target_df['yyyy_genre'] = onehot_target_df.apply(lambda x: f"{x.year}_{x.genre}",axis=1)
onehot_target_df['yyyymm_genre'] = onehot_target_df.apply(lambda x: f"{x.year}{x.month}_{x.genre}",axis=1)
onehot_target_df['bin_2month_genre'] = onehot_target_df.apply(lambda x: f"{x.bin_2month}_{x.genre}",axis=1)
onehot_target_df['bin_3month_genre'] = onehot_target_df.apply(lambda x: f"{x.bin_3month}_{x.genre}",axis=1)
onehot_target_df['bin_4month_genre'] = onehot_target_df.apply(lambda x: f"{x.bin_4month}_{x.genre}",axis=1)
onehot_target_df['bin_6month_genre'] = onehot_target_df.apply(lambda x: f"{x.bin_6month}_{x.genre}",axis=1)

onehot_target_df['yyyy_kmeans20'] = onehot_target_df.apply(lambda x: f"{x.year}_{x.kmeans20}",axis=1)
onehot_target_df['yyyymm_kmeans20'] = onehot_target_df.apply(lambda x: f"{x.year}{x.month}_{x.kmeans20}",axis=1)
onehot_target_df['bin_2month_kmeans20'] = onehot_target_df.apply(lambda x: f"{x.bin_2month}_{x.kmeans20}",axis=1)
onehot_target_df['bin_3month_kmeans20'] = onehot_target_df.apply(lambda x: f"{x.bin_3month}_{x.kmeans20}",axis=1)
onehot_target_df['bin_4month_kmeans20'] = onehot_target_df.apply(lambda x: f"{x.bin_4month}_{x.kmeans20}",axis=1)
onehot_target_df['bin_6month_kmeans20'] = onehot_target_df.apply(lambda x: f"{x.bin_6month}_{x.kmeans20}",axis=1)

onehot_target_df['yyyy_kmeans_bert20'] = onehot_target_df.apply(lambda x: f"{x.year}_{x.kmeans_bert20}",axis=1)
onehot_target_df['yyyymm_kmeans_bert20'] = onehot_target_df.apply(lambda x: f"{x.year}{x.month}_{x.kmeans_bert20}",axis=1)
onehot_target_df['bin_2month_kmeans_bert20'] = onehot_target_df.apply(lambda x: f"{x.bin_2month}_{x.kmeans_bert20}",axis=1)
onehot_target_df['bin_3month_kmeans_bert20'] = onehot_target_df.apply(lambda x: f"{x.bin_3month}_{x.kmeans_bert20}",axis=1)
onehot_target_df['bin_4month_kmeans_bert20'] = onehot_target_df.apply(lambda x: f"{x.bin_4month}_{x.kmeans_bert20}",axis=1)
onehot_target_df['bin_6month_kmeans_bert20'] = onehot_target_df.apply(lambda x: f"{x.bin_6month}_{x.kmeans_bert20}",axis=1)

_onehot_target_df = onehot_target_df.query('fav_novel_cnt_bin!=-1')
_ohe_fav_novel_cnt_bin = pd.DataFrame(_onehot_target_df['fav_novel_cnt_bin'].map(lambda x: np.eye(5)[x]).tolist(), columns=[f"target_{i}" for i in range(5)])
_onehot_target_df = pd.concat([_onehot_target_df, _ohe_fav_novel_cnt_bin], axis=1)

cols = ['year', 'yyyymm', 'bin_2month', 'bin_3month', 'bin_4month', 'bin_6month']
cols += ['genre', 'yyyy_genre', 'yyyymm_genre', 'bin_2month_genre', 'bin_3month_genre', 'bin_4month_genre', 'bin_6month_genre']
cols += ['yyyy_kmeans20', 'yyyymm_kmeans20', 'bin_2month_kmeans20', 'bin_3month_kmeans20', 'bin_4month_kmeans20', 'bin_6month_kmeans20']
cols += ['yyyy_kmeans_bert20', 'yyyymm_kmeans_bert20', 'bin_2month_kmeans_bert20', 'bin_3month_kmeans_bert20', 'bin_4month_kmeans_bert20', 'bin_6month_kmeans_bert20']

dfs = []
for col in cols:
    for i in range(5):
        _df = pd.DataFrame(onehot_target_df[col].map(_onehot_target_df.groupby(col).mean()[f'target_{i}'].to_dict()).values, columns=[col+f"_target_{i}_"+"mean"])
        dfs.append(_df)

        _df = pd.DataFrame(onehot_target_df[col].map(_onehot_target_df.groupby(col).std()[f'target_{i}'].to_dict()).values, columns=[col+f"_target_{i}_"+"std"])
        dfs.append(_df)

        _df = pd.DataFrame(onehot_target_df[col].map(_onehot_target_df.groupby(col).var()[f'target_{i}'].to_dict()).values, columns=[col+f"_target_{i}_"+"var"])
        dfs.append(_df)

        _df = pd.DataFrame(onehot_target_df[col].map(_onehot_target_df.groupby(col).median()[f'target_{i}'].to_dict()).values, columns=[col+f"_target_{i}_"+"median"])
        dfs.append(_df)

onehot_target_feat = pd.concat(dfs, axis=1).fillna(0)

#onehot_target_feat.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_onehot_target_v1.ftr")
#onehot_target_feat.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_onehot_target_v1.ftr")
onehot_target_feat.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_onehot_target_v2.ftr")
onehot_target_feat.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_onehot_target_v2.ftr")

"""## user cls"""

kw_all['userid'] = all_df['userid']
vecs = kw_all.groupby('userid').cumsum().values
kw_all = kw_all.drop('userid', axis=1)

kmeans = KMeans(n_clusters=10, random_state=0).fit(vecs)
pd.DataFrame(kmeans.labels_)[0].value_counts()

user_target_df = pd.concat([
                       date_all_df,
                       pd.DataFrame(kmeans.labels_, columns=['kmeans_user10']),
], axis=1)

user_target_df['yyyy_kmeans_user10'] = user_target_df.apply(lambda x: f"{x.year}_{x.kmeans_user10}",axis=1)
user_target_df['yyyymm_kmeans_user10'] = user_target_df.apply(lambda x: f"{x.year}{x.month}_{x.kmeans_user10}",axis=1)
user_target_df['bin_2month_kmeans_user10'] = user_target_df.apply(lambda x: f"{x.bin_2month}_{x.kmeans_user10}",axis=1)
user_target_df['bin_3month_kmeans_user10'] = user_target_df.apply(lambda x: f"{x.bin_3month}_{x.kmeans_user10}",axis=1)
user_target_df['bin_4month_kmeans_user10'] = user_target_df.apply(lambda x: f"{x.bin_4month}_{x.kmeans_user10}",axis=1)
user_target_df['bin_6month_kmeans_user10'] = user_target_df.apply(lambda x: f"{x.bin_6month}_{x.kmeans_user10}",axis=1)

_user_target_df = user_target_df.query('fav_novel_cnt_bin!=-1')
_ohe_fav_novel_cnt_bin = pd.DataFrame(_user_target_df['fav_novel_cnt_bin'].map(lambda x: np.eye(5)[x]).tolist(), columns=[f"target_{i}" for i in range(5)])
_user_target_df = pd.concat([_user_target_df, _ohe_fav_novel_cnt_bin], axis=1)

dfs = []
for col in ['yyyy_kmeans_user10', 'yyyymm_kmeans_user10', 'bin_2month_kmeans_user10', 'bin_3month_kmeans_user10', 'bin_4month_kmeans_user10', 'bin_6month_kmeans_user10']:
    for i in range(5):
        _df = pd.DataFrame(user_target_df[col].map(_user_target_df.groupby(col).mean()[f'target_{i}'].to_dict()).values, columns=[col+f"_target_{i}_"+"mean"])
        dfs.append(_df)

        _df = pd.DataFrame(user_target_df[col].map(_user_target_df.groupby(col).std()[f'target_{i}'].to_dict()).values, columns=[col+f"_target_{i}_"+"std"])
        dfs.append(_df)

        _df = pd.DataFrame(user_target_df[col].map(_user_target_df.groupby(col).var()[f'target_{i}'].to_dict()).values, columns=[col+f"_target_{i}_"+"var"])
        dfs.append(_df)

        _df = pd.DataFrame(user_target_df[col].map(_user_target_df.groupby(col).median()[f'target_{i}'].to_dict()).values, columns=[col+f"_target_{i}_"+"median"])
        dfs.append(_df)

    _df = pd.DataFrame(user_target_df[col].map(_user_target_df.groupby(col).mean()['fav_novel_cnt_bin'].to_dict()).values, columns=[col+"_mean"])
    dfs.append(_df)

    _df = pd.DataFrame(user_target_df[col].map(_user_target_df.groupby(col).std()['fav_novel_cnt_bin'].to_dict()).values, columns=[col+"_std"])
    dfs.append(_df)

    _df = pd.DataFrame(user_target_df[col].map(_user_target_df.groupby(col).var()['fav_novel_cnt_bin'].to_dict()).values, columns=[col+"_var"])
    dfs.append(_df)

    _df = pd.DataFrame(user_target_df[col].map(_user_target_df.groupby(col).median()['fav_novel_cnt_bin'].to_dict()).values, columns=[col+"_median"])
    dfs.append(_df)

user_target_feat = pd.concat(dfs, axis=1).fillna(0)

user_target_feat

user_target_feat.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_user_target_v3.ftr")
user_target_feat.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_user_target_v3.ftr")

"""## Stacking"""

oof_lgbm = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp108_lgbm_now_best/lgbm_oof_preds.csv")
oof_lgbm2 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp125_lgbm_now_best/lgbm_oof_preds.csv")  # with bert
oof_lgbm3 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp131_lgbm_now_best/lgbm_oof_preds.csv")  # with kmeans
oof_lgbm4 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp133_lgbm_now_best/lgbm_oof_preds.csv")  # with kmeans
oof_lgbm5 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp139_lgbm_now_best/lgbm_oof_preds.csv")
oof_lgbm6 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp148_lgbm_now_best/lgbm_oof_preds.csv") # userid2
oof_lgbm7 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp150_lgbm_now_best/lgbm_oof_preds.csv")

oof_cat = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp109_cat_now_best/cat_oof_preds.csv")
oof_cat2 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp126_cat_now_best/cat_oof_preds.csv")  # with bert
oof_cat3 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp132_cat_now_best/cat_oof_preds.csv")  # with kmeans
oof_cat4 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp134_cat_now_best/cat_oof_preds.csv")  # with kmeans
oof_cat5 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp140_cat_now_best/cat_oof_preds.csv") 
oof_cat6 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp144_cat_now_best/cat_oof_preds.csv") 
oof_cat7 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp147_cat_now_best/cat_oof_preds.csv") # userid2
oof_cat８ = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp149_cat_now_best/cat_oof_preds.csv") # userid2

oof_nn2 = np.load(f"{GCF.FEATURES_PATH}/nn_output_exp113_cls_token_train.npy")
oof_nn4 = np.load(f"{GCF.FEATURES_PATH}/nn_output_exp129_userid_2_train.npy")
#oof_nn5 = np.load(f"{GCF.FEATURES_PATH}/nn_output_exp138_not_onehot_train.npy")
oof_nn6 = np.load(f"{GCF.FEATURES_PATH}/nn_output_exp141_fix_leak_train.npy")
oof_nn7 = np.load(f"{GCF.FEATURES_PATH}/nn_output_exp142_no_ae_train.npy")
oof_nn9 = np.load(f"{GCF.FEATURES_PATH}/nn_output_exp151_user_cls_train.npy")

pred_lgbm = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp108_lgbm_now_best/lgbm_test_preds.csv")
pred_lgbm2 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp125_lgbm_now_best/lgbm_test_preds.csv")
pred_lgbm3 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp131_lgbm_now_best/lgbm_test_preds.csv")
pred_lgbm4 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp133_lgbm_now_best/lgbm_test_preds.csv")
pred_lgbm5 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp139_lgbm_now_best/lgbm_test_preds.csv")
pred_lgbm6 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp148_lgbm_now_best/lgbm_test_preds.csv")
pred_lgbm7 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp150_lgbm_now_best/lgbm_test_preds.csv")

pred_cat = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp109_cat_now_best/cat_test_preds.csv")
pred_cat2 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp126_cat_now_best/cat_test_preds.csv")
pred_cat3 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp132_cat_now_best/cat_test_preds.csv")
pred_cat4 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp134_cat_now_best/cat_test_preds.csv")
pred_cat5 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp140_cat_now_best/cat_test_preds.csv")
pred_cat6 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp144_cat_now_best/cat_test_preds.csv")
pred_cat7 = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp147_cat_now_best/cat_test_preds.csv")
pred_cat８ = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp149_cat_now_best/cat_test_preds.csv")

pred_nn2 = np.load(f"{GCF.FEATURES_PATH}/nn_output_exp113_cls_token_test.npy")
pred_nn4 = np.load(f"{GCF.FEATURES_PATH}/nn_output_exp129_userid_2_test.npy")
pred_nn5 = np.load(f"{GCF.FEATURES_PATH}/nn_output_exp138_not_onehot_test.npy")
pred_nn6 = np.load(f"{GCF.FEATURES_PATH}/nn_output_exp141_fix_leak_test.npy")
pred_nn7 = np.load(f"{GCF.FEATURES_PATH}/nn_output_exp142_no_ae_test.npy")
pred_nn9 = np.load(f"{GCF.FEATURES_PATH}/nn_output_exp151_user_cls_test.npy")

X = np.hstack([
               oof_lgbm.values[:, :-1],
               oof_lgbm2.values[:, :-1],
               oof_lgbm5.values[:, :-1],
               oof_lgbm6.values[:, :-1],
               #oof_lgbm7.values[:, :-1],
               oof_cat.values[:, :-1],
               oof_cat2.values[:, :-1],
               oof_cat5.values[:, :-1],
               oof_cat6.values[:, :-1],
               oof_cat7.values[:, :-1],
               oof_cat8.values[:, :-1],
               oof_nn2,
               oof_nn6,
               oof_nn7,
               oof_nn9,
])
y = train_df['fav_novel_cnt_bin'].values
X_test = np.hstack([
                    pred_lgbm.values[:, 1:],
                    pred_lgbm2.values[:, 1:],
                    pred_lgbm5.values[:, 1:],
                    pred_lgbm6.values[:, 1:],
                    #pred_lgbm7.values[:, 1:],
                    pred_cat.values[:, 1:],
                    pred_cat2.values[:, 1:],
                    pred_cat5.values[:, 1:],
                    pred_cat6.values[:, 1:],
                    pred_cat7.values[:, 1:],
                    pred_cat8.values[:, 1:],
                    pred_nn2,
                    pred_nn6,
                    pred_nn7,
                    pred_nn9,
])

oof = np.zeros((len(y), 5))
test_predicts = []
skf = StratifiedKFold(n_splits=GCF.N_FOLDS, random_state=GCF.SEED, shuffle=True).split(X, y)
for fold, (train_index, valid_index) in enumerate(skf):
    clf = LogisticRegression(max_iter=1000)
    clf.fit(X[train_index, :], y[train_index])
    y_pred_valid = clf.predict_proba(X[valid_index, :])
    score = log_loss(np.stack([np.eye(5)[i] for i in y[valid_index]]), y_pred_valid)
    print(f"fold-{fold}: {score}")
    oof[valid_index, :] = y_pred_valid
    y_pred_test = clf.predict_proba(X_test)
    test_predicts.append(y_pred_test)

oof_score = log_loss(np.stack([np.eye(5)[i] for i in y]), oof)
print("OOF:", oof_score)

# 0.6970684637841661: add Cat, kmeans
# 0.6925953265645629: add userid x 2
# 0.6971444814074064: replace userid x 2
# 0.6747773551905 : all in
# 0.6751966831649424
# Stacking_param_tune: 0.6794359540250047
# replace: 0.6800705033735318
# cat fix: 0.6792515517496378
# LGBM fix: 0.6793558475148042

sub_df[["proba_0","proba_1","proba_2","proba_3","proba_4"]] = np.stack(test_predicts).mean(0)
sub_df.to_csv('Stacking_userid_cat_nn.csv', index=None)

