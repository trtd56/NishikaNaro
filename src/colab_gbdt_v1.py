# -*- coding: utf-8 -*-
"""なろう_GBDT.ipynb のコピー

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h9bUAO4pVZhnraZECmg9l8BHf2sBchz0

# 小説家になろう ブクマ数予測 \~”伸びる”タイトルとは？\~

## Memo
- 文章ベクトル
- kwごとのtarget

### done
- PCA
- 単純なdate target
- pseudoでtestのtargetを修正
- userごとの修正
- dateのshift
- kw is none
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers unidic-lite fugashi ipadic python-Levenshtein sentencepiece
!pip install -U torch

"""## 共通設定"""

import gc
import os
import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from datetime import datetime as dt
from tqdm.notebook import tqdm
from collections import defaultdict
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
import itertools
import Levenshtein

from gensim.corpora.dictionary import Dictionary
from gensim.models import LdaModel, TfidfModel, CoherenceModel
from collections import defaultdict

# Models
import lightgbm as lgb
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA

import re

# Transformer
import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader

from transformers import get_cosine_schedule_with_warmup
from transformers import AutoConfig
from transformers import AutoTokenizer
from transformers import AutoModel
from transformers import AdamW

from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

device = torch.device("cuda")
scaler = torch.cuda.amp.GradScaler()

class GCF:
    EXP_NAME = 'exp080_target_encoding'

    INPUT_PATH = "/content/drive/MyDrive/Study/Nishika"
    FEATURES_PATH = f"{INPUT_PATH}/features"
    RESULT_PATH = f"{INPUT_PATH}/result"
    MODELS_PATH = f"{INPUT_PATH}/models/{EXP_NAME}"

    N_FOLDS = 5
    SEED = 0

    FEATURES = [
        "pc_or_k",
        "org_bin",
        "genre_ohe", "biggenre_ohe",
        "kw_ohe_50_norm",
        "kw_lda_50_norm",
        "yyyymm",
        "date2",
        #"userid_over5",
        "userid_only_test",
        "url_count_3",
        "kikaku_v1",
        "autopost",
        "cumsum_v1",
        "user_target_v1",
        #"user_target_v2_leak_fix",
        #"genre_target_v1",
        "genre_target_v2",
        #"lda_thr00",
        "bin_target",
        "date_target_v1",
        #"date_diff_v1",
        "kw_none",
        #"user_target_pseudo_v1",
        #"user_window_pseudo_v1",
    ]

train_df = pd.read_csv(f"{GCF.INPUT_PATH}/train.csv")
test_df = pd.read_csv(f"{GCF.INPUT_PATH}/test.csv")
sub_df = pd.read_csv(f"{GCF.INPUT_PATH}/sample_submission.csv")

test_df['fav_novel_cnt_bin'] = -1

"""## 特徴抽出

### PCか携帯か
"""

pc_or_k_dic = {0:[0, 0], 1:[1, 0], 2:[0, 1], 3:[1, 1]}
pc_or_k_train = pd.DataFrame(train_df['pc_or_k'].map(pc_or_k_dic).tolist(), columns=['pc', 'keitai'])
pc_or_k_test = pd.DataFrame(test_df['pc_or_k'].map(pc_or_k_dic).tolist(), columns=['pc', 'keitai'])

pc_or_k_train.to_feather(f"{GCF.FEATURES_PATH}/train_pc_or_k.ftr")
pc_or_k_test.to_feather(f"{GCF.FEATURES_PATH}/test_pc_or_k.ftr")

del pc_or_k_test, pc_or_k_train, pc_or_k_dic
gc.collect()

"""### オリジナル2値データ

- 元データそのままの2値データ
- endとisstopはすべての値が0なので無視する
"""

org_bin_feat = ['isr15', 'isbl', 'isgl', 'iszankoku', 'istensei', 'istenni']
org_bin_train = train_df[org_bin_feat]
org_bin_test = test_df[org_bin_feat]

org_bin_train = pd.concat([org_bin_train, train_df[['novel_type']] - 1], axis=1)
org_bin_test = pd.concat([org_bin_test, test_df[['novel_type']] - 1], axis=1)

org_bin_train.to_feather(f"{GCF.FEATURES_PATH}/train_org_bin.ftr")
org_bin_test.to_feather(f"{GCF.FEATURES_PATH}/test_org_bin.ftr")

del org_bin_train, org_bin_test, org_bin_feat
gc.collect()

"""### ジャンル

#### 単純なOne-Hot Encoding

##### ジャンル
"""

oe = OneHotEncoder(sparse=False, dtype = int)
genre_train = oe.fit_transform(train_df[['genre']])
genre_test = oe.transform(test_df[['genre']])

col = [f"genre_{i}" for i in oe.categories_[0]]

genre_train = pd.DataFrame(genre_train, columns=col)
genre_test = pd.DataFrame(genre_test, columns=col)

genre_train.to_feather(f"{GCF.FEATURES_PATH}/train_genre_ohe.ftr")
genre_test.to_feather(f"{GCF.FEATURES_PATH}/test_genre_ohe.ftr")

del genre_train, genre_test, col, oe
gc.collect()

"""##### 大ジャンル"""

oe = OneHotEncoder(sparse=False, dtype = int)
biggenre_train = oe.fit_transform(train_df[['biggenre']])
biggenre_test = oe.transform(test_df[['biggenre']])

col = [f"biggenre_{i}" for i in oe.categories_[0]]

biggenre_train = pd.DataFrame(biggenre_train, columns=col)
biggenre_test = pd.DataFrame(biggenre_test, columns=col)

biggenre_train.to_feather(f"{GCF.FEATURES_PATH}/train_biggenre_ohe.ftr")
biggenre_test.to_feather(f"{GCF.FEATURES_PATH}/test_biggenre_ohe.ftr")

del biggenre_train, biggenre_test, col, oe
gc.collect()

"""#### カテゴリカルencoding"""

genre_dic = {v: i for i, v in enumerate(train_df['genre'].unique())}

train_genre = train_df['genre'].map(genre_dic)
test_genre = test_df['genre'].map(genre_dic)

biggenre_dic = {v: i for i, v in enumerate(train_df['biggenre'].unique())}

train_biggenre = train_df['biggenre'].map(biggenre_dic)
test_biggenre = test_df['biggenre'].map(biggenre_dic)

train_genre_cate = pd.DataFrame(zip(train_genre.values, train_biggenre.values), columns=['genre_cate', 'biggenre_cate'])
test_genre_cate = pd.DataFrame(zip(test_genre.values, test_biggenre.values), columns=['genre_cate', 'biggenre_cate'])

train_genre_cate.to_feather(f"{GCF.FEATURES_PATH}/train_genre_cate.ftr")
test_genre_cate.to_feather(f"{GCF.FEATURES_PATH}/test_genre_cate.ftr")

del genre_dic, biggenre_dic, train_genre, test_genre, train_biggenre, test_biggenre, train_genre_cate, test_genre_cate
gc.collect()

"""### Keyword
- 出現上位100件のonehot
- 出現上位50件のonehot
"""

corpus_keyword = train_df['keyword'].tolist()

d = defaultdict(int)
for words in corpus_keyword:
    if type(words) is float:
        continue
    for w in words.split():
        d[w] += 1

#available_kw = [k for k, v in d.items() if v >= 100]
available_kw = [k for k, v in d.items() if v >= 50]
available_kw_dic = {v: np.eye(len(available_kw))[i] for i, v in enumerate(available_kw)}

def onehot_keywords(kw, dic):
    vec = np.zeros(len(dic))
    if type(kw) is float:
        return vec
    for w in kw.split():
        try:
            ohe = dic[w]
        except KeyError:
            continue
        vec += ohe
    return vec.tolist()

keyword_vecs = train_df['keyword'].map(lambda x: onehot_keywords(x, available_kw_dic)).tolist()
keyword_ohe_train = pd.DataFrame(keyword_vecs, columns=available_kw)
del keyword_vecs
keyword_vecs = test_df['keyword'].map(lambda x: onehot_keywords(x, available_kw_dic)).tolist()
keyword_ohe_test = pd.DataFrame(keyword_vecs, columns=available_kw)

#keyword_ohe_train.to_feather(f"{GCF.FEATURES_PATH}/train_kw_ohe_100.ftr")
#keyword_ohe_test.to_feather(f"{GCF.FEATURES_PATH}/test_kw_ohe_100.ftr")
keyword_ohe_train.to_feather(f"{GCF.FEATURES_PATH}/train_kw_ohe_50.ftr")
keyword_ohe_test.to_feather(f"{GCF.FEATURES_PATH}/test_kw_ohe_50.ftr")

"""#### is Nan"""

pd.DataFrame(train_df['keyword'].isnull().astype(int).values, columns=['kw_is_none']).to_feather(f"{GCF.FEATURES_PATH}/train_kw_none.ftr")
pd.DataFrame(test_df['keyword'].isnull().astype(int).values, columns=['kw_is_none']).to_feather(f"{GCF.FEATURES_PATH}/test_kw_none.ftr")

"""#### 企画っぽいやつ"""

kikaku_1 = [
            "ゲラゲラコンテスト３",
            "新人発掘コンテスト",
            "がうがうコン1",
            "夏の夜の恋物語企画",
            '架空戦記創作大会'
]
kikaku_2 = [
            'HJ2021',
            '夏のホラー2021',
            '冬童話2021',          
]
kikaku_3 = [
            'ネット小説大賞九感想',
            'ネット小説大賞七感想',
            'ネット小説大賞八感想',
            'ネット小説大賞九',
]
kikaku_4 = [
            'OVL大賞7M',
            'OVL大賞7',
            'OVL大賞7F',
]
kikaku_5 = [
            'ESN大賞２',
            'ESN大賞',
            'ESN大賞３',
]
kikaku_6 = [            
            'キネノベ大賞２',
            'キネノベ大賞３',
]
kikaku_7 = [               
            '集英社小説大賞2',
            '集英社WEB小説大賞',
            '集英社小説大賞２',
]
kikaku_8 = [
            '123大賞',
            'MBSラジオ短編賞1',
            'マンガＵＰ！賞１',
            'ネット小説大賞八',
            'なろうラジオ大賞2'
]
kikaku_cols = kikaku_1 + kikaku_2 + kikaku_3 + kikaku_4 + kikaku_5 + kikaku_6 + kikaku_7 + kikaku_8

kikaku_train = pd.concat([
           keyword_ohe_train[kikaku_1].max(1),
           keyword_ohe_train[kikaku_2].max(1),
           keyword_ohe_train[kikaku_3].max(1),
           keyword_ohe_train[kikaku_4].max(1),
           keyword_ohe_train[kikaku_5].max(1),
           keyword_ohe_train[kikaku_6].max(1),
           keyword_ohe_train[kikaku_7].max(1),
           keyword_ohe_train[kikaku_8].max(1),
           keyword_ohe_train[kikaku_cols].max(1),
], axis=1)
kikaku_train = (kikaku_train > 0).astype(int)
kikaku_train.columns = [f"kikaku_{i}"for i in range(8)] + ["kikaku_all"]

kikaku_test = pd.concat([
           keyword_ohe_test[kikaku_1].max(1),
           keyword_ohe_test[kikaku_2].max(1),
           keyword_ohe_test[kikaku_3].max(1),
           keyword_ohe_test[kikaku_4].max(1),
           keyword_ohe_test[kikaku_5].max(1),
           keyword_ohe_test[kikaku_6].max(1),
           keyword_ohe_test[kikaku_7].max(1),
           keyword_ohe_test[kikaku_8].max(1),
           keyword_ohe_test[kikaku_cols].max(1),
], axis=1)
kikaku_test = (kikaku_test > 0).astype(int)
kikaku_test.columns = [f"kikaku_{i}"for i in range(8)] + ["kikaku_all"]

kikaku_train.to_feather(f"{GCF.FEATURES_PATH}/train_kikaku_v1.ftr")
kikaku_test.to_feather(f"{GCF.FEATURES_PATH}/test_kikaku_v1.ftr")

del kikaku_cols, kikaku_1, kikaku_2, kikaku_3, kikaku_4, kikaku_5, kikaku_6, kikaku_7, kikaku_8
gc.collect()

"""#### 編集距離で絞り込み"""

'''
for i in keyword_ohe_train.columns:
    if len(i) <= 2:
        continue
    for j in keyword_ohe_train.columns:
        if i == j:
            continue
        if len(j) <= 2:
            continue
        d = Levenshtein.distance(i, j)
        if d < 2:
            print(i, j, d)
'''
norm_word_dic = {
    'コメディー': ['コメディ'],
    'ミステリー': ['ミステリ'],
    '新選組': ['新撰組'],
    '片思い': ['片想い'],
    'ざまあ': ['ざまぁ'],
}
norm_genre_dic = {
    '超能力': ['異能力'],
    '宇宙船': ['宇宙人'],
    #'異世界転移': ['異世界転生'],
}

for k, v in norm_word_dic.items():
    keyword_ohe_train[k] = keyword_ohe_train[k] + keyword_ohe_train[v].sum(1)
    keyword_ohe_train = keyword_ohe_train.drop(v, axis=1)

for k, v in norm_word_dic.items():
    keyword_ohe_test[k] = keyword_ohe_test[k] + keyword_ohe_test[v].sum(1)
    keyword_ohe_test = keyword_ohe_test.drop(v, axis=1)

keyword_ohe_train = (keyword_ohe_train > 0).astype(int)
keyword_ohe_test = (keyword_ohe_test > 0).astype(int)

keyword_ohe_train.to_feather(f"{GCF.FEATURES_PATH}/train_kw_ohe_50_norm.ftr")
keyword_ohe_test.to_feather(f"{GCF.FEATURES_PATH}/test_kw_ohe_50_norm.ftr")

del keyword_ohe_train, keyword_ohe_test, keyword_vecs, available_kw, available_kw_dic, d, w, corpus_keyword
gc.collect()

"""#### LDA"""

keyword_ohe_train = pd.read_feather(f"{GCF.FEATURES_PATH}/train_kw_ohe_50_norm.ftr")
keyword_ohe_test = pd.read_feather(f"{GCF.FEATURES_PATH}/test_kw_ohe_50_norm.ftr")

all_keywords = []
for _, row in keyword_ohe_train.iterrows():
    kws = row[row > 0].index.tolist()
    all_keywords.append(kws)
for _, row in keyword_ohe_test.iterrows():
    kws = row[row > 0].index.tolist()
    all_keywords.append(kws)

dictionary = Dictionary(all_keywords)
corpus = [dictionary.doc2bow(text) for text in all_keywords]
tfidf = TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]

coherence_vals = []
perplexity_vals = []
topic_lst = [10, 20, 30, 40, 50]
for n_topic in topic_lst:
    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=n_topic, random_state=0)
    perplexity_vals.append(np.exp2(-lda_model.log_perplexity(corpus)))
    coherence_model_lda = CoherenceModel(model=lda_model, texts=all_keywords, dictionary=dictionary, coherence='c_v')
    coherence_vals.append(coherence_model_lda.get_coherence())

# Perplexity は低ければ低い程，Coherence は高ければ高い程、良い
pd.DataFrame({
    'n_topic': topic_lst,
    'coherence': coherence_vals,
    'perplexity': perplexity_vals,
}).set_index('n_topic')#.plot()

NUM_TOPICS = 20
lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=NUM_TOPICS, random_state=0)

lda_model.show_topics()

def _lda_vectorizer(d):
    vec = np.zeros(NUM_TOPICS)
    for i, v in lda_model.get_document_topics(d):
        vec[i] = v
    return vec
    
vecs = []
for c in tqdm(corpus):
    v = _lda_vectorizer(c)
    vecs.append(v)

topic_vecs = pd.DataFrame(np.stack(vecs), columns=[f"lda_topic_{i:02}" for i in range(NUM_TOPICS)])
topic_vecs.head()

topic_vecs.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_kw_lda_50_norm.ftr")
topic_vecs.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_kw_lda_50_norm.ftr")

del keyword_ohe_train, keyword_ohe_test, dictionary, corpus, lda, score_by_topic, topic_vecs, vecs, lda_model
gc.collect()

"""### アップロード日付

#### 年・月
"""

year_train = train_df['general_firstup'].map(lambda x: x.split('-')[0]).astype(int)
month_train = train_df['general_firstup'].map(lambda x: x.split('-')[1]).astype(int)
year_test = test_df['general_firstup'].map(lambda x: x.split('-')[0]).astype(int)
month_test = test_df['general_firstup'].map(lambda x: x.split('-')[1]).astype(int)

date_train = pd.DataFrame(zip(year_train.tolist(), month_train.tolist()), columns=['upload_year', 'upload_month'])
date_test = pd.DataFrame(zip(year_test.tolist(), month_test.tolist()), columns=['upload_year', 'upload_month'])

date_train.to_feather(f"{GCF.FEATURES_PATH}/train_yyyymm.ftr")
date_test.to_feather(f"{GCF.FEATURES_PATH}/test_yyyymm.ftr")

del date_train, date_test, month_test, year_test, month_train, year_train
gc.collect()

"""#### 時間帯・曜日

"""

hour_train = train_df['general_firstup'].map(lambda x: int(x.split()[1].split(":")[0])).values
hour_test = test_df['general_firstup'].map(lambda x: int(x.split()[1].split(":")[0])).values

day_of_week_train = train_df['general_firstup'].map(lambda x: dt.strptime(x, '%Y-%m-%d %H:%M:%S').isoweekday()).values
day_of_week_test = test_df['general_firstup'].map(lambda x: dt.strptime(x, '%Y-%m-%d %H:%M:%S').isoweekday()).values

date2_train = pd.DataFrame(zip(hour_train, day_of_week_train), columns=['hour', 'day_of_week'])
date2_test = pd.DataFrame(zip(hour_test, day_of_week_test), columns=['hour', 'day_of_week'])

date2_train.to_feather(f"{GCF.FEATURES_PATH}/train_date2.ftr")
date2_test.to_feather(f"{GCF.FEATURES_PATH}/test_date2.ftr")

del date2_train, date2_test, hour_train, hour_test, day_of_week_train, day_of_week_test
gc.collect()

"""#### 自動投稿か"""

set_time_train = train_df['general_firstup'].map(lambda x: x.split()[-1][3:] == '00:00').astype(int)
set_time_test = test_df['general_firstup'].map(lambda x: x.split()[-1][3:] == '00:00').astype(int)

pd.DataFrame(set_time_train.tolist(), columns=['auto_post']).to_feather(f"{GCF.FEATURES_PATH}/train_autopost.ftr")
pd.DataFrame(set_time_test.tolist(), columns=['auto_post']).to_feather(f"{GCF.FEATURES_PATH}/test_autopost.ftr")

del set_time_train, set_time_test
gc.collect()

"""### URL"""

removes = ['http://', 'https://', 'www.', '.co', '.ne', '.jp', '.com']
def get_domain(string): 
    url = re.findall('https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+', string)
    _url = []
    for u in url:
        for s in removes:
            u = u.replace(s, '')
        _url.append(u)
    return _url

train_urls = train_df['story'].map(get_domain)
test_urls = test_df['story'].map(get_domain)

all_urls = train_urls.tolist() + test_urls.tolist()
domein_count = pd.DataFrame(list(itertools.chain.from_iterable(all_urls)))[0].value_counts()

domein_over3 = domein_count[domein_count > 3].index.values

def get_count_url(urls_lst, domein_over3):
    lst = []
    for urls in urls_lst:
        count_url = np.zeros(len(domein_over3))
        for url in urls:
            count_url += (domein_over3 == url).astype(int)
        lst.append(count_url)
    return pd.DataFrame(lst, columns=domein_over3)

train_url_count = get_count_url(train_urls, domein_over3)
test_url_count = get_count_url(test_urls, domein_over3)

max_count = max(test_url_count.max().max(), train_url_count.max().max())
train_url_count = train_url_count / max_count
test_url_count = test_url_count / max_count

train_url_count.to_feather(f"{GCF.FEATURES_PATH}/train_url_count_3.ftr")
test_url_count.to_feather(f"{GCF.FEATURES_PATH}/test_url_count_3.ftr")

del train_url_count, test_url_count, max_count, domein_over3, domein_count, all_urls, train_urls, test_urls
del removes, get_domain, get_count_url
gc.collect()

"""### One-Hot Encoding"""

month_train = train_df['general_firstup'].map(lambda x: np.eye(12)[int(x.split('-')[1])-1]).tolist()
month_test = test_df['general_firstup'].map(lambda x: np.eye(12)[int(x.split('-')[1])-1]).tolist()

hour_train = train_df['general_firstup'].map(lambda x: np.eye(24)[int(x.split()[1].split(":")[0])]).tolist()
hour_test = test_df['general_firstup'].map(lambda x: np.eye(24)[int(x.split()[1].split(":")[0])]).tolist()

day_of_week_train = train_df['general_firstup'].map(lambda x: np.eye(7)[dt.strptime(x, '%Y-%m-%d %H:%M:%S').isoweekday()-1]).tolist()
day_of_week_test = test_df['general_firstup'].map(lambda x: np.eye(7)[dt.strptime(x, '%Y-%m-%d %H:%M:%S').isoweekday()-1]).tolist()

cols = [f"month_ohe_{i:02}" for i in range(12)] + [f"hour_ohe_{i:02}" for i in range(24)] + [f"day_of_week_ohe_{i}" for i in range(7)]

date_ohe_train = pd.DataFrame(np.concatenate([np.stack(month_train), np.stack(hour_train), np.stack(day_of_week_train)], axis=1), columns=cols)
date_ohe_test = pd.DataFrame(np.concatenate([np.stack(month_test), np.stack(hour_test), np.stack(day_of_week_test)], axis=1), columns=cols)

date_ohe_train.to_feather(f"{GCF.FEATURES_PATH}/train_date_ohe.ftr")
date_ohe_test.to_feather(f"{GCF.FEATURES_PATH}/test_date_ohe.ftr")

del month_train, month_test, hour_train, hour_test, day_of_week_train, day_of_week_test, date_ohe_train, date_ohe_test, cols
gc.collect()

"""### UserID"""

# 投稿が多いからと言ってスコアが高いわけではなさそう
#lst = []
#for userid, df in train_df.groupby('userid'):
#    d = len(df), df['fav_novel_cnt_bin'].max(), df['fav_novel_cnt_bin'].min(), df['fav_novel_cnt_bin'].mean(), df['fav_novel_cnt_bin'].median()
#    lst.append(d)
#pd.DataFrame(lst).plot.scatter(x=0, y=4)

def get_userid_idx(_id, dic):
    try:
        return dic[_id]
    except KeyError:
        return len(dic)

all_userid = test_df['userid'].tolist() + train_df['userid'].tolist()

dic = {}
idx = 0
for _id in set(all_userid):
    n = all_userid.count(_id)
    if n < 6:
        continue
    dic[_id] = idx
    idx += 1

userid_train = train_df['userid'].map(lambda x: get_userid_idx(x, dic))
userid_test = test_df['userid'].map(lambda x: get_userid_idx(x, dic))

userid_train = pd.DataFrame(userid_train.tolist(), columns=['userid_over5'])
userid_test = pd.DataFrame(userid_test.tolist(), columns=['userid_over5'])

userid_train.to_feather(f"{GCF.FEATURES_PATH}/train_userid_over5.ftr")
userid_test.to_feather(f"{GCF.FEATURES_PATH}/test_userid_over5.ftr")

del userid_train, userid_test, dic, idx, n, all_userid
gc.collect()

"""#### testのみでfix"""

test_userids = set(test_df['userid'].tolist())

dic = {}
idx = 0
for uid in test_userids:
    _df = train_df.query(f"userid=='{uid}'")
    if len(_df) == 0:
        continue
    dic[uid] = idx
    idx += 1

userid_train = train_df['userid'].map(lambda x: get_userid_idx(x, dic))
userid_test = test_df['userid'].map(lambda x: get_userid_idx(x, dic))

userid_train = pd.DataFrame(userid_train.tolist(), columns=['userid_only_test'])
userid_test = pd.DataFrame(userid_test.tolist(), columns=['userid_only_test'])

userid_train.to_feather(f"{GCF.FEATURES_PATH}/train_userid_only_test.ftr")
userid_test.to_feather(f"{GCF.FEATURES_PATH}/test_userid_only_test.ftr")

len(dic)

"""### userごとの特徴"""

def get_date_diff(df):
    _date = df['general_firstup'].map(lambda x: dt.strptime(x, "%Y-%m-%d %H:%M:%S"))
    dt_diff_prev = _date - _date.shift(1)
    dt_diff_next =  _date.shift(-1) - _date
    dt_diff_prev = dt_diff_prev.map(lambda x: x.days)
    dt_diff_next = dt_diff_next.map(lambda x: x.days)
    dt_diff_init = _date - _date.iloc[0]

    dic = {}
    for i in range(12):
        d = (i+1)*30
        dic[f"prev_over_{d}"] = dt_diff_prev > d
        dic[f"next_over_{d}"] = dt_diff_next > d
    date_diff = pd.DataFrame(dic).astype(int)
    date_diff['init_diff'] = dt_diff_init.map(lambda x: x.days)
    return date_diff

all_df = pd.concat([train_df, test_df]).reset_index(drop=True)
_df = get_date_diff(all_df)

_df.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_date_diff_v1.ftr")
_df.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_date_diff_v1.ftr")

"""#### 累積特徴（cumsum）"""

genre_ohe_train = pd.read_feather(f"{GCF.FEATURES_PATH}/train_genre_ohe.ftr")
genre_ohe_test = pd.read_feather(f"{GCF.FEATURES_PATH}/test_genre_ohe.ftr")
all_df = pd.concat([
    pd.concat([train_df, genre_ohe_train], axis=1),
    pd.concat([test_df, genre_ohe_test], axis=1),
], axis=0).reset_index(drop=True)

org_bin_feat = ['isr15', 'isbl', 'isgl', 'iszankoku', 'istensei', 'istenni']
genre_feat = genre_ohe_train.columns.tolist()
del genre_ohe_train, genre_ohe_test

feats = np.zeros((len(all_df), len(org_bin_feat) + len(genre_feat)))
for userid, df in all_df.groupby('userid'):
    feats[df.index, :] = df[org_bin_feat+genre_feat].cumsum().values
cumsum_feat_df = pd.DataFrame(feats, columns=org_bin_feat+genre_feat)
del feats

post_count = pd.DataFrame(zip([1 for _ in range(len(all_df))], all_df['userid'].tolist())).groupby(1).cumsum(0)
post_count.columns = ['post_count']
cumsum_feat_norm_df = pd.DataFrame(cumsum_feat_df.values / post_count.values, columns=[f"cumsum_{c}_norm"for c in org_bin_feat+genre_feat])
post_count_norm = post_count/post_count.max()
post_count_norm.columns = ['post_count_norm']

cumsum_feat_norm_df = pd.concat([post_count_norm, cumsum_feat_norm_df], axis=1)
del post_count_norm, post_count

cumsum_feat_norm_df.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_cumsum_v1.ftr")
cumsum_feat_norm_df.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_cumsum_v1.ftr")

del all_df, org_bin_feat, genre_feat, cumsum_feat_norm_df, cumsum_feat_df
gc.collect()

"""### text"""

def story_feat(df):
    story_len = df['story'].map(lambda x: len(x))/1000
    story_br = df['story'].map(lambda x: len(x.split('\n')))/100
    _df = pd.concat([story_len, story_br], axis=1)
    _df.columns = ['story_len', 'story_br']
    return _df

story_feat(train_df).to_feather(f"{GCF.FEATURES_PATH}/train_story_feat_v1.ftr")
story_feat(test_df).to_feather(f"{GCF.FEATURES_PATH}/test_story_feat_v1.ftr")

#N4771GY...円周率
#N4326HD...遺言

"""### Target encoding"""

all_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)

date_df = pd.DataFrame(all_df['general_firstup'].map(lambda x: x.split()[0].split('-')[:2]).tolist(), columns=['year', 'month'])
date_df.loc[date_df.query('year=="2021" and month=="09"').index, 'month'] = "08"
date_df['yyyymm'] = date_df.apply(lambda x: f"{x.year}_{x.month}", axis=1)

feat = np.zeros((len(all_df), 4)).astype(str)
for year, df in date_df.groupby('year'):
    m2 = pd.cut(df['month'].astype(int), [0, 2, 4, 6, 8, 10, 12], labels=False)
    m3 = pd.cut(df['month'].astype(int), [0, 3, 6, 9, 12], labels=False)
    m4 = pd.cut(df['month'].astype(int), [0, 4, 8, 12], labels=False)
    m6 = pd.cut(df['month'].astype(int), [0, 6, 12], labels=False)

    m2 = m2.map(lambda x: f"bin_{year}_{x}")
    m3 = m3.map(lambda x: f"bin_{year}_{x}")
    m4 = m4.map(lambda x: f"bin_{year}_{x}")
    m6 = m6.map(lambda x: f"bin_{year}_{x}")
    _df = pd.concat([m2, m3, m4, m6], axis=1)

    feat[df.index, :] = _df.values
bin_df = pd.DataFrame(feat, columns=['bin_2month', 'bin_3month', 'bin_4month', 'bin_6month'])

date_df = pd.concat([date_df, bin_df], axis=1)

kw_all.query('fav_novel_cnt_bin==4').sum().sort_values().tail(30)

"""#### KeyWords"""

keyword_ohe_train = pd.read_feather(f"{GCF.FEATURES_PATH}/train_kw_ohe_50_norm.ftr")
keyword_ohe_test = pd.read_feather(f"{GCF.FEATURES_PATH}/test_kw_ohe_50_norm.ftr")
kw_all = pd.concat([keyword_ohe_train, keyword_ohe_test]).reset_index(drop=True)
kw_all['fav_novel_cnt_bin'] = all_df['fav_novel_cnt_bin']

use_kws = kw_all[date_df['year'].map(lambda x: int(x) >= 2020)]
use_kws = use_kws.query('fav_novel_cnt_bin!=-1')
use_kws = use_kws[use_kws.columns[use_kws.sum() > 200].tolist()]

dfs = []
for c in use_kws.columns:
    if c == 'fav_novel_cnt_bin':
        continue
    df = pd.concat([
        kw_all[c].map(use_kws.groupby(c).mean()['fav_novel_cnt_bin'].to_dict()),
        kw_all[c].map(use_kws.groupby(c).median()['fav_novel_cnt_bin'].to_dict()),
        kw_all[c].map(use_kws.groupby(c).var()['fav_novel_cnt_bin'].to_dict()),
        kw_all[c].map(use_kws.groupby(c).std()['fav_novel_cnt_bin'].to_dict()),
        kw_all[c].map(use_kws.groupby(c).max()['fav_novel_cnt_bin'].to_dict())/5,
        kw_all[c].map(use_kws.groupby(c).min()['fav_novel_cnt_bin'].to_dict())/5,
        kw_all[c].map(use_kws.groupby(c).skew()['fav_novel_cnt_bin'].to_dict()),
    ], axis=1)
    df.columns= [f'cur_{c}_target_{i}' for i in range(df.shape[1])]
    dfs.append(df)

cur_kw_target = pd.concat(dfs, axis=1)

cur_kw_target.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_cur_kw_target_v1.ftr")
cur_kw_target.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_cur_kw_target_v1.ftr")

"""#### date"""

date_group_df = pd.concat([date_df, all_df[['fav_novel_cnt_bin']]], axis=1)

_date_group_df = date_group_df.query('fav_novel_cnt_bin!=-1')

dfs = []
for col in ['year', 'yyyymm', 'bin_2month', 'bin_3month', 'bin_4month', 'bin_6month']:
    dic = _date_group_df.groupby(col).mean()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(date_group_df[col].map(dic).values, columns=[f"{col}_mean"])
    dfs.append(_df)

    dic = _date_group_df.groupby(col).std()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(date_group_df[col].map(dic).values, columns=[f"{col}_std"])
    dfs.append(_df)

    dic = _date_group_df.groupby(col).var()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(date_group_df[col].map(dic).values, columns=[f"{col}_var"])
    dfs.append(_df)

    dic = _date_group_df.groupby(col).skew()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(date_group_df[col].map(dic).values, columns=[f"{col}_skew"])
    dfs.append(_df)

    dic = _date_group_df.groupby(col).median()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(date_group_df[col].map(dic).values, columns=[f"{col}_median"])
    dfs.append(_df)

date_target_df = pd.concat(dfs, axis=1).fillna(0)

date_target_df.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_date_target_v1.ftr")
date_target_df.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_date_target_v1.ftr")



"""#### UserID"""

pseudo = pd.read_csv("/content/drive/MyDrive/Study/Nishika/result/exp099_mlp2.csv")
pseudo_test_df = test_df
pseudo_test_df['fav_novel_cnt_bin'] = pseudo.values[:, 1:].argmax(1)
pseudo_all_df = pd.concat([train_df, pseudo_test_df], axis=0).reset_index(drop=True)

def get_cumsum(target):
    res = []
    lst = []
    for i, v in enumerate(target):
        if v == -1:
            max_v, min_v, avg_v, med_v, std_v, var_v = -1, -1, -1, -1, -1, -1
        else:
            lst.append(v)
            max_v = max(lst) / 5
            min_v = min(lst) / 5
            avg_v = sum(lst)/len(lst)
            med_v = np.median(lst)
            std_v = np.std(lst)
            var_v = np.var(lst)
        res.append({
            'max_v': max_v,
            'min_v': min_v,
            'avg_v': avg_v,
            'med_v': med_v,
            'std_v': std_v,
            'var_v': var_v,
        })
    return pd.DataFrame(res)

target_feat = np.zeros((len(all_df), 9))
#for userid, df in tqdm(all_df.groupby('userid'), total=len(all_df['userid'].unique())):
for userid, df in tqdm(pseudo_all_df.groupby('userid'), total=len(pseudo_all_df['userid'].unique())):
    #_df = pd.concat([df['fav_novel_cnt_bin'].shift(1), df['fav_novel_cnt_bin'].shift(2), df['fav_novel_cnt_bin'].shift(3)], axis=1).fillna(-2)
    _df = pd.concat([df['fav_novel_cnt_bin'].shift(1), df['fav_novel_cnt_bin'].shift(2), df['fav_novel_cnt_bin'].shift(3)], axis=1).fillna(-1)
    #_df = _df.applymap(lambda x: None if x == -1 else x).fillna(method='ffill').applymap(lambda x: -1 if x == -2 else x).fillna(method='ffill')
    _df.columns = ['target_1', 'target_2', 'target_3']
    cumcum_df = get_cumsum(_df['target_1'])
    _feat = pd.concat([_df.reset_index(drop=True), cumcum_df], axis=1)
    target_feat[df.index, :] = _feat.values
user_target_feat_df = pd.DataFrame(target_feat, columns=_feat.columns)

#user_target_feat_df.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_user_target_v1.ftr")
#user_target_feat_df.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_user_target_v1.ftr")
user_target_feat_df.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_user_target_pseudo_v1.ftr")
user_target_feat_df.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_user_target_pseudo_v1.ftr")

del user_target_feat_df, target_feat
gc.collect()

"""#### window"""

def get_window_target(_target, w):
    d = {
        f"w{w}_mean": _target.rolling(w).mean().fillna(-1),
        f"w{w}_max": _target.rolling(w).max().fillna(-1),
        f"w{w}_min": _target.rolling(w).min().fillna(-1),
        f"w{w}_median": _target.rolling(w).median().fillna(-1),
        f"w{w}_var": _target.rolling(w).var().fillna(-1),
        f"w{w}_std": _target.rolling(w).std().fillna(-1),
        f"w{w}_skew": _target.rolling(w).skew().fillna(0),
        f"w{w}_kurt": _target.rolling(w).kurt().fillna(0),
    }
    return pd.DataFrame(d)

target_feat = np.zeros((len(all_df), 40))
#for userid, df in tqdm(all_df.groupby('userid'), total=len(all_df['userid'].unique())):
for userid, df in tqdm(pseudo_all_df.groupby('userid'), total=len(pseudo_all_df['userid'].unique())):
    #_target = df['fav_novel_cnt_bin'].map(lambda x: None if x == -1 else x).fillna(method='ffill')
    _target = df['fav_novel_cnt_bin']
    _target = _target.shift(1)
    window_feat = pd.concat([
           get_window_target(_target, 2),
           get_window_target(_target, 3),
           get_window_target(_target, 4),
           get_window_target(_target, 5),
           get_window_target(_target, 6),
    ], axis=1)
    target_feat[df.index, :] = window_feat.values

user_target_feat_df = pd.DataFrame(target_feat, columns=window_feat.columns)

#user_target_feat_df.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_user_target_v2_leak_fix.ftr")
#user_target_feat_df.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_user_target_v2_leak_fix.ftr")
user_target_feat_df.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_user_window_pseudo_v1.ftr")
user_target_feat_df.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_user_window_pseudo_v1.ftr")

"""#### カテゴリ"""

cate_df = pd.concat([date_df, all_df[['genre', 'fav_novel_cnt_bin']]], axis=1)

cate_df['yyyy_genre'] = cate_df.apply(lambda x: f"{x.year}_{x.genre}",axis=1)
cate_df['yyyymm_genre'] = cate_df.apply(lambda x: f"{x.year}{x.month}_{x.genre}",axis=1)
cate_df['bin_2month_genre'] = cate_df.apply(lambda x: f"{x.bin_2month}_{x.genre}",axis=1)
cate_df['bin_3month_genre'] = cate_df.apply(lambda x: f"{x.bin_3month}_{x.genre}",axis=1)
cate_df['bin_4month_genre'] = cate_df.apply(lambda x: f"{x.bin_4month}_{x.genre}",axis=1)
cate_df['bin_6month_genre'] = cate_df.apply(lambda x: f"{x.bin_6month}_{x.genre}",axis=1)

_cate_df = cate_df.query('fav_novel_cnt_bin!=-1')

dfs = []
for col in ['genre', 'yyyy_genre', 'yyyymm_genre', 'bin_2month_genre', 'bin_3month_genre', 'bin_4month_genre', 'bin_6month_genre']:
    dic = _cate_df.groupby(col).mean()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(cate_df[col].map(dic).values, columns=[f"{col}_mean"])
    dfs.append(_df)

    dic = _cate_df.groupby(col).std()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(cate_df[col].map(dic).values, columns=[f"{col}_std"])
    dfs.append(_df)

    dic = _cate_df.groupby(col).var()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(cate_df[col].map(dic).values, columns=[f"{col}_var"])
    dfs.append(_df)

    dic = _cate_df.groupby(col).skew()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(cate_df[col].map(dic).values, columns=[f"{col}_skew"])
    dfs.append(_df)

    dic = _cate_df.groupby(col).median()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(cate_df[col].map(dic).values, columns=[f"{col}_median"])
    dfs.append(_df)

genre_target_df = pd.concat(dfs, axis=1).fillna(0)

genre_target_df.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_genre_target_v2.ftr")
genre_target_df.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_genre_target_v2.ftr")

"""#### キーワード LDA"""

train_kw_lda_50 = pd.read_feather(f"{GCF.FEATURES_PATH}/train_kw_lda_50_norm.ftr")
test_kw_lda_50 = pd.read_feather(f"{GCF.FEATURES_PATH}/test_kw_lda_50_norm.ftr")
lda_all_df = pd.concat([train_kw_lda_50, test_kw_lda_50], axis=0).reset_index(drop=True)

thr = 0.2
lda_thr_df = pd.DataFrame(lda_all_df > thr)
lda_thr_df.columns = [f"{c}_t{thr}" for c in lda_thr_df.columns]

date_lda_df = pd.concat([date_df, lda_thr_df], axis=1)

dfs = []
for col in lda_thr_df.columns:
    dfs.append(date_lda_df.apply(lambda x: f'{x.year}_{x[col]}', axis=1))
    dfs.append(date_lda_df.apply(lambda x: f'{x.yyyymm}_{x[col]}', axis=1))
    dfs.append(date_lda_df.apply(lambda x: f'{x.bin_2month}_{x[col]}', axis=1))
    dfs.append(date_lda_df.apply(lambda x: f'{x.bin_3month}_{x[col]}', axis=1))
    dfs.append(date_lda_df.apply(lambda x: f'{x.bin_4month}_{x[col]}', axis=1))
    dfs.append(date_lda_df.apply(lambda x: f'{x.bin_6month}_{x[col]}', axis=1))

lda_date_for_group = pd.concat(dfs, axis=1)

date_lda_df = pd.concat([date_df, lda_thr_df, all_df[['fav_novel_cnt_bin']]], axis=1)
lda_date_for_group['fav_novel_cnt_bin'] = all_df['fav_novel_cnt_bin']
_lda_date_for_group = lda_date_for_group.query('fav_novel_cnt_bin!=-1')

dfs = []
for col in tqdm(range(120)):
    dic = _lda_date_for_group.groupby(col).mean()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(lda_date_for_group[col].map(dic).values, columns=[f"{col}_mean"])
    dfs.append(_df)

    dic = _lda_date_for_group.groupby(col).std()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(lda_date_for_group[col].map(dic).values, columns=[f"{col}_std"])
    dfs.append(_df)

    dic = _lda_date_for_group.groupby(col).var()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(lda_date_for_group[col].map(dic).values, columns=[f"{col}_var"])
    dfs.append(_df)

    dic = _lda_date_for_group.groupby(col).skew()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(lda_date_for_group[col].map(dic).values, columns=[f"{col}_skew"])
    dfs.append(_df)

    dic = _lda_date_for_group.groupby(col).median()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(lda_date_for_group[col].map(dic).values, columns=[f"{col}_median"])
    dfs.append(_df)
lda_target_feat = pd.concat(dfs, axis=1).fillna(0)
lda_target_feat.columns = [f"lda_thr{thr}_{c}" for c in lda_target_feat.columns]

#lda_target_feat.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_lda_thr00.ftr")
#lda_target_feat.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_lda_thr00.ftr")
lda_target_feat.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_lda_thr02.ftr")
lda_target_feat.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_lda_thr02.ftr")

"""#### bin feat"""

bin_ferat_df = pd.concat([date_df, all_df[['novel_type', 'isr15', 'isbl', 'isgl', 'iszankoku', 'istensei', 'istenni', 'fav_novel_cnt_bin']]], axis=1)

bin_cols = ['novel_type', 'isr15', 'isbl', 'isgl', 'iszankoku', 'istensei', 'istenni',]
dfs = []
for col in bin_cols:
    dfs.append(bin_ferat_df.apply(lambda x: f'{x.year}_{x[col]}', axis=1))
    dfs.append(bin_ferat_df.apply(lambda x: f'{x.yyyymm}_{x[col]}', axis=1))
    dfs.append(bin_ferat_df.apply(lambda x: f'{x.bin_2month}_{x[col]}', axis=1))
    dfs.append(bin_ferat_df.apply(lambda x: f'{x.bin_3month}_{x[col]}', axis=1))
    dfs.append(bin_ferat_df.apply(lambda x: f'{x.bin_4month}_{x[col]}', axis=1))
    dfs.append(bin_ferat_df.apply(lambda x: f'{x.bin_6month}_{x[col]}', axis=1))
bin_feat_for_group = pd.concat(dfs, axis=1)

bin_feat_for_group['fav_novel_cnt_bin'] = all_df['fav_novel_cnt_bin']
_bin_feat_for_group = bin_feat_for_group.query('fav_novel_cnt_bin!=-1')

dfs = []
for col in tqdm(range(42)):
    dic = _bin_feat_for_group.groupby(col).mean()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(bin_feat_for_group[col].map(dic).values, columns=[f"{col}_mean"])
    dfs.append(_df)

    dic = _bin_feat_for_group.groupby(col).std()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(bin_feat_for_group[col].map(dic).values, columns=[f"{col}_std"])
    dfs.append(_df)

    dic = _bin_feat_for_group.groupby(col).var()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(bin_feat_for_group[col].map(dic).values, columns=[f"{col}_var"])
    dfs.append(_df)

    dic = _bin_feat_for_group.groupby(col).skew()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(bin_feat_for_group[col].map(dic).values, columns=[f"{col}_skew"])
    dfs.append(_df)

    dic = _bin_feat_for_group.groupby(col).median()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(bin_feat_for_group[col].map(dic).values, columns=[f"{col}_median"])
    dfs.append(_df)
bin_target_feat = pd.concat(dfs, axis=1).fillna(0)
bin_target_feat.columns = [f"bin_feat_{c}" for c in bin_target_feat.columns]

bin_feat_for_group

bin_target_feat.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_bin_target.ftr")
bin_target_feat.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_bin_target.ftr")



"""## 学習"""

X_cate = pd.concat([pd.read_feather(f"{GCF.FEATURES_PATH}/train_{i}.ftr") for i in GCF.FEATURES], axis=1)
X_cate_test = pd.concat([pd.read_feather(f"{GCF.FEATURES_PATH}/test_{i}.ftr") for i in GCF.FEATURES], axis=1)

# year
X_cate['upload_year_norm'] = X_cate['upload_year'].map(lambda x: (x-2007)/14)
X_cate_test['upload_year_norm'] = X_cate_test['upload_year'].map(lambda x: (x-2007)/14)
# monthy
X_cate['month_sin'] = np.sin(2 * np.pi * X_cate['upload_month']/12)
X_cate['month_cos'] = np.cos(2 * np.pi * X_cate['upload_month']/12)
X_cate_test['month_sin'] = np.sin(2 * np.pi * X_cate_test['upload_month']/12)
X_cate_test['month_cos'] = np.cos(2 * np.pi * X_cate_test['upload_month']/12)
# hour
X_cate['hour_sin'] = np.sin(2 * np.pi * X_cate['hour']/24)
X_cate['hour_cos'] = np.cos(2 * np.pi * X_cate['hour']/24)
X_cate_test['hour_sin'] = np.sin(2 * np.pi * X_cate_test['hour']/24)
X_cate_test['hour_cos'] = np.cos(2 * np.pi * X_cate_test['hour']/24)
# day_of_week
X_cate['day_of_week_sin'] = np.sin(2 * np.pi * X_cate['day_of_week']/7)
X_cate['day_of_week_cos'] = np.cos(2 * np.pi * X_cate['day_of_week']/7)
X_cate_test['day_of_week_sin'] = np.sin(2 * np.pi * X_cate_test['day_of_week']/7)
X_cate_test['day_of_week_cos'] = np.cos(2 * np.pi * X_cate_test['day_of_week']/7)

# カテゴリ変数
# userid
#userid_columns = 'userid_over5'
userid_columns = 'userid_only_test'

userid_train = X_cate[userid_columns]
userid_test = X_cate_test[userid_columns]
n_userid = len(set(userid_train.tolist() + userid_test.tolist()))
# カテゴリ数の辞書
n_cate_dic = {
    'n_userid': n_userid,
}

# 不要カラム削除
drop_features = ['upload_year', 'upload_month', 'hour', 'day_of_week', userid_columns] + ['target_1', 'target_2', 'target_3'] 
X_cate = X_cate.drop(drop_features, axis=1)
X_cate_test = X_cate_test.drop(drop_features, axis=1)

fav_novel_cnt_bin = train_df['fav_novel_cnt_bin'].values

X_cate[userid_columns] = userid_train
X_cate_test[userid_columns] = userid_test

#nn_output_train = np.load(f"{GCF.FEATURES_PATH}/nn_output_exp092_train.npy")
#nn_output_test = np.load(f"{GCF.FEATURES_PATH}/nn_output_exp092_test.npy")
#nn_output_train_df = pd.DataFrame(nn_output_train, columns=[f"nn_{i:04}" for i in range(1792)])
#nn_output_test_df = pd.DataFrame(nn_output_test, columns=[f"nn_{i:04}" for i in range(1792)])

X_cate = pd.concat([
                    X_cate,
                    #nn_output_train_df,
                    train_df[['genre', 'biggenre']]
], axis=1) #, 'keyword']]], axis=1)
X_cate_test = pd.concat([
                    X_cate_test,
                    #nn_output_test_df,
                    test_df[['genre', 'biggenre']]
    ], axis=1) #, 'keyword']]], axis=1)

#X_cate['keyword'] = X_cate['keyword'].fillna("")
#X_cate_test['keyword'] = X_cate_test['keyword'].fillna("")

category = ['genre', 'biggenre', userid_columns]
#text_feat = ['keyword']

X_cont = X_cate.drop(category, axis=1)
X_cont_test = X_cate_test.drop(category, axis=1)

pca = PCA(n_components=100)
pca.fit(X_cont)

pca_feat = pd.DataFrame(pca.transform(X_cont), columns=[f'pca{i:02}' for i in range(100)])
pca_feat_test = pd.DataFrame(pca.transform(X_cont_test), columns=[f'pca{i:02}' for i in range(100)])

bert_vector_test = np.load(f"{GCF.FEATURES_PATH}/bert_vector_test.npy")
bert_vector_train = np.load(f"{GCF.FEATURES_PATH}/bert_vector_train.npy")

X_cate = pd.concat([X_cate, pd.DataFrame(bert_vector_train, columns=[f"bert_{i}" for i in range(768)]) ], axis=1)
X_cate_test = pd.concat([X_cate_test, pd.DataFrame(bert_vector_test, columns=[f"bert_{i}" for i in range(768)])], axis=1)

X_cate = pd.concat([X_cate, pca_feat], axis=1)
X_cate_test = pd.concat([X_cate_test, pca_feat_test], axis=1)

"""### LGBM"""

params = {
    'objective': 'multiclass',
    'num_classes': 5,
    'metric': 'multi_logloss',
    'num_leaves': 5,
    'max_depth': 7,
    "feature_fraction": 0.9,
    'subsample_freq': 1,
    "bagging_fraction": 0.8,
    'min_data_in_leaf': 20,
    'learning_rate': 0.003,
    "boosting": "gbdt",
    "lambda_l1": 0.1,
    "lambda_l2": 0.1,
    "verbosity": -1,
    "random_state": 42,
    "num_boost_round": 50000,
    "early_stopping_rounds": 100
}

feature_imp_lst = []
predicts = []
oof = np.zeros((len(fav_novel_cnt_bin), 5))
skf = StratifiedKFold(n_splits=GCF.N_FOLDS, random_state=GCF.SEED, shuffle=True).split(X_cate, fav_novel_cnt_bin)
for fold, (train_index, valid_index) in enumerate(skf):
    print(f"Fold-{fold}")

    X_train = X_cate.loc[train_index, :]
    X_valid = X_cate.loc[valid_index, :]
    y_train = fav_novel_cnt_bin[train_index]
    y_valid = fav_novel_cnt_bin[valid_index]

    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_valid, label=y_valid)

    model = lgb.train(
        params,
        train_data, 
        categorical_feature = category,
        valid_names = ['train', 'valid'],
        valid_sets =[train_data, valid_data], 
        verbose_eval = 100,
    )

    feature_imp = pd.DataFrame(sorted(zip(model.feature_importance(), X_cate.columns)), columns=['importance', 'feature'])
    feature_imp_lst.append(feature_imp)

    pred_valid = model.predict(X_valid, num_iteration=model.best_iteration)
    oof[valid_index] = pred_valid

    pred_test = model.predict(X_cate_test, num_iteration=model.best_iteration)
    predicts.append(pred_test)

oof_score = log_loss(np.stack([np.eye(5)[i] for i in fav_novel_cnt_bin]).astype(int), oof)
print(f"OOF score = {oof_score}")

"""
- Baseline: 7467963794548287
- SAME BERT: 0.7461979531840308
- LDA: 0.7445166143379154
- -lda_thr00:　0.749698387566938
- +window: 0.744748767722195

- lr=0.03: 0.7432655956131774

- tune baseline: 0.7468310817672308
- lr=0.01: 0.7430740488551509


- PCA: 0.7502898999184148
- leaf 45: 0.745840240107752
- leaf 20: 0.7431842663631871
- feature frac 0.7: 0.7433380690098762 
- feature frac 0.9:　0.7424653338531051 
- leaf 10: 0.7418907444044353
- leaf 5: 0.7413953239596401
- bagging f 0.8:　0.7410196941592777
- bagging f 0.7:　0.7429743395034268
- lambda 2: 0.7433872459817962
- depth 9: 0.7410196941592777
- bert: 0.7379432960475678"""

predicts_avg = sum(predicts)/5
sub_df[["proba_0","proba_1","proba_2","proba_3","proba_4"]] = predicts_avg

!mkdir -p /content/drive/MyDrive/Study/Nishika/models/exp125_lgbm_now_best
sub_df.to_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp125_lgbm_now_best/lgbm_test_preds.csv", index=None)

oof_df = pd.DataFrame(oof, columns=["proba_0","proba_1","proba_2","proba_3","proba_4"])
oof_df['ncode'] = train_df['ncode']
oof_df.to_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp125_lgbm_now_best/lgbm_oof_preds.csv", index=None)

lgb.plot_importance(model, figsize=(12,8), max_num_features=50, importance_type='gain')
plt.tight_layout()
plt.show()

"""### CatBoost"""

!pip install catboost ipywidgets
!jupyter nbextension enable --py widgetsnbextension

from catboost import Pool
from catboost import CatBoostClassifier

from google.colab import output
output.enable_custom_widget_manager()
#output.disable_custom_widget_manager()

feature_imp_lst = []
predicts = []
oof = np.zeros((len(fav_novel_cnt_bin), 5))
skf = StratifiedKFold(n_splits=GCF.N_FOLDS, random_state=GCF.SEED, shuffle=True).split(X_cate, fav_novel_cnt_bin)
for fold, (train_index, valid_index) in enumerate(skf):
    print(f"Fold-{fold}")

    X_train = X_cate.loc[train_index, :]
    X_valid = X_cate.loc[valid_index, :]
    y_train = fav_novel_cnt_bin[train_index]
    y_valid = fav_novel_cnt_bin[valid_index]

    train_pool = Pool(X_train, y_train, cat_features=category) #, text_features=text_feat)
    valid_pool = Pool(X_valid, y_valid, cat_features=category) #, text_features=text_feat)

    model = CatBoostClassifier(
    random_seed=42,
    verbose=100,
    #learning_rate=3e-2,
    #depth=10,
    )
    model.fit(train_pool,
            eval_set=valid_pool,    # 検証用データ
            early_stopping_rounds=10,  # 10回以上精度が改善しなければ中止
            use_best_model=True,       # 最も精度が高かったモデルを使用するかの設定
            plot=True)

    pred_valid = pred_valid = model.predict(X_valid, prediction_type='Probability')
    oof[valid_index] = pred_valid
        
    pred_test = model.predict(X_cate_test, prediction_type='Probability')
    predicts.append(pred_test)

    feature_imp = pd.DataFrame(sorted(zip(model.get_feature_importance(valid_pool, type="PredictionValuesChange"), X_cate.columns)),
                               columns=['importance', 'feature'])
    feature_imp_lst.append(feature_imp)

oof_score = log_loss(np.stack([np.eye(5)[i] for i in fav_novel_cnt_bin]).astype(int), oof)
print(f"OOF score = {oof_score}")

predicts_avg = sum(predicts)/5
sub_df[["proba_0","proba_1","proba_2","proba_3","proba_4"]] = predicts_avg

!mkdir -p /content/drive/MyDrive/Study/Nishika/models/exp126_cat_now_best
sub_df.to_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp126_cat_now_best/cat_test_preds.csv", index=None)

oof_df = pd.DataFrame(oof, columns=["proba_0","proba_1","proba_2","proba_3","proba_4"])
oof_df['ncode'] = train_df['ncode']
oof_df.to_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp126_cat_now_best/cat_oof_preds.csv", index=None)

feats = []
for feature_imp in feature_imp_lst:
    feats += feature_imp.query('importance==0')['feature'].tolist()
counts = pd.DataFrame(feats)[0].value_counts()

unuseful_feat = counts[counts == 5].index.tolist()

import pickle

#with open(f"{GCF.FEATURES_PATH}/unuse_feat_v1.pkl", "wb") as f:
#    pickle.dump(unuseful_feat, f)

feature_imp.tail(30)

"""### XGBoost"""

import xgboost as xgb

params = {
        'objective': 'multi:softprob',
        'silent':1,
        'random_state':42, 
        'eval_metric': 'mlogloss',
        'num_class': 5,
        'booster': 'dart',
        'alpha': 1,
    }
num_round = 500

predicts = []
oof = np.zeros((len(fav_novel_cnt_bin), 5))
skf = StratifiedKFold(n_splits=GCF.N_FOLDS, random_state=GCF.SEED, shuffle=True).split(X_cate, fav_novel_cnt_bin)
for fold, (train_index, valid_index) in enumerate(skf):
    print("Fold:", fold)
    X_train = X_cate.loc[train_index, :]
    X_valid = X_cate.loc[valid_index, :]
    y_train = fav_novel_cnt_bin[train_index]
    y_valid = fav_novel_cnt_bin[valid_index]
    #y_train = np.stack([np.eye(5)[i] for i in fav_novel_cnt_bin[train_index]])
    #y_valid = np.stack([np.eye(5)[i] for i in fav_novel_cnt_bin[valid_index]])

    dtrain = xgb.DMatrix(X_train, label=y_train)
    dvalid = xgb.DMatrix(X_valid, label=y_valid)
    dtest = xgb.DMatrix(X_cate_test)


    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]#訓練データはdtrain、評価用のテストデータはdvalidと設定

    model = xgb.train(params,
                        dtrain,#訓練データ
                        num_round,#設定した学習回数
                        early_stopping_rounds=20,
                        evals=watchlist,
                        )

    y_pred_valid = model.predict(dvalid, ntree_limit = model.best_ntree_limit)
    #score = log_loss(np.stack([np.eye(5)[i] for i in fav_novel_cnt_bin[valid_index]]), y_pred_valid)
    #print(f"fold-{fold}: {score}")
    oof[valid_index, :] = y_pred_valid
    y_pred_test = model.predict(dtest, ntree_limit=model.best_ntree_limit)
    predicts.append(y_pred_test)

oof_score = log_loss(np.stack([np.eye(5)[i] for i in fav_novel_cnt_bin]), oof)
print("OOF:", oof_score)

predicts_avg = sum(predicts)/5
sub_df[["proba_0","proba_1","proba_2","proba_3","proba_4"]] = predicts_avg

!mkdir -p /content/drive/MyDrive/Study/Nishika/models/exp115_xgb_now_best
sub_df.to_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp115_xgb_now_best/xgb_test_preds.csv", index=None)

oof_df = pd.DataFrame(oof, columns=["proba_0","proba_1","proba_2","proba_3","proba_4"])
oof_df['ncode'] = train_df['ncode']
oof_df.to_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp115_xgb_now_best/xgb_oof_preds.csv", index=None)

"""## Stacking"""

oof_xgb = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp115_xgb_now_best/xgb_oof_preds.csv")
oof_lgbm = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp108_lgbm_now_best/lgbm_oof_preds.csv")
oof_cat = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp109_cat_now_best/cat_oof_preds.csv")
oof_nn = np.load(f"{GCF.FEATURES_PATH}/nn_output_exp107_train.npy")
oof_nn2 = np.load(f"{GCF.FEATURES_PATH}/nn_output_exp113_cls_token_train.npy")
oof_nn3 = np.load(f"{GCF.FEATURES_PATH}/nn_output_exp114_cls_token_train.npy")

pred_xgb = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp115_xgb_now_best/xgb_test_preds.csv")
pred_lgbm = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp108_lgbm_now_best/lgbm_test_preds.csv")
pred_cat = pd.read_csv(f"/content/drive/MyDrive/Study/Nishika/models/exp109_cat_now_best/cat_test_preds.csv")
pred_nn = np.load(f"{GCF.FEATURES_PATH}/nn_output_exp107_test.npy")
pred_nn2 = np.load(f"{GCF.FEATURES_PATH}/nn_output_exp113_cls_token_test.npy")
pred_nn3 = np.load(f"{GCF.FEATURES_PATH}/nn_output_exp114_cls_token_test.npy")

X = np.hstack([
               oof_lgbm.values[:, :-1],
               oof_cat.values[:, :-1],
               #oof_xgb.values[:, :-1],
               #oof_nn,
               #oof_nn2,
               oof_nn3,
])
#y = np.array([np.eye(5)[i] for i in train_df['fav_novel_cnt_bin'].values])
y = train_df['fav_novel_cnt_bin'].values
X_test = np.hstack([
                    pred_lgbm.values[:, 1:],
                    pred_cat.values[:, 1:],
                    #pred_xgb.values[:, 1:],
                    #pred_nn,
                    #pred_nn2,
                    pred_nn3,
])

oof = np.zeros((len(fav_novel_cnt_bin), 5))
test_predicts = []
skf = StratifiedKFold(n_splits=GCF.N_FOLDS, random_state=GCF.SEED, shuffle=True).split(X, fav_novel_cnt_bin)
for fold, (train_index, valid_index) in enumerate(skf):
    #clf = GaussianNB()
    clf = LogisticRegression(max_iter=1000)
    #clf = RandomForestClassifier()
    clf.fit(X[train_index, :], y[train_index])
    y_pred_valid = clf.predict_proba(X[valid_index, :])
    score = log_loss(np.stack([np.eye(5)[i] for i in y[valid_index]]), y_pred_valid)
    print(f"fold-{fold}: {score}")
    oof[valid_index, :] = y_pred_valid
    y_pred_test = clf.predict_proba(X_test)
    test_predicts.append(y_pred_test)

oof_score = log_loss(np.stack([np.eye(5)[i] for i in y]), oof)
print("OOF:", oof_score)

sub_df[["proba_0","proba_1","proba_2","proba_3","proba_4"]] = np.stack(test_predicts).mean(0)
#sub_df[["proba_0","proba_1","proba_2","proba_3","proba_4"]] = np.median(np.stack(test_predicts), axis=0)

sub_df.to_csv('Stacking_107_108_114_mean.csv', index=None)

sub_df[sub_df.values[:, 1:].max(1) > 0.9]

"""## Pseudo"""

pseudo = pd.read_csv("/content/drive/MyDrive/Study/Nishika/result/exp107_userid_fix.csv")

pd.DataFrame(zip(pseudo[pseudo.values[:, 1:].max(1) > 0.9]['ncode'].values, pseudo[pseudo.values[:, 1:].max(1) > 0.9].values[:, 1:].argmax(1)))

"""## PostProcessing"""

#sub1 = pd.read_csv("/content/drive/MyDrive/Study/Nishika/result/exp092_mlp1024.csv")
pseudo = pd.read_csv("/content/drive/MyDrive/Study/Nishika/result/exp099_mlp2.csv")

pseudo[pseudo.values[:, 1:].max(1)>0.9].values[:, 1:].argmax(1)

#sub2[sub1.values[:, 1:].argmax(1) != sub2.values[:, 1:].argmax(1)]

#sub1[sub1.values[:, 1:].argmax(1) != sub2.values[:, 1:].argmax(1)]

lst = []
for row in sub.values[4:, 1:]:
    idx = row.argmax()
    #if idx == 4:
    #if row[idx] > 0.9:
    #    row[idx] = 1.0
    #lst.append(row)
    break

plt.plot(row)

sub[['proba_0', 'proba_1', 'proba_2', 'proba_3', 'proba_4']] = np.array(lst)

#sub.to_csv('exp092_mlp1024_pp01.csv', index=None)

(pd.read_csv('exp092_mlp1024_pp01.csv').values[:, 1:] - pd.read_csv('exp092_mlp1024_pp02.csv').values[:, 1:]).mean()



