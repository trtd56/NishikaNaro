# -*- coding: utf-8 -*-
"""なろう.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ejGshCAaZRqdj_ZW_7dWAcaH7j1GUupU

# 小説家になろう ブクマ数予測 \~”伸びる”タイトルとは？\~

## Memo
- test
    - yearは2021のみ
    - monthは8, 9月のみ
    - genreは9801が無い
    - biggenreは98が無い
- マルチモーダルの学習率を変える
- 短編と長編でブックマーク数の分布が違いそう
- pseudo labeling
- target encording
- roberaやwholeモデルにする（→https://huggingface.co/cl-tohoku　）
"""

!nvidia-smi

from google.colab import drive
drive.mount('/content/drive')

#!apt-get -y install fonts-ipafont-gothic
#!rm /root/.cache/matplotlib/fontlist-v310.json
!pip install transformers unidic-lite fugashi
!pip install -U torch

"""## 共通設定"""

import gc
import os
import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from datetime import datetime as dt
from tqdm.notebook import tqdm
from collections import defaultdict
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss

# Models
import lightgbm as lgb
from sklearn.linear_model import LogisticRegression

# Transformer
import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader

from transformers import get_cosine_schedule_with_warmup
from transformers import AutoConfig
from transformers import AutoTokenizer
from transformers import AutoModel
from transformers import AdamW

device = torch.device("cuda")
scaler = torch.cuda.amp.GradScaler()

class GCF:
    EXP_NAME = 'exp024_date_ohe'

    INPUT_PATH = "/content/drive/MyDrive/Study/Nishika"
    FEATURES_PATH = f"{INPUT_PATH}/features"
    RESULT_PATH = f"{INPUT_PATH}/result"
    MODELS_PATH = f"{INPUT_PATH}/models/{EXP_NAME}"

    N_FOLDS = 5
    SEED = 0

    FEATURES = [
        "pc_or_k",
        "org_bin",
        "genre_ohe", "biggenre_ohe",
        #"genre_cate",
        "kw_ohe_100",
        "yyyymm",
        #"date2",
        "date_ohe",
        "userid_over5",
    ]
    CATEGORY = [
        "userid_over5"
    ]

    MODEL_NAME = "cl-tohoku/bert-base-japanese-v2"
    TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME)
    MAX_LEN = 256

    BS = 16
    N_USE_LAYER = 1
    LR = 1e-3
    WEIGHT_DECAY = 1e-3
    N_EPOCHS = 4
    ACCUMULATE = 2
    WARM_UP_RATIO = 0.1
    MLP_HIDDEN = 512
    PATIENT = 8

train_df = pd.read_csv(f"{GCF.INPUT_PATH}/train.csv")
test_df = pd.read_csv(f"{GCF.INPUT_PATH}/test.csv")
sub_df = pd.read_csv(f"{GCF.INPUT_PATH}/sample_submission.csv")

"""## 特徴抽出

### PCか携帯か
"""

pc_or_k_dic = {0:[0, 0], 1:[1, 0], 2:[0, 1], 3:[1, 1]}
pc_or_k_train = pd.DataFrame(train_df['pc_or_k'].map(pc_or_k_dic).tolist(), columns=['pc', 'keitai'])
pc_or_k_test = pd.DataFrame(test_df['pc_or_k'].map(pc_or_k_dic).tolist(), columns=['pc', 'keitai'])

pc_or_k_train.to_feather(f"{GCF.FEATURES_PATH}/train_pc_or_k.ftr")
pc_or_k_test.to_feather(f"{GCF.FEATURES_PATH}/test_pc_or_k.ftr")

del pc_or_k_test, pc_or_k_train, pc_or_k_dic
gc.collect()

"""### オリジナル2値データ

- 元データそのままの2値データ
- endとisstopはすべての値が0なので無視する
"""

org_bin_feat = ['isr15', 'isbl', 'isgl', 'iszankoku', 'istensei', 'istenni']
org_bin_train = train_df[org_bin_feat]
org_bin_test = test_df[org_bin_feat]

org_bin_train = pd.concat([org_bin_train, train_df[['novel_type']] - 1], axis=1)
org_bin_test = pd.concat([org_bin_test, test_df[['novel_type']] - 1], axis=1)

org_bin_train.to_feather(f"{GCF.FEATURES_PATH}/train_org_bin.ftr")
org_bin_test.to_feather(f"{GCF.FEATURES_PATH}/test_org_bin.ftr")

del org_bin_train, org_bin_test, org_bin_feat
gc.collect()

"""### ジャンル

#### 単純なOne-Hot Encoding

##### ジャンル
"""

oe = OneHotEncoder(sparse=False, dtype = int)
genre_train = oe.fit_transform(train_df[['genre']])
genre_test = oe.transform(test_df[['genre']])

col = [f"genre_{i}" for i in oe.categories_[0]]

genre_train = pd.DataFrame(genre_train, columns=col)
genre_test = pd.DataFrame(genre_test, columns=col)

genre_train.to_feather(f"{GCF.FEATURES_PATH}/train_genre_ohe.ftr")
genre_test.to_feather(f"{GCF.FEATURES_PATH}/test_genre_ohe.ftr")

del genre_train, genre_test, col, oe
gc.collect()

"""##### 大ジャンル"""

oe = OneHotEncoder(sparse=False, dtype = int)
biggenre_train = oe.fit_transform(train_df[['biggenre']])
biggenre_test = oe.transform(test_df[['biggenre']])

col = [f"biggenre_{i}" for i in oe.categories_[0]]

biggenre_train = pd.DataFrame(biggenre_train, columns=col)
biggenre_test = pd.DataFrame(biggenre_test, columns=col)

biggenre_train.to_feather(f"{GCF.FEATURES_PATH}/train_biggenre_ohe.ftr")
biggenre_test.to_feather(f"{GCF.FEATURES_PATH}/test_biggenre_ohe.ftr")

del biggenre_train, biggenre_test, col, oe
gc.collect()

"""#### カテゴリカルencoding"""

genre_dic = {v: i for i, v in enumerate(train_df['genre'].unique())}

train_genre = train_df['genre'].map(genre_dic)
test_genre = test_df['genre'].map(genre_dic)

biggenre_dic = {v: i for i, v in enumerate(train_df['biggenre'].unique())}

train_biggenre = train_df['biggenre'].map(biggenre_dic)
test_biggenre = test_df['biggenre'].map(biggenre_dic)

train_genre_cate = pd.DataFrame(zip(train_genre.values, train_biggenre.values), columns=['genre_cate', 'biggenre_cate'])
test_genre_cate = pd.DataFrame(zip(test_genre.values, test_biggenre.values), columns=['genre_cate', 'biggenre_cate'])

train_genre_cate.to_feather(f"{GCF.FEATURES_PATH}/train_genre_cate.ftr")
test_genre_cate.to_feather(f"{GCF.FEATURES_PATH}/test_genre_cate.ftr")

del genre_dic, biggenre_dic, train_genre, test_genre, train_biggenre, test_biggenre, train_genre_cate, test_genre_cate
gc.collect()

"""### Keyword
出現上位100件のonehot

"""

corpus_keyword = train_df['keyword'].tolist()

d = defaultdict(int)
for words in corpus_keyword:
    if type(words) is float:
        continue
    for w in words.split():
        d[w] += 1

available_kw = [k for k, v in d.items() if v >= 100]
available_kw_dic = {v: np.eye(len(available_kw))[i] for i, v in enumerate(available_kw)}

def onehot_keywords(kw, dic):
    vec = np.zeros(len(dic))
    if type(kw) is float:
        return vec
    for w in kw.split():
        try:
            ohe = dic[w]
        except KeyError:
            continue
        vec += ohe
    return vec.tolist()

keyword_vecs = train_df['keyword'].map(lambda x: onehot_keywords(x, available_kw_dic)).tolist()
keyword_ohe_train = pd.DataFrame(keyword_vecs, columns=available_kw)
del keyword_vecs
keyword_vecs = test_df['keyword'].map(lambda x: onehot_keywords(x, available_kw_dic)).tolist()
keyword_ohe_test = pd.DataFrame(keyword_vecs, columns=available_kw)

keyword_ohe_train.to_feather(f"{GCF.FEATURES_PATH}/train_kw_ohe_100.ftr")
keyword_ohe_test.to_feather(f"{GCF.FEATURES_PATH}/test_kw_ohe_100.ftr")

del keyword_ohe_train, keyword_ohe_test, keyword_vecs, available_kw, available_kw_dic, d, w, corpus_keyword
gc.collect()

"""### アップロード日付

#### 年・月
"""

year_train = train_df['general_firstup'].map(lambda x: x.split('-')[0]).astype(int)
month_train = train_df['general_firstup'].map(lambda x: x.split('-')[1]).astype(int)
year_test = test_df['general_firstup'].map(lambda x: x.split('-')[0]).astype(int)
month_test = test_df['general_firstup'].map(lambda x: x.split('-')[1]).astype(int)

date_train = pd.DataFrame(zip(year_train.tolist(), month_train.tolist()), columns=['upload_year', 'upload_month'])
date_test = pd.DataFrame(zip(year_test.tolist(), month_test.tolist()), columns=['upload_year', 'upload_month'])

date_train.to_feather(f"{GCF.FEATURES_PATH}/train_yyyymm.ftr")
date_test.to_feather(f"{GCF.FEATURES_PATH}/test_yyyymm.ftr")

del date_train, date_test, month_test, year_test, month_train, year_train
gc.collect()

"""#### 時間帯・曜日

"""

hour_train = train_df['general_firstup'].map(lambda x: int(x.split()[1].split(":")[0])).values
hour_test = test_df['general_firstup'].map(lambda x: int(x.split()[1].split(":")[0])).values

day_of_week_train = train_df['general_firstup'].map(lambda x: dt.strptime(x, '%Y-%m-%d %H:%M:%S').isoweekday()).values
day_of_week_test = test_df['general_firstup'].map(lambda x: dt.strptime(x, '%Y-%m-%d %H:%M:%S').isoweekday()).values

date2_train = pd.DataFrame(zip(hour_train, day_of_week_train), columns=['hour', 'day_of_week'])
date2_test = pd.DataFrame(zip(hour_test, day_of_week_test), columns=['hour', 'day_of_week'])

date2_train.to_feather(f"{GCF.FEATURES_PATH}/train_date2.ftr")
date2_test.to_feather(f"{GCF.FEATURES_PATH}/test_date2.ftr")

del date2_train, date2_test, hour_train, hour_test, day_of_week_train, day_of_week_test
gc.collect()

"""### One-Hot Encoding"""

month_train = train_df['general_firstup'].map(lambda x: np.eye(12)[int(x.split('-')[1])-1]).tolist()
month_test = test_df['general_firstup'].map(lambda x: np.eye(12)[int(x.split('-')[1])-1]).tolist()

hour_train = train_df['general_firstup'].map(lambda x: np.eye(24)[int(x.split()[1].split(":")[0])]).tolist()
hour_test = test_df['general_firstup'].map(lambda x: np.eye(24)[int(x.split()[1].split(":")[0])]).tolist()

day_of_week_train = train_df['general_firstup'].map(lambda x: np.eye(7)[dt.strptime(x, '%Y-%m-%d %H:%M:%S').isoweekday()-1]).tolist()
day_of_week_test = test_df['general_firstup'].map(lambda x: np.eye(7)[dt.strptime(x, '%Y-%m-%d %H:%M:%S').isoweekday()-1]).tolist()

cols = [f"month_ohe_{i:02}" for i in range(12)] + [f"hour_ohe_{i:02}" for i in range(24)] + [f"day_of_week_ohe_{i}" for i in range(7)]

date_ohe_train = pd.DataFrame(np.concatenate([np.stack(month_train), np.stack(hour_train), np.stack(day_of_week_train)], axis=1), columns=cols)
date_ohe_test = pd.DataFrame(np.concatenate([np.stack(month_test), np.stack(hour_test), np.stack(day_of_week_test)], axis=1), columns=cols)

date_ohe_train.to_feather(f"{GCF.FEATURES_PATH}/train_date_ohe.ftr")
date_ohe_test.to_feather(f"{GCF.FEATURES_PATH}/test_date_ohe.ftr")

del month_train, month_test, hour_train, hour_test, day_of_week_train, day_of_week_test, date_ohe_train, date_ohe_test, cols
gc.collect()

"""### UserID"""

# 投稿が多いからと言ってスコアが高いわけではなさそう
#lst = []
#for userid, df in train_df.groupby('userid'):
#    d = len(df), df['fav_novel_cnt_bin'].max(), df['fav_novel_cnt_bin'].min(), df['fav_novel_cnt_bin'].mean(), df['fav_novel_cnt_bin'].median()
#    lst.append(d)
#pd.DataFrame(lst).plot.scatter(x=0, y=4)

all_userid = test_df['userid'].tolist() + train_df['userid'].tolist()

dic = {}
idx = 0
for _id in set(all_userid):
    n = all_userid.count(_id)
    if n < 6:
        continue
    dic[_id] = idx
    idx += 1

def get_userid_idx(_id, dic):
    try:
        return dic[_id]
    except KeyError:
        return len(dic)

userid_train = train_df['userid'].map(lambda x: get_userid_idx(x, dic))
userid_test = test_df['userid'].map(lambda x: get_userid_idx(x, dic))

userid_train = pd.DataFrame(userid_train.tolist(), columns=['userid_over5'])
userid_test = pd.DataFrame(userid_test.tolist(), columns=['userid_over5'])

userid_train.to_feather(f"{GCF.FEATURES_PATH}/train_userid_over5.ftr")
userid_test.to_feather(f"{GCF.FEATURES_PATH}/test_userid_over5.ftr")

del userid_train, userid_test, dic, idx, n, all_userid
gc.collect()

"""## カテゴリデータの学習"""

y = train_df['fav_novel_cnt_bin'].values
X = pd.concat([pd.read_feather(f"{GCF.FEATURES_PATH}/train_{i}.ftr") for i in GCF.FEATURES], axis=1)
X_test = pd.concat([pd.read_feather(f"{GCF.FEATURES_PATH}/test_{i}.ftr") for i in GCF.FEATURES], axis=1)

"""### ロジスティック回帰"""

predicts = []
oof = np.zeros((len(y), 5))
skf = StratifiedKFold(n_splits=GCF.N_FOLDS, random_state=GCF.SEED, shuffle=True).split(X, y)
for fold, (train_index, valid_index) in enumerate(skf):
    print(f"Fold-{fold}")

    X_train = X.loc[train_index, :]
    X_valid = X.loc[valid_index, :]
    y_train = y[train_index]
    y_valid = y[valid_index]

    clf = LogisticRegression()
    clf.fit(X_train, y_train)
    pred_train = clf.predict_proba(X_train)
    pred_valid = clf.predict_proba(X_valid)

    train_score = log_loss(np.stack([np.eye(5)[i] for i in y_train]).astype(int), pred_train)
    valid_score = log_loss(np.stack([np.eye(5)[i] for i in y_valid]).astype(int), pred_valid)
    print(f"  train logloss = {train_score}")
    print(f"  valid logloss = {valid_score}")
    oof[valid_index] = pred_valid
    pred = clf.predict_proba(X_test)
    predicts.append(pred)

oof_score = log_loss(np.stack([np.eye(5)[i] for i in y]).astype(int), oof)

print(f"OOF score = {oof_score}")

predicts_avg = sum(predicts)/5
sub_df[["proba_0","proba_1","proba_2","proba_3","proba_4"]] = predicts_avg

sub_df.to_csv(f"{GCF.RESULT_PATH}/exp001_LogisticRegression_category_features.csv", index=None)

"""CV=0.9091 / LB=0.8317

### LightGBM
"""

params = {
    'objective': 'multiclass',
    'num_classes': 5,
    'metric': 'multi_logloss',
    'num_leaves': 42,
    'max_depth': 7,
    "feature_fraction": 0.8,
    'subsample_freq': 1,
    "bagging_fraction": 0.95,
    'min_data_in_leaf': 2,
    'learning_rate': 0.1,
    "boosting": "gbdt",
    "lambda_l1": 0.1,
    "lambda_l2": 10,
    "verbosity": -1,
    "random_state": 42,
    "num_boost_round": 50000,
    "early_stopping_rounds": 100
}

train_data = lgb.Dataset(X_train, label=y_train)
valid_data = lgb.Dataset(X_valid, label=y_valid)

feature_imp_lst = []
predicts = []
oof = np.zeros((len(y), 5))
skf = StratifiedKFold(n_splits=GCF.N_FOLDS, random_state=GCF.SEED, shuffle=True).split(X, y)
for fold, (train_index, valid_index) in enumerate(skf):
    print(f"Fold-{fold}")

    X_train = X.loc[train_index, :]
    X_valid = X.loc[valid_index, :]
    y_train = y[train_index]
    y_valid = y[valid_index]

    model = lgb.train(
        params,
        train_data, 
        categorical_feature = GCF.CATEGORY,
        valid_names = ['train', 'valid'],
        valid_sets =[train_data, valid_data], 
        verbose_eval = 100,
    )

    feature_imp = pd.DataFrame(sorted(zip(model.feature_importance(), X.columns)), columns=['importance', 'feature'])
    feature_imp_lst.append(feature_imp)

    pred_valid = model.predict(X_valid, num_iteration=model.best_iteration)
    oof[valid_index] = pred_valid

    pred_test = model.predict(X_test, num_iteration=model.best_iteration)
    predicts.append(pred_test)

oof_score = log_loss(np.stack([np.eye(5)[i] for i in y]).astype(int), oof)

print(f"OOF score = {oof_score}")

#lgb.plot_importance(model, figsize=(12,8), max_num_features=50, importance_type='gain')
#plt.tight_layout()
#plt.show()

predicts_avg = sum(predicts)/5
sub_df[["proba_0","proba_1","proba_2","proba_3","proba_4"]] = predicts_avg

#sub_df.to_csv(f"{GCF.RESULT_PATH}/exp002_LightGBM_category_features.csv", index=None)
#sub_df.to_csv(f"{GCF.RESULT_PATH}/exp003_LightGBM_add_month.csv", index=None)
sub_df.to_csv(f"{GCF.RESULT_PATH}/exp004_LightGBM_userid.csv", index=None)

"""|exp|CV|LB|memo|
|--|--|--|--|
|002|0.7934|0.8098|Baseline|
|003|0.7604|0.8049|年と月の追加|
|004|0.690４|0.7499|useridの追加|

## ニューラルネット
- BERT
- MLP
- マルチモーダル

### MLP Model & Dataset
"""

class NishikaCateDataset(Dataset):
    
    def __init__(self, X_cate, fav_novel_cnt_bin=None):
        self.X_cate = X_cate
        self.fav_novel_cnt_bin = fav_novel_cnt_bin

    def __len__(self):
        return len(self.X_cate)
    
    def __getitem__(self, item):
        X_cate = self.X_cate.iloc[item]
        if self.fav_novel_cnt_bin is None:
            fav_novel_cnt_bin = -1
        else:
            fav_novel_cnt_bin = self.fav_novel_cnt_bin[item]
            #fav_novel_cnt_bin = torch.eye(5)[:self.fav_novel_cnt_bin[item]+1].sum(0)

        d = {
            "X": torch.tensor(X_cate, dtype=torch.float),
            "y": torch.tensor(fav_novel_cnt_bin, dtype=torch.long),
            #"y": fav_novel_cnt_bin,
        }
        return d

class NishikaCateModel(nn.Module):
    
    def __init__(self, n_features, n_userid):
        super(NishikaCateModel, self).__init__()
        n_emb = 64
        self.emb = nn.Embedding(n_userid, n_emb, padding_idx=0)
        self.mlp1 = nn.Sequential(
            nn.Linear(n_features+n_emb, GCF.MLP_HIDDEN),
            nn.BatchNorm1d(GCF.MLP_HIDDEN),
            nn.ReLU(),
        )
        self.mlp2 = nn.Sequential(
            nn.Linear(GCF.MLP_HIDDEN, GCF.MLP_HIDDEN),
            nn.BatchNorm1d(GCF.MLP_HIDDEN),
            nn.ReLU(),
        )
        #self.dropouts = nn.ModuleList([nn.Dropout(0.5) for _ in range(8)])
        self.classifier = nn.Linear(GCF.MLP_HIDDEN, 5)

    def forward(self, X, y=None):
        e = self.emb(X[:, -1].long())
        h = torch.cat([X[:, :-1], e], dim=1)
        h = self.mlp1(h)
        h = self.mlp2(h)
        logits = self.classifier(h)
        #logits = sum([self.classifier(dropout(h)) for dropout in self.dropouts]) / 8

        if y is not None:
            loss = self.loss_fn(logits, y)
        else:
            loss = None
        return logits, loss

    def loss_fn(self, y_pred, y_true):
        #criterion1 = nn.CrossEntropyLoss()
        #criterion2 = nn.BCEWithLogitsLoss()
        #loss1 = criterion1(y_pred, y_true.argmin(1))
        #loss2 = criterion2(y_pred, y_true)
        #loss = loss1 + loss2 * 0.5
        loss = nn.CrossEntropyLoss()(y_pred, y_true)
        return loss

"""### BERT Model & Dataset"""

class NishikaDataset(Dataset):
    
    def __init__(self, df):
        self.title = df["title"].tolist()
        #self.writer = df["writer"].tolist()
        self.story = df["story"].tolist()
        self.fav_novel_cnt_bin = df["fav_novel_cnt_bin"].tolist()

    def __len__(self):
        return len(self.title)
    
    def __getitem__(self, item):
        title = self.title[item]
        #writer = self.writer[item]
        story = self.story[item]
        fav_novel_cnt_bin = self.fav_novel_cnt_bin[item]

        tok = GCF.TOKENIZER.encode_plus(
            title,
            story,
            truncation='only_second',
            max_length=GCF.MAX_LEN,
            padding='max_length'
        )

        d = {
            "input_ids": torch.tensor(tok['input_ids'], dtype=torch.long),
            "attention_mask": torch.tensor(tok['attention_mask'], dtype=torch.long),
            "token_type_ids": torch.tensor(tok['token_type_ids'], dtype=torch.long),
            "fav_novel_cnt_bin": torch.tensor(fav_novel_cnt_bin, dtype=torch.long),
        }
        return d

class NishikaModel(nn.Module):
    
    def __init__(self):
        super(NishikaModel, self).__init__()
        self.config = AutoConfig.from_pretrained(GCF.MODEL_NAME)
        self.transformer_model = AutoModel.from_pretrained(
            GCF.MODEL_NAME, 
            output_hidden_states=True
        )
        self.classifier = nn.Linear(self.config.hidden_size, 5)
        self._init_weights(self.classifier)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)


    def forward(self, input_ids, attention_mask, token_type_ids, y=None):
        outputs = self.transformer_model(input_ids, attention_mask, token_type_ids)
        sequence_output = torch.cat([outputs["hidden_states"][-1*i].mean(1) for i in range(1, GCF.N_USE_LAYER+1)], dim=1)
        logits = self.classifier(sequence_output)
        if y is not None:
            loss = self.loss_fn(logits, y)
        else:
            loss = None
        return logits, loss

    def loss_fn(self, y_pred, y_true):
        criterion = nn.CrossEntropyLoss()
        loss = criterion(y_pred, y_true)
        return loss

"""### Multimodal Model & Dataset"""

class NishikaMultiDataset(Dataset):
    def __init__(self, X_cate, df):
        self.X_cate = X_cate
        self.title = df["title"].tolist()
        self.story = df["story"].tolist()
        self.fav_novel_cnt_bin = df["fav_novel_cnt_bin"].tolist()

    def __len__(self):
        return len(self.X_cate)
    
    def __getitem__(self, item):
        X_cate = self.X_cate.iloc[item]
        fav_novel_cnt_bin = self.fav_novel_cnt_bin[item]
        title = self.title[item]
        story = self.story[item]
        fav_novel_cnt_bin = self.fav_novel_cnt_bin[item]

        tok = GCF.TOKENIZER.encode_plus(
            title,
            story,
            truncation='only_second',
            max_length=GCF.MAX_LEN,
            padding='max_length'
        )

        d = {
            "X_cate": torch.tensor(X_cate, dtype=torch.float),
            "input_ids": torch.tensor(tok['input_ids'], dtype=torch.long),
            "attention_mask": torch.tensor(tok['attention_mask'], dtype=torch.long),
            "token_type_ids": torch.tensor(tok['token_type_ids'], dtype=torch.long),
            "fav_novel_cnt_bin": torch.tensor(fav_novel_cnt_bin, dtype=torch.long),
        }
        return d

class NishikaMultiModel(nn.Module):
    
    def __init__(self, n_features, n_cate_dic):
        super(NishikaMultiModel, self).__init__()
        # BERT
        self.config = AutoConfig.from_pretrained(GCF.MODEL_NAME)
        self.transformer_model = AutoModel.from_pretrained(
            GCF.MODEL_NAME, 
            output_hidden_states=True
        )
        # MLP
        n_emb = 64 #+ 8 + 4
        self.userid_emb = nn.Embedding(n_cate_dic['n_userid'], 64, padding_idx=0)
        #self.genre_emb = nn.Embedding(n_cate_dic['n_genre'], 8, padding_idx=0)
        #self.biggenre_emb = nn.Embedding(n_cate_dic['n_biggenre'], 4, padding_idx=0)
        self.mlp1 = nn.Sequential(
            nn.Linear(n_features+n_emb, GCF.MLP_HIDDEN),
            nn.BatchNorm1d(GCF.MLP_HIDDEN),
            nn.ReLU(),
        )
        self.mlp2 = nn.Sequential(
            nn.Linear(GCF.MLP_HIDDEN, GCF.MLP_HIDDEN),
            nn.BatchNorm1d(GCF.MLP_HIDDEN),
            nn.ReLU(),
        )
        # head
        #self.cat_mlp = nn.Sequential(
            # 1層目
            #nn.Linear(self.config.hidden_size+GCF.MLP_HIDDEN, self.config.hidden_size+GCF.MLP_HIDDEN),
            #nn.BatchNorm1d(self.config.hidden_size+GCF.MLP_HIDDEN),
            #nn.ReLU(),
            # 2層目
            #nn.Linear(self.config.hidden_size+GCF.MLP_HIDDEN, self.config.hidden_size+GCF.MLP_HIDDEN),
            #nn.BatchNorm1d(self.config.hidden_size+GCF.MLP_HIDDEN),
            #nn.ReLU(),
        #)
        self.classifier = nn.Linear(self.config.hidden_size+GCF.MLP_HIDDEN, 5)
        #self._init_weights(self.cat_mlp)
        self._init_weights(self.classifier)

        # transformer freeze
        for param in self.transformer_model.parameters():
            param.requires_grad = False
        # mlp freeze
        #for param in self.cat_mlp.parameters():
        #    param.requires_grad = False
        

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)

    def forward(self, X_cate, input_ids, attention_mask, token_type_ids, y=None):
        # BERT
        outputs = self.transformer_model(input_ids, attention_mask, token_type_ids)
        sequence_output = torch.cat([outputs["hidden_states"][-1*i].mean(1) for i in range(1, GCF.N_USE_LAYER+1)], dim=1)
        # MLP
        #e1 = self.userid_emb(X_cate[:, -3].long())
        #e2 = self.genre_emb(X_cate[:, -2].long())
        #e3 = self.biggenre_emb(X_cate[:, -1].long())
        #h = torch.cat([X_cate[:, :-3], e1, e2, e3], dim=1)
        e = self.userid_emb(X_cate[:, -1].long())
        h = torch.cat([X_cate[:, :-1], e], dim=1)
        h = self.mlp1(h)
        h = self.mlp2(h)
        # head
        cat_h = torch.cat([sequence_output, h], dim=1)
        #cat_h = self.cat_mlp(cat_h)
        logits = self.classifier(cat_h)
        if y is not None:
            loss = self.loss_fn(logits, y)
        else:
            loss = None
        return logits, loss

    def loss_fn(self, y_pred, y_true):
        criterion = nn.CrossEntropyLoss()
        loss = criterion(y_pred, y_true)
        return loss

"""### Main Processing"""

def set_seed(seed=GCF.SEED):
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def train_loop(model, train_loader, optimizer, scheduler, bar=None):
    n_step = 0
    losses = []
    lrs = []
    model.train()
    optimizer.zero_grad()
    for d in train_loader:    
        with torch.cuda.amp.autocast(): 
            logits, loss = model(
                d['X_cate'].to(device),
                d['input_ids'].to(device),
                d['attention_mask'].to(device),
                d['token_type_ids'].to(device),
                d['fav_novel_cnt_bin'].to(device),
            )
            loss = loss / GCF.ACCUMULATE
        #logits, loss = model(d['X'].to(device), d['y'].to(device))

        losses.append(loss.item())
        lr = np.array([param_group["lr"] for param_group in optimizer.param_groups]).mean()
        lrs.append(lr)

        scaler.scale(loss).backward()

        if n_step % GCF.ACCUMULATE == 0:
            scaler.step(optimizer) 
            scaler.update() 
            optimizer.zero_grad()
            scheduler.step()   
        n_step += 1
        if bar is not None:
            bar.update(1)
    loss = np.array(losses).mean()
    return loss, lrs

def valid_loop(model, valid_loader):
    losses = []
    predicts = []
    model.eval()
    for d in valid_loader:    
        with torch.no_grad():
            logits, loss = model(
                d['X_cate'].to(device),
                d['input_ids'].to(device),
                d['attention_mask'].to(device),
                d['token_type_ids'].to(device),
                d['fav_novel_cnt_bin'].to(device),
            )
            #logits, loss = model(d['X'].to(device), d['y'].to(device))
        predicts.append(logits)
        losses.append(loss.item())
    predict = torch.vstack(predicts).cpu().softmax(1).numpy()
    loss = np.array(losses).mean()
    return loss, predict

def test_loop(model, test_loader):
    predicts = []
    model.eval()
    for d in test_loader:    
        with torch.no_grad():
            logits, _ = model(
                d['X_cate'].to(device),
                d['input_ids'].to(device),
                d['attention_mask'].to(device),
                d['token_type_ids'].to(device),
            )
            #logits, _ = model(d['X'].to(device))
        predicts.append(logits)
    predict = torch.vstack(predicts).cpu().softmax(1).numpy()
    return predict

fav_novel_cnt_bin = train_df['fav_novel_cnt_bin'].values
X_cate = pd.concat([pd.read_feather(f"{GCF.FEATURES_PATH}/train_{i}.ftr") for i in GCF.FEATURES], axis=1)
X_cate_test = pd.concat([pd.read_feather(f"{GCF.FEATURES_PATH}/test_{i}.ftr") for i in GCF.FEATURES], axis=1)

# year
X_cate['upload_year_norm'] = X_cate['upload_year'].map(lambda x: (x-2007)/14)
X_cate_test['upload_year_norm'] = X_cate_test['upload_year'].map(lambda x: (x-2007)/14)
# monthy
#X_cate['month_sin'] = np.sin(2 * np.pi * X_cate['upload_month']/12)
#X_cate['month_cos'] = np.cos(2 * np.pi * X_cate['upload_month']/12)
#X_cate_test['month_sin'] = np.sin(2 * np.pi * X_cate_test['upload_month']/12)
#X_cate_test['month_cos'] = np.cos(2 * np.pi * X_cate_test['upload_month']/12)
# hour
#X_cate['hour_sin'] = np.sin(2 * np.pi * X_cate['hour']/24)
#X_cate['hour_cos'] = np.cos(2 * np.pi * X_cate['hour']/24)
#X_cate_test['hour_sin'] = np.sin(2 * np.pi * X_cate_test['hour']/24)
#X_cate_test['hour_cos'] = np.cos(2 * np.pi * X_cate_test['hour']/24)
# day_of_week
#X_cate['day_of_week_sin'] = np.sin(2 * np.pi * X_cate['day_of_week']/7)
#X_cate['day_of_week_cos'] = np.cos(2 * np.pi * X_cate['day_of_week']/7)
#X_cate_test['day_of_week_sin'] = np.sin(2 * np.pi * X_cate_test['day_of_week']/7)
#X_cate_test['day_of_week_cos'] = np.cos(2 * np.pi * X_cate_test['day_of_week']/7)

# カテゴリ変数
# userid
userid_over5 = X_cate['userid_over5']
userid_over5_test = X_cate_test['userid_over5']
n_userid = len(set(userid_over5.tolist() + userid_over5_test.tolist()))
# genre and biggenre
#train_genre_cate = X_cate['genre_cate']
#train_biggenre_cate = X_cate['biggenre_cate']
#test_genre_cate = X_cate_test['genre_cate']
#test_biggenre_cate = X_cate_test['biggenre_cate']
#n_genre = len(train_genre_cate.unique())
#n_biggenre = len(train_biggenre_cate.unique())
# カテゴリ数の辞書
n_cate_dic = {
    'n_userid': n_userid,
    #'n_genre': n_genre,
    #'n_biggenre': n_biggenre,
}

# 不要カラム削除
#drop_features = ['upload_year', 'upload_month', 'userid_over5', 'hour', 'day_of_week', 'genre_cate', 'biggenre_cate']
drop_features = ['upload_year', 'upload_month', 'userid_over5']
X_cate = X_cate.drop(drop_features, axis=1)
X_cate_test = X_cate_test.drop(drop_features, axis=1)
n_features = X_cate.shape[1]

X_cate['userid_over5'] = userid_over5
#X_cate['genre_cate'] = train_biggenre_cate
#X_cate['biggenre_cate'] = train_biggenre_cate
X_cate_test['userid_over5'] = userid_over5_test
#X_cate_test['genre_cate'] = test_genre_cate
#X_cate_test['biggenre_cate'] = test_biggenre_cate

test_df['fav_novel_cnt_bin'] = -1
#test_dset = NishikaDataset(test_df)
#test_dset = NishikaCateDataset(X_cate_test)
test_dset = NishikaMultiDataset(X_cate_test, test_df)
test_loader = DataLoader(test_dset, batch_size=GCF.BS,
                           pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())

os.makedirs(f'{GCF.MODELS_PATH}', exist_ok=True)
predicts = []
results = []
oof = np.zeros((len(train_df), 5))
skf = StratifiedKFold(n_splits=GCF.N_FOLDS, random_state=GCF.SEED, shuffle=True).split(train_df['ncode'].values, fav_novel_cnt_bin)
for fold, (train_index, valid_index) in enumerate(skf):
    #train_dset = NishikaDataset(train_df.loc[train_index])
    #valid_dset = NishikaDataset(train_df.loc[valid_index])
    #train_dset = NishikaCateDataset(X_cate.loc[train_index], fav_novel_cnt_bin[train_index])
    #valid_dset = NishikaCateDataset(X_cate.loc[valid_index], fav_novel_cnt_bin[valid_index])
    train_dset = NishikaMultiDataset(X_cate.loc[train_index], train_df.loc[train_index])
    valid_dset = NishikaMultiDataset(X_cate.loc[valid_index], train_df.loc[valid_index])

    train_loader = DataLoader(train_dset, batch_size=GCF.BS,
                               pin_memory=True, shuffle=True, drop_last=True, num_workers=os.cpu_count(),
                               worker_init_fn=lambda x: set_seed())
    valid_loader = DataLoader(valid_dset, batch_size=GCF.BS,
                               pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())
    
    #model = NishikaModel()
    #model = NishikaCateModel(n_features, n_userid)
    model = NishikaMultiModel(n_features, n_cate_dic)
    model.to(device)

    optimizer = AdamW(model.parameters(), lr=GCF.LR, weight_decay=GCF.WEIGHT_DECAY)
    max_train_steps = GCF.N_EPOCHS * len(train_loader) // GCF.ACCUMULATE
    warmup_steps = int(max_train_steps * GCF.WARM_UP_RATIO)
    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=max_train_steps
    )

    bar = tqdm(total=int(GCF.N_EPOCHS * len(train_loader)))
    bar.set_description(f'{GCF.EXP_NAME} Fold-{fold}')

    valid_targert = np.array([np.eye(5)[i] for i in valid_dset.fav_novel_cnt_bin]).astype(int)

    early_stop = 0
    valid_best = float('inf')
    set_seed()
    for epoch in range(GCF.N_EPOCHS):
        if epoch == 3:
            for param in model.transformer_model.parameters():
                param.requires_grad = True
            #for param in model.cat_mlp.parameters():
            #    param.requires_grad = True
        train_loss, lrs = train_loop(model, train_loader, optimizer, scheduler, bar)
        valid_loss, valid_predict = valid_loop(model, valid_loader)
        valid_score = log_loss(valid_targert, valid_predict)
        print(f'epoch {epoch}, train_loss={train_loss}, valid_loss={valid_loss}, valid_score={valid_score}')
        if valid_best > valid_score:
            oof[valid_index, :] = valid_predict
            valid_best = valid_score
            print('    -> best score update!!')
            torch.save(model.state_dict(), f'{GCF.MODELS_PATH}/m_f{fold}.bin')
            early_stop = 0
        else:
            early_stop += 1

        if early_stop > GCF.PATIENT:
            print("### EARLY STOP ###")
            break
        results.append({
            'train_loss': train_loss,
            'valid_loss': valid_loss,
            'valid_score': valid_score,
        })

    model.load_state_dict(torch.load(f'{GCF.MODELS_PATH}/m_f{fold}.bin'))
    test_preds = test_loop(model, test_loader)
    predicts.append(test_preds)

    pd.DataFrame(results).to_csv(f'{GCF.MODELS_PATH}/result.csv', index=None)

    del model, optimizer, scheduler, valid_targert, bar
    torch.cuda.empty_cache()

oof_score = log_loss(np.stack([np.eye(5)[i] for i in fav_novel_cnt_bin]).astype(int), oof)
print(f"OOF score = {oof_score}")

predicts_avg = np.array(predicts).mean(0)
sub_df[["proba_0","proba_1","proba_2","proba_3","proba_4"]] = predicts_avg
sub_df.to_csv(f"{GCF.RESULT_PATH}/{GCF.EXP_NAME}.csv", index=None)

"""|exp|CV|LB|memo|
|--|--|--|--|
|005|0.9343||BERT, fold-0のみ|
|006|0.8979||カテゴリをNNで, fold-0のみ|
|007|0.8302||特徴量のnormalize, fold-0のみ, 3epochでoverfit|
|008|0.8268||batch normalization, fold-0のみ, 3epochでoverfit|
|009|0.8346||MSD, fold-0のみ, 3epochでoverfit|
|010|0.8362||hidden=256, fold-0のみ, 3epochでoverfit|
|011|0.8290||MLP1層のみ, fold-0のみ, 4epochでoverfit|
|012|0.8403||h1024, fold-0のみ, 4epochでoverfit|
|013|0.8465||useridのembed=16, fold-0のみ, 4epochでoverfit|
|||||
|014|0.8243|0.7194|useridのembed=64|
|015|||順序回帰, 全然だめ|
|016|||順序回帰の損失を0.5, 全然だめ|
|017|0.8254|0.7254|曜日と時間帯の追加|
|018|0.8735||マルチモーダルに学習, fold-0のみ, 2epochで打ち切り|
|019|0.8790||連結にmlpを1層追加|
|020|0.7717|0.7133|BERTをfreeze, (oof平均)|
|021|0.7659|0.6921|途中でunfreeze|
|022|||mlpが入ってなかったので修正, fold-2まで学習して微妙だったので打ち切り|
|023|0.7692|0.7279|ジャンル系をカテゴリにしてembed|
|||||
|024|||カテゴリはすべてonehot|

fold-0
```
epoch 0, train_loss=0.4729570716023445, valid_loss=0.8451534967720509
epoch 1, train_loss=0.4096998051404953, valid_loss=0.8090386870503425
epoch 2, train_loss=0.37696062195301056, valid_loss=0.7829894825518131
epoch 3, train_loss=0.36571439534425737, valid_loss=0.7690463020503521
```

fold-1
```
epoch 0, train_loss=0.47672616118192673, valid_loss=0.8172269129157066
epoch 1, train_loss=0.4106237145960331, valid_loss=0.7994993416965008
epoch 2, train_loss=0.3765521739721298, valid_loss=0.770296095252037
epoch 3, train_loss=0.3645930057168007, valid_loss=0.7576712134480477
```

fold-2
```
epoch 0, train_loss=0.4737464160919189, valid_loss=0.847214619576931
epoch 1, train_loss=0.40764222997426985, valid_loss=0.8028247025609017
epoch 2, train_loss=0.3737793028950691, valid_loss=0.7812070498168469
epoch 3, train_loss=0.3586164472401142, valid_loss=0.7694104026556015
```

fold-3
```
epoch 0, train_loss=0.4765675675272942, valid_loss=0.883497886300087
epoch 1, train_loss=0.4077773263454437, valid_loss=0.7950156292617321
epoch 2, train_loss=0.37347859781980514, valid_loss=0.7728625036776066
epoch 3, train_loss=0.35834020206332207, valid_loss=0.7584532390236854
```

fold-4
```
epoch 0, train_loss=0.47375704061985013, valid_loss=0.8585369145870209
epoch 1, train_loss=0.4084460366368294, valid_loss=0.7977879850268363
epoch 2, train_loss=0.3743955251574516, valid_loss=0.8062791697978974
epoch 3, train_loss=0.36531265267729757, valid_loss=0.7749437160491943
```
"""

