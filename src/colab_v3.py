# -*- coding: utf-8 -*-
"""なろう.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ejGshCAaZRqdj_ZW_7dWAcaH7j1GUupU

# 小説家になろう ブクマ数予測 \~”伸びる”タイトルとは？\~

## Memo
- pseudo labeling
- target encording
- ncodeを特徴量として利用
- binary特徴をタグとして入れる
- URLけす
- 続き、や続編
- user_id
    - 増やす

### Not Work
- test
    - yearは2021のみ
    - monthは8, 9月のみ
    - genreは9801が無い
    - biggenreは98が無い
    - ジャンルの偏りはなさそう
- yyyy, >= 2016か >=2018らへんが目安になりそう
- 長編・短編でstratified
    - 普通に均等だった
"""

!nvidia-smi

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers unidic-lite fugashi ipadic
!pip install -U torch

"""## 共通設定"""

import gc
import os
import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from datetime import datetime as dt
from tqdm.notebook import tqdm
from collections import defaultdict
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
import itertools

# Models
import lightgbm as lgb
from sklearn.linear_model import LogisticRegression

import re

# Transformer
import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader

from transformers import get_cosine_schedule_with_warmup
from transformers import AutoConfig
from transformers import AutoTokenizer
from transformers import AutoModel
from transformers import AdamW

device = torch.device("cuda")
scaler = torch.cuda.amp.GradScaler()

class GCF:
    EXP_NAME = 'exp055_reg_loss'

    INPUT_PATH = "/content/drive/MyDrive/Study/Nishika"
    FEATURES_PATH = f"{INPUT_PATH}/features"
    RESULT_PATH = f"{INPUT_PATH}/result"
    MODELS_PATH = f"{INPUT_PATH}/models/{EXP_NAME}"

    N_FOLDS = 5
    SEED = 0

    FEATURES = [
        "pc_or_k",
        "org_bin",
        "genre_ohe", "biggenre_ohe",
        "kw_ohe_100",
        "yyyymm",
        "date2",
        "userid_over5",
        #"url_count_3",
    ]

    MODEL_NAME = "cl-tohoku/bert-base-japanese-whole-word-masking"
    TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME)
    MAX_LEN = 256

    BS = 16
    N_USE_LAYER = 2
    LR = 1e-3
    WEIGHT_DECAY = 1e-3
    N_EPOCHS = 4
    ACCUMULATE = 2
    WARM_UP_RATIO = 0.1
    MLP_HIDDEN = 512
    PATIENT = 8
    REINIT_LAYERS = 0
    N_MSD = 8

train_df = pd.read_csv(f"{GCF.INPUT_PATH}/train.csv")
test_df = pd.read_csv(f"{GCF.INPUT_PATH}/test.csv")
sub_df = pd.read_csv(f"{GCF.INPUT_PATH}/sample_submission.csv")

"""## 特徴抽出

### PCか携帯か
"""

pc_or_k_dic = {0:[0, 0], 1:[1, 0], 2:[0, 1], 3:[1, 1]}
pc_or_k_train = pd.DataFrame(train_df['pc_or_k'].map(pc_or_k_dic).tolist(), columns=['pc', 'keitai'])
pc_or_k_test = pd.DataFrame(test_df['pc_or_k'].map(pc_or_k_dic).tolist(), columns=['pc', 'keitai'])

pc_or_k_train.to_feather(f"{GCF.FEATURES_PATH}/train_pc_or_k.ftr")
pc_or_k_test.to_feather(f"{GCF.FEATURES_PATH}/test_pc_or_k.ftr")

del pc_or_k_test, pc_or_k_train, pc_or_k_dic
gc.collect()

"""### オリジナル2値データ

- 元データそのままの2値データ
- endとisstopはすべての値が0なので無視する
"""

org_bin_feat = ['isr15', 'isbl', 'isgl', 'iszankoku', 'istensei', 'istenni']
org_bin_train = train_df[org_bin_feat]
org_bin_test = test_df[org_bin_feat]

org_bin_train = pd.concat([org_bin_train, train_df[['novel_type']] - 1], axis=1)
org_bin_test = pd.concat([org_bin_test, test_df[['novel_type']] - 1], axis=1)

org_bin_train.to_feather(f"{GCF.FEATURES_PATH}/train_org_bin.ftr")
org_bin_test.to_feather(f"{GCF.FEATURES_PATH}/test_org_bin.ftr")

del org_bin_train, org_bin_test, org_bin_feat
gc.collect()

"""### ジャンル

#### 単純なOne-Hot Encoding

##### ジャンル
"""

oe = OneHotEncoder(sparse=False, dtype = int)
genre_train = oe.fit_transform(train_df[['genre']])
genre_test = oe.transform(test_df[['genre']])

col = [f"genre_{i}" for i in oe.categories_[0]]

genre_train = pd.DataFrame(genre_train, columns=col)
genre_test = pd.DataFrame(genre_test, columns=col)

genre_train.to_feather(f"{GCF.FEATURES_PATH}/train_genre_ohe.ftr")
genre_test.to_feather(f"{GCF.FEATURES_PATH}/test_genre_ohe.ftr")

del genre_train, genre_test, col, oe
gc.collect()

"""##### 大ジャンル"""

oe = OneHotEncoder(sparse=False, dtype = int)
biggenre_train = oe.fit_transform(train_df[['biggenre']])
biggenre_test = oe.transform(test_df[['biggenre']])

col = [f"biggenre_{i}" for i in oe.categories_[0]]

biggenre_train = pd.DataFrame(biggenre_train, columns=col)
biggenre_test = pd.DataFrame(biggenre_test, columns=col)

biggenre_train.to_feather(f"{GCF.FEATURES_PATH}/train_biggenre_ohe.ftr")
biggenre_test.to_feather(f"{GCF.FEATURES_PATH}/test_biggenre_ohe.ftr")

del biggenre_train, biggenre_test, col, oe
gc.collect()

"""#### カテゴリカルencoding"""

genre_dic = {v: i for i, v in enumerate(train_df['genre'].unique())}

train_genre = train_df['genre'].map(genre_dic)
test_genre = test_df['genre'].map(genre_dic)

biggenre_dic = {v: i for i, v in enumerate(train_df['biggenre'].unique())}

train_biggenre = train_df['biggenre'].map(biggenre_dic)
test_biggenre = test_df['biggenre'].map(biggenre_dic)

train_genre_cate = pd.DataFrame(zip(train_genre.values, train_biggenre.values), columns=['genre_cate', 'biggenre_cate'])
test_genre_cate = pd.DataFrame(zip(test_genre.values, test_biggenre.values), columns=['genre_cate', 'biggenre_cate'])

train_genre_cate.to_feather(f"{GCF.FEATURES_PATH}/train_genre_cate.ftr")
test_genre_cate.to_feather(f"{GCF.FEATURES_PATH}/test_genre_cate.ftr")

del genre_dic, biggenre_dic, train_genre, test_genre, train_biggenre, test_biggenre, train_genre_cate, test_genre_cate
gc.collect()

"""### Keyword
出現上位100件のonehot

"""

corpus_keyword = train_df['keyword'].tolist()

d = defaultdict(int)
for words in corpus_keyword:
    if type(words) is float:
        continue
    for w in words.split():
        d[w] += 1

available_kw = [k for k, v in d.items() if v >= 100]
available_kw_dic = {v: np.eye(len(available_kw))[i] for i, v in enumerate(available_kw)}

def onehot_keywords(kw, dic):
    vec = np.zeros(len(dic))
    if type(kw) is float:
        return vec
    for w in kw.split():
        try:
            ohe = dic[w]
        except KeyError:
            continue
        vec += ohe
    return vec.tolist()

keyword_vecs = train_df['keyword'].map(lambda x: onehot_keywords(x, available_kw_dic)).tolist()
keyword_ohe_train = pd.DataFrame(keyword_vecs, columns=available_kw)
del keyword_vecs
keyword_vecs = test_df['keyword'].map(lambda x: onehot_keywords(x, available_kw_dic)).tolist()
keyword_ohe_test = pd.DataFrame(keyword_vecs, columns=available_kw)

keyword_ohe_train.to_feather(f"{GCF.FEATURES_PATH}/train_kw_ohe_100.ftr")
keyword_ohe_test.to_feather(f"{GCF.FEATURES_PATH}/test_kw_ohe_100.ftr")

del keyword_ohe_train, keyword_ohe_test, keyword_vecs, available_kw, available_kw_dic, d, w, corpus_keyword
gc.collect()

"""### アップロード日付

#### 年・月
"""

year_train = train_df['general_firstup'].map(lambda x: x.split('-')[0]).astype(int)
month_train = train_df['general_firstup'].map(lambda x: x.split('-')[1]).astype(int)
year_test = test_df['general_firstup'].map(lambda x: x.split('-')[0]).astype(int)
month_test = test_df['general_firstup'].map(lambda x: x.split('-')[1]).astype(int)

date_train = pd.DataFrame(zip(year_train.tolist(), month_train.tolist()), columns=['upload_year', 'upload_month'])
date_test = pd.DataFrame(zip(year_test.tolist(), month_test.tolist()), columns=['upload_year', 'upload_month'])

date_train.to_feather(f"{GCF.FEATURES_PATH}/train_yyyymm.ftr")
date_test.to_feather(f"{GCF.FEATURES_PATH}/test_yyyymm.ftr")

del date_train, date_test, month_test, year_test, month_train, year_train
gc.collect()

"""#### 時間帯・曜日

"""

hour_train = train_df['general_firstup'].map(lambda x: int(x.split()[1].split(":")[0])).values
hour_test = test_df['general_firstup'].map(lambda x: int(x.split()[1].split(":")[0])).values

day_of_week_train = train_df['general_firstup'].map(lambda x: dt.strptime(x, '%Y-%m-%d %H:%M:%S').isoweekday()).values
day_of_week_test = test_df['general_firstup'].map(lambda x: dt.strptime(x, '%Y-%m-%d %H:%M:%S').isoweekday()).values

date2_train = pd.DataFrame(zip(hour_train, day_of_week_train), columns=['hour', 'day_of_week'])
date2_test = pd.DataFrame(zip(hour_test, day_of_week_test), columns=['hour', 'day_of_week'])

date2_train.to_feather(f"{GCF.FEATURES_PATH}/train_date2.ftr")
date2_test.to_feather(f"{GCF.FEATURES_PATH}/test_date2.ftr")

del date2_train, date2_test, hour_train, hour_test, day_of_week_train, day_of_week_test
gc.collect()

"""### URL"""

removes = ['http://', 'https://', 'www.', '.co', '.ne', '.jp', '.com']
def get_domain(string): 
    url = re.findall('https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+', string)
    _url = []
    for u in url:
        for s in removes:
            u = u.replace(s, '')
        _url.append(u)
    return _url

train_urls = train_df['story'].map(get_domain)
test_urls = test_df['story'].map(get_domain)

all_urls = train_urls.tolist() + test_urls.tolist()
domein_count = pd.DataFrame(list(itertools.chain.from_iterable(all_urls)))[0].value_counts()

domein_over3 = domein_count[domein_count > 3].index.values

def get_count_url(urls_lst, domein_over3):
    lst = []
    for urls in urls_lst:
        count_url = np.zeros(len(domein_over3))
        for url in urls:
            count_url += (domein_over3 == url).astype(int)
        lst.append(count_url)
    return pd.DataFrame(lst, columns=domein_over3)

train_url_count = get_count_url(train_urls, domein_over3)
test_url_count = get_count_url(test_urls, domein_over3)

max_count = max(test_url_count.max().max(), train_url_count.max().max())
train_url_count = train_url_count / max_count
test_url_count = test_url_count / max_count

train_url_count.to_feather(f"{GCF.FEATURES_PATH}/train_url_count_3.ftr")
test_url_count.to_feather(f"{GCF.FEATURES_PATH}/test_url_count_3.ftr")

del train_url_count, test_url_count, max_count, domein_over3, domein_count, all_urls, train_urls, test_urls
del removes, get_domain, get_count_url
gc.collect()

"""### One-Hot Encoding"""

month_train = train_df['general_firstup'].map(lambda x: np.eye(12)[int(x.split('-')[1])-1]).tolist()
month_test = test_df['general_firstup'].map(lambda x: np.eye(12)[int(x.split('-')[1])-1]).tolist()

hour_train = train_df['general_firstup'].map(lambda x: np.eye(24)[int(x.split()[1].split(":")[0])]).tolist()
hour_test = test_df['general_firstup'].map(lambda x: np.eye(24)[int(x.split()[1].split(":")[0])]).tolist()

day_of_week_train = train_df['general_firstup'].map(lambda x: np.eye(7)[dt.strptime(x, '%Y-%m-%d %H:%M:%S').isoweekday()-1]).tolist()
day_of_week_test = test_df['general_firstup'].map(lambda x: np.eye(7)[dt.strptime(x, '%Y-%m-%d %H:%M:%S').isoweekday()-1]).tolist()

cols = [f"month_ohe_{i:02}" for i in range(12)] + [f"hour_ohe_{i:02}" for i in range(24)] + [f"day_of_week_ohe_{i}" for i in range(7)]

date_ohe_train = pd.DataFrame(np.concatenate([np.stack(month_train), np.stack(hour_train), np.stack(day_of_week_train)], axis=1), columns=cols)
date_ohe_test = pd.DataFrame(np.concatenate([np.stack(month_test), np.stack(hour_test), np.stack(day_of_week_test)], axis=1), columns=cols)

date_ohe_train.to_feather(f"{GCF.FEATURES_PATH}/train_date_ohe.ftr")
date_ohe_test.to_feather(f"{GCF.FEATURES_PATH}/test_date_ohe.ftr")

del month_train, month_test, hour_train, hour_test, day_of_week_train, day_of_week_test, date_ohe_train, date_ohe_test, cols
gc.collect()

"""### UserID"""

# 投稿が多いからと言ってスコアが高いわけではなさそう
#lst = []
#for userid, df in train_df.groupby('userid'):
#    d = len(df), df['fav_novel_cnt_bin'].max(), df['fav_novel_cnt_bin'].min(), df['fav_novel_cnt_bin'].mean(), df['fav_novel_cnt_bin'].median()
#    lst.append(d)
#pd.DataFrame(lst).plot.scatter(x=0, y=4)

all_userid = test_df['userid'].tolist() + train_df['userid'].tolist()

dic = {}
idx = 0
for _id in set(all_userid):
    n = all_userid.count(_id)
    if n < 6:
        continue
    dic[_id] = idx
    idx += 1

def get_userid_idx(_id, dic):
    try:
        return dic[_id]
    except KeyError:
        return len(dic)

userid_train = train_df['userid'].map(lambda x: get_userid_idx(x, dic))
userid_test = test_df['userid'].map(lambda x: get_userid_idx(x, dic))

userid_train = pd.DataFrame(userid_train.tolist(), columns=['userid_over5'])
userid_test = pd.DataFrame(userid_test.tolist(), columns=['userid_over5'])

userid_train.to_feather(f"{GCF.FEATURES_PATH}/train_userid_over5.ftr")
userid_test.to_feather(f"{GCF.FEATURES_PATH}/test_userid_over5.ftr")

del userid_train, userid_test, dic, idx, n, all_userid
gc.collect()

"""## ニューラルネット
- マルチモーダル

### Multimodal Model & Dataset
"""

class NishikaMultiDataset(Dataset):
    def __init__(self, X_cate, df):
        self.X_cate = X_cate
        self.title = df['title_genre_tag'].tolist()
        self.story = df["story"].tolist()
        #self.story = df["writer_story"].tolist()
        self.fav_novel_cnt_bin = df["fav_novel_cnt_bin"].tolist()

    def __len__(self):
        return len(self.X_cate)
    
    def __getitem__(self, item):
        X_cate = self.X_cate.iloc[item]
        fav_novel_cnt_bin = self.fav_novel_cnt_bin[item]
        title = self.title[item]
        story = self.story[item]
        fav_novel_cnt_bin = self.fav_novel_cnt_bin[item]
        #fav_novel_cnt_bin = torch.eye(5)[self.fav_novel_cnt_bin[item]]

        tok = GCF.TOKENIZER.encode_plus(
            title,
            story,
            truncation='only_second',
            max_length=GCF.MAX_LEN,
            padding='max_length'
        )

        d = {
            "X_cate": torch.tensor(X_cate, dtype=torch.float),
            "input_ids": torch.tensor(tok['input_ids'], dtype=torch.long),
            "attention_mask": torch.tensor(tok['attention_mask'], dtype=torch.long),
            "token_type_ids": torch.tensor(tok['token_type_ids'], dtype=torch.long),
             #"fav_novel_cnt_bin": fav_novel_cnt_bin,
            "fav_novel_cnt_bin": torch.tensor(fav_novel_cnt_bin, dtype=torch.long),
             
        }
        return d

class NishikaMultiModel(nn.Module):
    
    def __init__(self, n_features, n_cate_dic):
        super(NishikaMultiModel, self).__init__()
        # BERT
        self.config = AutoConfig.from_pretrained(GCF.MODEL_NAME)
        self.transformer_model = AutoModel.from_pretrained(
            GCF.MODEL_NAME, 
            output_hidden_states=True
        )
        #self.layer_norm = nn.LayerNorm(self.config.hidden_size*GCF.N_USE_LAYER)
        self.transformer_head = nn.Sequential(
            nn.Linear(self.config.hidden_size*GCF.N_USE_LAYER, self.config.hidden_size*GCF.N_USE_LAYER),
            #nn.BatchNorm1d(self.config.hidden_size*GCF.N_USE_LAYER),
            #nn.ReLU(),
            #nn.Linear(self.config.hidden_size*GCF.N_USE_LAYER, self.config.hidden_size*GCF.N_USE_LAYER),
        )
        
        # MLP
        n_emb = 64
        self.userid_emb = nn.Embedding(n_cate_dic['n_userid'], n_emb, padding_idx=0)
        self.mlp1 = nn.Sequential(
            nn.Linear(n_features+n_emb, GCF.MLP_HIDDEN),
            nn.BatchNorm1d(GCF.MLP_HIDDEN),
            nn.ReLU(),
        )
        self.mlp2 = nn.Sequential(
            nn.Linear(GCF.MLP_HIDDEN, GCF.MLP_HIDDEN),
            nn.BatchNorm1d(GCF.MLP_HIDDEN),
            nn.ReLU(),
        )

        #self.cat_mlp = nn.Sequential(
        #    nn.Linear(self.config.hidden_size*GCF.N_USE_LAYER+n_emb+n_features, self.config.hidden_size*GCF.N_USE_LAYER+n_emb+n_features),
        #    nn.BatchNorm1d(self.config.hidden_size*GCF.N_USE_LAYER+n_emb+n_features),
        #    nn.ReLU(),
        #)
        #self.classifier_1 = nn.Linear(self.config.hidden_size*GCF.N_USE_LAYER+n_emb+n_features, 5)
        #self.classifier_2 = nn.Linear(self.config.hidden_size*GCF.N_USE_LAYER+n_emb+n_features, 5)

        # head
        self.regressor = nn.Linear(self.config.hidden_size*GCF.N_USE_LAYER+GCF.MLP_HIDDEN, 1)
        self.classifier_1 = nn.Linear(self.config.hidden_size*GCF.N_USE_LAYER+GCF.MLP_HIDDEN, 5)
        self.classifier_2 = nn.Linear(self.config.hidden_size*GCF.N_USE_LAYER+GCF.MLP_HIDDEN, 5)
        self.dropouts_1 = nn.ModuleList([nn.Dropout(0.2) for _ in range(GCF.N_MSD)])
        self.dropouts_2 = nn.ModuleList([nn.Dropout(0.2) for _ in range(GCF.N_MSD)])
        

        self.transformer_model.resize_token_embeddings(len(GCF.TOKENIZER))
        self._init_weights(self.transformer_head)
        self._init_weights(self.regressor)
        #self._init_weights(self.cat_mlp)
        self._init_weights(self.classifier_1)
        self._init_weights(self.classifier_2)

        # transformer freeze
        for param in self.transformer_model.parameters():
            param.requires_grad = False

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)

    def forward(self, X_cate, input_ids, attention_mask, token_type_ids, y=None):
        # BERT
        outputs = self.transformer_model(input_ids, attention_mask, token_type_ids)
        sequence_output = torch.cat([outputs["hidden_states"][-1*i].mean(1) for i in range(1, GCF.N_USE_LAYER+1)], dim=1)
        #sequence_output = self.layer_norm(sequence_output)
        sequence_output = self.transformer_head(sequence_output)
        # MLP
        e = self.userid_emb(X_cate[:, -1].long())
        h = torch.cat([X_cate[:, :-1], e], dim=1)
        h = self.mlp1(h)
        h = self.mlp2(h)
        # head
        cat_h = torch.cat([sequence_output, h], dim=1)
        regr = self.regressor(cat_h)
        #cat_h = self.cat_mlp(cat_h)
        logits_1 = sum([self.classifier_1(dropout(cat_h)) for dropout in self.dropouts_1]) / GCF.N_MSD
        logits_2 = sum([self.classifier_2(dropout(cat_h)) for dropout in self.dropouts_2]) / GCF.N_MSD

        mask = X_cate[:, 8]
        logits =  logits_1, logits_2, regr, mask  #.unsqueeze(1)
        if y is not None:
            loss = self.loss_fn(logits, y)
        else:
            loss = None
        dual_logits = logits_1 * (1 - mask).unsqueeze(1) + logits_2 * mask.unsqueeze(1)

        return dual_logits, loss

    def loss_fn(self, y_pred, y_true):
        y_pred_1, y_pred_2, regr, mask = y_pred
        reg_loss = torch.sqrt(nn.MSELoss()(regr, y_true.float()))
        loss_1 = nn.CrossEntropyLoss(reduction='none')(y_pred_1, y_true)
        loss_2 = nn.CrossEntropyLoss(reduction='none')(y_pred_2, y_true)
        #loss_1 = nn.BCEWithLogitsLoss(reduction='none')(y_pred_1, y_true)
        #loss_2 = nn.BCEWithLogitsLoss(reduction='none')(y_pred_2, y_true)
        loss = loss_1 * (1 - mask) + loss_2 * mask + reg_loss * 0.5
        return loss.mean()

"""### Main Processing"""

def set_seed(seed=GCF.SEED):
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def train_loop(model, train_loader, optimizer, scheduler, bar=None):
    n_step = 0
    losses = []
    lrs = []
    model.train()
    optimizer.zero_grad()
    for d in train_loader:    
        with torch.cuda.amp.autocast(): 
            _, loss = model(
                d['X_cate'].to(device),
                d['input_ids'].to(device),
                d['attention_mask'].to(device),
                d['token_type_ids'].to(device),
                d['fav_novel_cnt_bin'].to(device),
            )
            loss = loss / GCF.ACCUMULATE

        losses.append(loss.item())
        lr = np.array([param_group["lr"] for param_group in optimizer.param_groups]).mean()
        lrs.append(lr)

        scaler.scale(loss).backward()

        if n_step % GCF.ACCUMULATE == 0:
            scaler.step(optimizer) 
            scaler.update() 
            optimizer.zero_grad()
            scheduler.step()   
        n_step += 1
        if bar is not None:
            bar.update(1)
    loss = np.array(losses).mean()
    return loss, lrs

def valid_loop(model, valid_loader):
    losses = []
    predicts = []
    model.eval()
    for d in valid_loader:    
        with torch.no_grad():
            logits, loss = model(
                d['X_cate'].to(device),
                d['input_ids'].to(device),
                d['attention_mask'].to(device),
                d['token_type_ids'].to(device),
                d['fav_novel_cnt_bin'].to(device),
            )

        predicts.append(logits)
        losses.append(loss.item())
    predict = torch.vstack(predicts).cpu().softmax(1).numpy()
    #predict = torch.vstack(predicts).cpu().sigmoid().numpy()
    loss = np.array(losses).mean()
    return loss, predict

def test_loop(model, test_loader):
    predicts = []
    model.eval()
    for d in test_loader:    
        with torch.no_grad():
            logits, _ = model(
                d['X_cate'].to(device),
                d['input_ids'].to(device),
                d['attention_mask'].to(device),
                d['token_type_ids'].to(device),
            )
        predicts.append(logits)
    predict = torch.vstack(predicts).cpu().softmax(1).numpy()
    #predict = torch.vstack(predicts).cpu().sigmoid().numpy()
    return predict

X_cate = pd.concat([pd.read_feather(f"{GCF.FEATURES_PATH}/train_{i}.ftr") for i in GCF.FEATURES], axis=1)
X_cate_test = pd.concat([pd.read_feather(f"{GCF.FEATURES_PATH}/test_{i}.ftr") for i in GCF.FEATURES], axis=1)

# year
X_cate['upload_year_norm'] = X_cate['upload_year'].map(lambda x: (x-2007)/14)
X_cate_test['upload_year_norm'] = X_cate_test['upload_year'].map(lambda x: (x-2007)/14)
# monthy
X_cate['month_sin'] = np.sin(2 * np.pi * X_cate['upload_month']/12)
X_cate['month_cos'] = np.cos(2 * np.pi * X_cate['upload_month']/12)
X_cate_test['month_sin'] = np.sin(2 * np.pi * X_cate_test['upload_month']/12)
X_cate_test['month_cos'] = np.cos(2 * np.pi * X_cate_test['upload_month']/12)
# hour
X_cate['hour_sin'] = np.sin(2 * np.pi * X_cate['hour']/24)
X_cate['hour_cos'] = np.cos(2 * np.pi * X_cate['hour']/24)
X_cate_test['hour_sin'] = np.sin(2 * np.pi * X_cate_test['hour']/24)
X_cate_test['hour_cos'] = np.cos(2 * np.pi * X_cate_test['hour']/24)
# day_of_week
X_cate['day_of_week_sin'] = np.sin(2 * np.pi * X_cate['day_of_week']/7)
X_cate['day_of_week_cos'] = np.cos(2 * np.pi * X_cate['day_of_week']/7)
X_cate_test['day_of_week_sin'] = np.sin(2 * np.pi * X_cate_test['day_of_week']/7)
X_cate_test['day_of_week_cos'] = np.cos(2 * np.pi * X_cate_test['day_of_week']/7)

# カテゴリ変数
# userid
userid_over5 = X_cate['userid_over5']
userid_over5_test = X_cate_test['userid_over5']
n_userid = len(set(userid_over5.tolist() + userid_over5_test.tolist()))
# カテゴリ数の辞書
n_cate_dic = {
    'n_userid': n_userid,
}

# 不要カラム削除
drop_features = ['upload_year', 'upload_month', 'userid_over5', 'hour', 'day_of_week'] #+ ['genre_9801', 'biggenre_98']
X_cate = X_cate.drop(drop_features, axis=1)
X_cate_test = X_cate_test.drop(drop_features, axis=1)
n_features = X_cate.shape[1]

X_cate['userid_over5'] = userid_over5
X_cate_test['userid_over5'] = userid_over5_test

# titleにgenreのタグを追加
genre_tag_dic = {g: f'[GENRE_{g:04}]' for g in train_df['genre'].unique()}
GCF.TOKENIZER.add_tokens(list(genre_tag_dic.values()))
print('add token:', GCF.TOKENIZER.added_tokens_encoder)
train_df['title_genre_tag'] = train_df.apply(lambda x: genre_tag_dic[x['genre']] + x['title'], axis=1)
test_df['title_genre_tag'] = test_df.apply(lambda x: genre_tag_dic[x['genre']] + x['title'], axis=1)

# witerを追加
#train_df['writer_story'] = train_df.apply(lambda x: x['writer']+"[SEP]"+x['story'], axis=1)
#test_df['writer_story'] = test_df.apply(lambda x: x['writer']+"[SEP]"+x['story'], axis=1)

# testデータに合わせて学習データを絞り込み
#is_over_year = train_df['general_firstup'].map(lambda x: int(x.split('-')[0]) >= 2016)
#train_df = train_df[is_over_year].reset_index(drop=True)
#X_cate = X_cate[is_over_year].reset_index(drop=True)
#drop98_index = train_df.query("biggenre!=98").index.values
#train_df = train_df.loc[drop98_index].reset_index(drop=True)
#X_cate = X_cate.loc[drop98_index].reset_index(drop=True)

fav_novel_cnt_bin = train_df['fav_novel_cnt_bin'].values

test_df['fav_novel_cnt_bin'] = -1
test_dset = NishikaMultiDataset(X_cate_test, test_df)
test_loader = DataLoader(test_dset, batch_size=GCF.BS,
                           pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())

os.makedirs(f'{GCF.MODELS_PATH}', exist_ok=True)
predicts = []
results = []
oof = np.zeros((len(train_df), 5))
skf = StratifiedKFold(n_splits=GCF.N_FOLDS, random_state=GCF.SEED, shuffle=True).split(train_df['ncode'].values, fav_novel_cnt_bin)
for fold, (train_index, valid_index) in enumerate(skf):
    train_dset = NishikaMultiDataset(X_cate.loc[train_index], train_df.loc[train_index])
    valid_dset = NishikaMultiDataset(X_cate.loc[valid_index], train_df.loc[valid_index])

    train_loader = DataLoader(train_dset, batch_size=GCF.BS,
                               pin_memory=True, shuffle=True, drop_last=True, num_workers=os.cpu_count(),
                               worker_init_fn=lambda x: set_seed())
    valid_loader = DataLoader(valid_dset, batch_size=GCF.BS,
                               pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())

    model = NishikaMultiModel(n_features, n_cate_dic)
    model.to(device)

    
    optimizer = AdamW(model.parameters(), lr=GCF.LR, weight_decay=GCF.WEIGHT_DECAY)

    max_train_steps = GCF.N_EPOCHS * len(train_loader) // GCF.ACCUMULATE
    warmup_steps = int(max_train_steps * GCF.WARM_UP_RATIO)
    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=max_train_steps
    )

    bar = tqdm(total=int(GCF.N_EPOCHS * len(train_loader)))
    bar.set_description(f'{GCF.EXP_NAME} Fold-{fold}')

    valid_targert = np.array([np.eye(5)[i] for i in valid_dset.fav_novel_cnt_bin]).astype(int)

    early_stop = 0
    valid_best = float('inf')
    set_seed()
    for epoch in range(GCF.N_EPOCHS):
        if epoch == 3:
            for param in model.transformer_model.parameters():
                param.requires_grad = True
        train_loss, lrs = train_loop(model, train_loader, optimizer, scheduler, bar)
        valid_loss, valid_predict = valid_loop(model, valid_loader)
        valid_score = log_loss(valid_targert, valid_predict)
        print(f'epoch {epoch}, train_loss={train_loss}, valid_loss={valid_loss}, valid_score={valid_score}')
        if valid_best > valid_score:
            oof[valid_index, :] = valid_predict
            valid_best = valid_score
            print('    -> best score update!!')
            torch.save(model.state_dict(), f'{GCF.MODELS_PATH}/m_f{fold}.bin')
            early_stop = 0
        else:
            early_stop += 1

        if early_stop > GCF.PATIENT:
            print("### EARLY STOP ###")
            break
        results.append({
            'train_loss': train_loss,
            'valid_loss': valid_loss,
            'valid_score': valid_score,
        })

    model.load_state_dict(torch.load(f'{GCF.MODELS_PATH}/m_f{fold}.bin'))
    test_preds = test_loop(model, test_loader)
    predicts.append(test_preds)

    pd.DataFrame(results).to_csv(f'{GCF.MODELS_PATH}/result.csv', index=None)

    del model, optimizer, scheduler, valid_targert, bar
    torch.cuda.empty_cache()

oof_score = log_loss(np.stack([np.eye(5)[i] for i in fav_novel_cnt_bin]).astype(int), oof)
print(f"OOF score = {oof_score}")

predicts_avg = np.array(predicts).mean(0)
sub_df[["proba_0","proba_1","proba_2","proba_3","proba_4"]] = predicts_avg
sub_df.to_csv(f"{GCF.RESULT_PATH}/{GCF.EXP_NAME}.csv", index=None)

"""|exp|CV|LB|memo|
|--|--|--|--|
|024|0.7898||カテゴリはすべてonehot, fold-0のみ|
|025|0.7734|0.7186|曜日と時間の復活|
|026|0.7734|0.7127|wholeモデル|
|027|0.7970|0.7301|5 epoch|
|028|0.8114||3 epoch, fold-0のみ|
|029|0.7838|0.7239|学習率減衰|
|030|0.7771||学習率減衰, 2epchでfreeze解除, fold-0のみ|
|031|0.7698|0.6927|genreをonehotに|
|032|0.7682|0.6883|BERTにMLPを追加|
|033|0.7717|0.6900|init 1 layer|
|034|0.7665|0.6990|BERT2layer|
|035|0.7654|0.6973|BERT2layer no init|
|036|0.7672|0.6913|ジャンルのタグを利用|
|037|0.7735|0.6856|2 head|
|038|0.7694|0.6854|MSD|
|||||
|039|0.7662|0.6969|2 layer|
|040|0.7734||4 layer, fold-2まで, 良くない|
|041|0.7667|0.6977|writerを追加|
|||||
|042|0.8947|0.8148|2020年以降のデータのみで学習|
|043|0.9117||2018年以降のデータのみで学習, fold-1まで, 良くない|
|044|||2016年以降のデータのみで学習, バグが見つかり中断|
|||||
|045|0.7288|0.6978|2020年以降のデータのみで学習, バグ修正|
|046|0.7516|0.7098|2018年以降のデータのみで学習, バグ修正|
|046|0.7561|0.7005|2016年以降のデータのみで学習, バグ修正|
|047|0.7795||カテゴリ98除外|
|||||
|048|0.7719|0.6996|BCE|
|049|0.7668|0.6973|URL domain|
|050|0.7656|0.7014|layer norm|
|051|0.7717||mlp 2層, fold-2まで, 良くないので打ち切り|
|052|0.7618|0.6861|embed128|
|053|0.7943||中間層を挟まずにconcat, fold-0のみ, 良くない|
|054|0.7784|0.7014|bert headにrelu+bnorm|
|055|||回帰loss0.5|
"""

!ls

