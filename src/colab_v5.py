# -*- coding: utf-8 -*-
"""なろう_学習.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ejGshCAaZRqdj_ZW_7dWAcaH7j1GUupU

# 小説家になろう ブクマ数予測 \~”伸びる”タイトルとは？\~

## Memo
- pseudo labeling
- target encording
- ncodeを特徴量として利用
- binary特徴をタグとして入れる
- LGBM, CatBoost
- Adversal validation
- ほかのターゲットを使う（例えばジャンル）

### feature
- userやカテゴリでのtarget encoding
- キーワードでのtarget encoding
- binary特徴でのtarget encoding

### Not Work
- 続き、や続編、はkeywordにある
- user_id増やす
- keyword増やす
- lrの修正
- userのこれまでの投稿数
- userのジャンルなどのカウント（時系列を考慮）
"""

!nvidia-smi

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers unidic-lite fugashi ipadic python-Levenshtein sentencepiece
!pip install -U torch

"""## 共通設定"""

import gc
import os
import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from datetime import datetime as dt
from tqdm.notebook import tqdm
from collections import defaultdict
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
import itertools
import Levenshtein

from gensim.corpora.dictionary import Dictionary
from gensim.models import LdaModel, TfidfModel, CoherenceModel
from collections import defaultdict

# Models
import lightgbm as lgb
from sklearn.linear_model import LogisticRegression

import re

# Transformer
import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader

from transformers import get_cosine_schedule_with_warmup
from transformers import AutoConfig
from transformers import AutoTokenizer
from transformers import AutoModel
from transformers import AdamW
#from transformers import T5Tokenizer

device = torch.device("cuda")
scaler = torch.cuda.amp.GradScaler()

class GCF:
    EXP_NAME = 'exp081_drop_shift'

    INPUT_PATH = "/content/drive/MyDrive/Study/Nishika"
    FEATURES_PATH = f"{INPUT_PATH}/features"
    RESULT_PATH = f"{INPUT_PATH}/result"
    MODELS_PATH = f"{INPUT_PATH}/models/{EXP_NAME}"

    N_FOLDS = 5
    SEED = 0

    FEATURES = [
        "pc_or_k",
        "org_bin",
        "genre_ohe", "biggenre_ohe",
        #"kw_ohe_100",
        #"kw_ohe_50",
        "kw_ohe_50_norm",
        #"kw_lda_50_norm",
        "yyyymm",
        "date2",
        "userid_over5",
        "url_count_3",
        "kikaku_v1",
        "autopost",
        "cumsum_v1",
        "user_target_v1",
        "genre_target_v1",
    ]

    MODEL_NAME = "cl-tohoku/bert-base-japanese-whole-word-masking"
    TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME)
    #MODEL_NAME = "rinna/japanese-roberta-base"
    #TOKENIZER = T5Tokenizer.from_pretrained(MODEL_NAME)
    MAX_LEN = 256

    BS = 16
    N_USE_LAYER = 1
    LR = 3e-4
    WEIGHT_DECAY = 1e-3
    N_EPOCHS = 4
    ACCUMULATE = 2
    WARM_UP_RATIO = 0.1
    MLP_HIDDEN = 512
    PATIENT = 8
    N_MSD = 8
    N_EMBED = 128
    DROPOUT_RATIO = 0.2

train_df = pd.read_csv(f"{GCF.INPUT_PATH}/train.csv")
test_df = pd.read_csv(f"{GCF.INPUT_PATH}/test.csv")
sub_df = pd.read_csv(f"{GCF.INPUT_PATH}/sample_submission.csv")

test_df['fav_novel_cnt_bin'] = -1

"""## 特徴抽出

### PCか携帯か
"""

pc_or_k_dic = {0:[0, 0], 1:[1, 0], 2:[0, 1], 3:[1, 1]}
pc_or_k_train = pd.DataFrame(train_df['pc_or_k'].map(pc_or_k_dic).tolist(), columns=['pc', 'keitai'])
pc_or_k_test = pd.DataFrame(test_df['pc_or_k'].map(pc_or_k_dic).tolist(), columns=['pc', 'keitai'])

pc_or_k_train.to_feather(f"{GCF.FEATURES_PATH}/train_pc_or_k.ftr")
pc_or_k_test.to_feather(f"{GCF.FEATURES_PATH}/test_pc_or_k.ftr")

del pc_or_k_test, pc_or_k_train, pc_or_k_dic
gc.collect()

"""### オリジナル2値データ

- 元データそのままの2値データ
- endとisstopはすべての値が0なので無視する
"""

org_bin_feat = ['isr15', 'isbl', 'isgl', 'iszankoku', 'istensei', 'istenni']
org_bin_train = train_df[org_bin_feat]
org_bin_test = test_df[org_bin_feat]

org_bin_train = pd.concat([org_bin_train, train_df[['novel_type']] - 1], axis=1)
org_bin_test = pd.concat([org_bin_test, test_df[['novel_type']] - 1], axis=1)

org_bin_train.to_feather(f"{GCF.FEATURES_PATH}/train_org_bin.ftr")
org_bin_test.to_feather(f"{GCF.FEATURES_PATH}/test_org_bin.ftr")

del org_bin_train, org_bin_test, org_bin_feat
gc.collect()

"""### ジャンル

#### 単純なOne-Hot Encoding

##### ジャンル
"""

oe = OneHotEncoder(sparse=False, dtype = int)
genre_train = oe.fit_transform(train_df[['genre']])
genre_test = oe.transform(test_df[['genre']])

col = [f"genre_{i}" for i in oe.categories_[0]]

genre_train = pd.DataFrame(genre_train, columns=col)
genre_test = pd.DataFrame(genre_test, columns=col)

genre_train.to_feather(f"{GCF.FEATURES_PATH}/train_genre_ohe.ftr")
genre_test.to_feather(f"{GCF.FEATURES_PATH}/test_genre_ohe.ftr")

del genre_train, genre_test, col, oe
gc.collect()

"""##### 大ジャンル"""

oe = OneHotEncoder(sparse=False, dtype = int)
biggenre_train = oe.fit_transform(train_df[['biggenre']])
biggenre_test = oe.transform(test_df[['biggenre']])

col = [f"biggenre_{i}" for i in oe.categories_[0]]

biggenre_train = pd.DataFrame(biggenre_train, columns=col)
biggenre_test = pd.DataFrame(biggenre_test, columns=col)

biggenre_train.to_feather(f"{GCF.FEATURES_PATH}/train_biggenre_ohe.ftr")
biggenre_test.to_feather(f"{GCF.FEATURES_PATH}/test_biggenre_ohe.ftr")

del biggenre_train, biggenre_test, col, oe
gc.collect()

"""#### カテゴリカルencoding"""

genre_dic = {v: i for i, v in enumerate(train_df['genre'].unique())}

train_genre = train_df['genre'].map(genre_dic)
test_genre = test_df['genre'].map(genre_dic)

biggenre_dic = {v: i for i, v in enumerate(train_df['biggenre'].unique())}

train_biggenre = train_df['biggenre'].map(biggenre_dic)
test_biggenre = test_df['biggenre'].map(biggenre_dic)

train_genre_cate = pd.DataFrame(zip(train_genre.values, train_biggenre.values), columns=['genre_cate', 'biggenre_cate'])
test_genre_cate = pd.DataFrame(zip(test_genre.values, test_biggenre.values), columns=['genre_cate', 'biggenre_cate'])

train_genre_cate.to_feather(f"{GCF.FEATURES_PATH}/train_genre_cate.ftr")
test_genre_cate.to_feather(f"{GCF.FEATURES_PATH}/test_genre_cate.ftr")

del genre_dic, biggenre_dic, train_genre, test_genre, train_biggenre, test_biggenre, train_genre_cate, test_genre_cate
gc.collect()

"""### Keyword
- 出現上位100件のonehot
- 出現上位50件のonehot
"""

corpus_keyword = train_df['keyword'].tolist()

d = defaultdict(int)
for words in corpus_keyword:
    if type(words) is float:
        continue
    for w in words.split():
        d[w] += 1

#available_kw = [k for k, v in d.items() if v >= 100]
available_kw = [k for k, v in d.items() if v >= 50]
available_kw_dic = {v: np.eye(len(available_kw))[i] for i, v in enumerate(available_kw)}

def onehot_keywords(kw, dic):
    vec = np.zeros(len(dic))
    if type(kw) is float:
        return vec
    for w in kw.split():
        try:
            ohe = dic[w]
        except KeyError:
            continue
        vec += ohe
    return vec.tolist()

keyword_vecs = train_df['keyword'].map(lambda x: onehot_keywords(x, available_kw_dic)).tolist()
keyword_ohe_train = pd.DataFrame(keyword_vecs, columns=available_kw)
del keyword_vecs
keyword_vecs = test_df['keyword'].map(lambda x: onehot_keywords(x, available_kw_dic)).tolist()
keyword_ohe_test = pd.DataFrame(keyword_vecs, columns=available_kw)

#keyword_ohe_train.to_feather(f"{GCF.FEATURES_PATH}/train_kw_ohe_100.ftr")
#keyword_ohe_test.to_feather(f"{GCF.FEATURES_PATH}/test_kw_ohe_100.ftr")
keyword_ohe_train.to_feather(f"{GCF.FEATURES_PATH}/train_kw_ohe_50.ftr")
keyword_ohe_test.to_feather(f"{GCF.FEATURES_PATH}/test_kw_ohe_50.ftr")

"""#### 企画っぽいやつ"""

kikaku_1 = [
            "ゲラゲラコンテスト３",
            "新人発掘コンテスト",
            "がうがうコン1",
            "夏の夜の恋物語企画",
            '架空戦記創作大会'
]
kikaku_2 = [
            'HJ2021',
            '夏のホラー2021',
            '冬童話2021',          
]
kikaku_3 = [
            'ネット小説大賞九感想',
            'ネット小説大賞七感想',
            'ネット小説大賞八感想',
            'ネット小説大賞九',
]
kikaku_4 = [
            'OVL大賞7M',
            'OVL大賞7',
            'OVL大賞7F',
]
kikaku_5 = [
            'ESN大賞２',
            'ESN大賞',
            'ESN大賞３',
]
kikaku_6 = [            
            'キネノベ大賞２',
            'キネノベ大賞３',
]
kikaku_7 = [               
            '集英社小説大賞2',
            '集英社WEB小説大賞',
            '集英社小説大賞２',
]
kikaku_8 = [
            '123大賞',
            'MBSラジオ短編賞1',
            'マンガＵＰ！賞１',
            'ネット小説大賞八',
            'なろうラジオ大賞2'
]
kikaku_cols = kikaku_1 + kikaku_2 + kikaku_3 + kikaku_4 + kikaku_5 + kikaku_6 + kikaku_7 + kikaku_8

kikaku_train = pd.concat([
           keyword_ohe_train[kikaku_1].max(1),
           keyword_ohe_train[kikaku_2].max(1),
           keyword_ohe_train[kikaku_3].max(1),
           keyword_ohe_train[kikaku_4].max(1),
           keyword_ohe_train[kikaku_5].max(1),
           keyword_ohe_train[kikaku_6].max(1),
           keyword_ohe_train[kikaku_7].max(1),
           keyword_ohe_train[kikaku_8].max(1),
           keyword_ohe_train[kikaku_cols].max(1),
], axis=1)
kikaku_train = (kikaku_train > 0).astype(int)
kikaku_train.columns = [f"kikaku_{i}"for i in range(8)] + ["kikaku_all"]

kikaku_test = pd.concat([
           keyword_ohe_test[kikaku_1].max(1),
           keyword_ohe_test[kikaku_2].max(1),
           keyword_ohe_test[kikaku_3].max(1),
           keyword_ohe_test[kikaku_4].max(1),
           keyword_ohe_test[kikaku_5].max(1),
           keyword_ohe_test[kikaku_6].max(1),
           keyword_ohe_test[kikaku_7].max(1),
           keyword_ohe_test[kikaku_8].max(1),
           keyword_ohe_test[kikaku_cols].max(1),
], axis=1)
kikaku_test = (kikaku_test > 0).astype(int)
kikaku_test.columns = [f"kikaku_{i}"for i in range(8)] + ["kikaku_all"]

kikaku_train.to_feather(f"{GCF.FEATURES_PATH}/train_kikaku_v1.ftr")
kikaku_test.to_feather(f"{GCF.FEATURES_PATH}/test_kikaku_v1.ftr")

del kikaku_cols, kikaku_1, kikaku_2, kikaku_3, kikaku_4, kikaku_5, kikaku_6, kikaku_7, kikaku_8
gc.collect()

"""#### 編集距離で絞り込み"""

'''
for i in keyword_ohe_train.columns:
    if len(i) <= 2:
        continue
    for j in keyword_ohe_train.columns:
        if i == j:
            continue
        if len(j) <= 2:
            continue
        d = Levenshtein.distance(i, j)
        if d < 2:
            print(i, j, d)
'''
norm_word_dic = {
    'コメディー': ['コメディ'],
    'ミステリー': ['ミステリ'],
    '新選組': ['新撰組'],
    '片思い': ['片想い'],
    'ざまあ': ['ざまぁ'],
}
norm_genre_dic = {
    '超能力': ['異能力'],
    '宇宙船': ['宇宙人'],
    #'異世界転移': ['異世界転生'],
}

for k, v in norm_word_dic.items():
    keyword_ohe_train[k] = keyword_ohe_train[k] + keyword_ohe_train[v].sum(1)
    keyword_ohe_train = keyword_ohe_train.drop(v, axis=1)

for k, v in norm_word_dic.items():
    keyword_ohe_test[k] = keyword_ohe_test[k] + keyword_ohe_test[v].sum(1)
    keyword_ohe_test = keyword_ohe_test.drop(v, axis=1)

keyword_ohe_train = (keyword_ohe_train > 0).astype(int)
keyword_ohe_test = (keyword_ohe_test > 0).astype(int)

keyword_ohe_train.to_feather(f"{GCF.FEATURES_PATH}/train_kw_ohe_50_norm.ftr")
keyword_ohe_test.to_feather(f"{GCF.FEATURES_PATH}/test_kw_ohe_50_norm.ftr")

del keyword_ohe_train, keyword_ohe_test, keyword_vecs, available_kw, available_kw_dic, d, w, corpus_keyword
gc.collect()

"""#### LDA"""

keyword_ohe_train = pd.read_feather(f"{GCF.FEATURES_PATH}/train_kw_ohe_50_norm.ftr")
keyword_ohe_test = pd.read_feather(f"{GCF.FEATURES_PATH}/test_kw_ohe_50_norm.ftr")

all_keywords = []
for _, row in keyword_ohe_train.iterrows():
    kws = row[row > 0].index.tolist()
    all_keywords.append(kws)
for _, row in keyword_ohe_test.iterrows():
    kws = row[row > 0].index.tolist()
    all_keywords.append(kws)

dictionary = Dictionary(all_keywords)
corpus = [dictionary.doc2bow(text) for text in all_keywords]
tfidf = TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]

coherence_vals = []
perplexity_vals = []
topic_lst = [10, 20, 30, 40, 50]
for n_topic in topic_lst:
    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=n_topic, random_state=0)
    perplexity_vals.append(np.exp2(-lda_model.log_perplexity(corpus)))
    coherence_model_lda = CoherenceModel(model=lda_model, texts=all_keywords, dictionary=dictionary, coherence='c_v')
    coherence_vals.append(coherence_model_lda.get_coherence())

# Perplexity は低ければ低い程，Coherence は高ければ高い程、良い
pd.DataFrame({
    'n_topic': topic_lst,
    'coherence': coherence_vals,
    'perplexity': perplexity_vals,
}).set_index('n_topic')#.plot()

NUM_TOPICS = 20
lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=NUM_TOPICS, random_state=0)

lda_model.show_topics()

def _lda_vectorizer(d):
    vec = np.zeros(NUM_TOPICS)
    for i, v in lda_model.get_document_topics(d):
        vec[i] = v
    return vec
    
vecs = []
for c in tqdm(corpus):
    v = _lda_vectorizer(c)
    vecs.append(v)

topic_vecs = pd.DataFrame(np.stack(vecs), columns=[f"lda_topic_{i:02}" for i in range(NUM_TOPICS)])
topic_vecs.head()

topic_vecs.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_kw_lda_50_norm.ftr")
topic_vecs.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_kw_lda_50_norm.ftr")

del keyword_ohe_train, keyword_ohe_test, dictionary, corpus, lda, score_by_topic, topic_vecs, vecs, lda_model
gc.collect()

"""### アップロード日付

#### 年・月
"""

year_train = train_df['general_firstup'].map(lambda x: x.split('-')[0]).astype(int)
month_train = train_df['general_firstup'].map(lambda x: x.split('-')[1]).astype(int)
year_test = test_df['general_firstup'].map(lambda x: x.split('-')[0]).astype(int)
month_test = test_df['general_firstup'].map(lambda x: x.split('-')[1]).astype(int)

date_train = pd.DataFrame(zip(year_train.tolist(), month_train.tolist()), columns=['upload_year', 'upload_month'])
date_test = pd.DataFrame(zip(year_test.tolist(), month_test.tolist()), columns=['upload_year', 'upload_month'])

date_train.to_feather(f"{GCF.FEATURES_PATH}/train_yyyymm.ftr")
date_test.to_feather(f"{GCF.FEATURES_PATH}/test_yyyymm.ftr")

del date_train, date_test, month_test, year_test, month_train, year_train
gc.collect()

"""#### 時間帯・曜日

"""

hour_train = train_df['general_firstup'].map(lambda x: int(x.split()[1].split(":")[0])).values
hour_test = test_df['general_firstup'].map(lambda x: int(x.split()[1].split(":")[0])).values

day_of_week_train = train_df['general_firstup'].map(lambda x: dt.strptime(x, '%Y-%m-%d %H:%M:%S').isoweekday()).values
day_of_week_test = test_df['general_firstup'].map(lambda x: dt.strptime(x, '%Y-%m-%d %H:%M:%S').isoweekday()).values

date2_train = pd.DataFrame(zip(hour_train, day_of_week_train), columns=['hour', 'day_of_week'])
date2_test = pd.DataFrame(zip(hour_test, day_of_week_test), columns=['hour', 'day_of_week'])

date2_train.to_feather(f"{GCF.FEATURES_PATH}/train_date2.ftr")
date2_test.to_feather(f"{GCF.FEATURES_PATH}/test_date2.ftr")

del date2_train, date2_test, hour_train, hour_test, day_of_week_train, day_of_week_test
gc.collect()

"""#### 自動投稿か"""

set_time_train = train_df['general_firstup'].map(lambda x: x.split()[-1][3:] == '00:00').astype(int)
set_time_test = test_df['general_firstup'].map(lambda x: x.split()[-1][3:] == '00:00').astype(int)

pd.DataFrame(set_time_train.tolist(), columns=['auto_post']).to_feather(f"{GCF.FEATURES_PATH}/train_autopost.ftr")
pd.DataFrame(set_time_test.tolist(), columns=['auto_post']).to_feather(f"{GCF.FEATURES_PATH}/test_autopost.ftr")

del set_time_train, set_time_test
gc.collect()

"""### URL"""

removes = ['http://', 'https://', 'www.', '.co', '.ne', '.jp', '.com']
def get_domain(string): 
    url = re.findall('https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+', string)
    _url = []
    for u in url:
        for s in removes:
            u = u.replace(s, '')
        _url.append(u)
    return _url

train_urls = train_df['story'].map(get_domain)
test_urls = test_df['story'].map(get_domain)

all_urls = train_urls.tolist() + test_urls.tolist()
domein_count = pd.DataFrame(list(itertools.chain.from_iterable(all_urls)))[0].value_counts()

domein_over3 = domein_count[domein_count > 3].index.values

def get_count_url(urls_lst, domein_over3):
    lst = []
    for urls in urls_lst:
        count_url = np.zeros(len(domein_over3))
        for url in urls:
            count_url += (domein_over3 == url).astype(int)
        lst.append(count_url)
    return pd.DataFrame(lst, columns=domein_over3)

train_url_count = get_count_url(train_urls, domein_over3)
test_url_count = get_count_url(test_urls, domein_over3)

max_count = max(test_url_count.max().max(), train_url_count.max().max())
train_url_count = train_url_count / max_count
test_url_count = test_url_count / max_count

train_url_count.to_feather(f"{GCF.FEATURES_PATH}/train_url_count_3.ftr")
test_url_count.to_feather(f"{GCF.FEATURES_PATH}/test_url_count_3.ftr")

del train_url_count, test_url_count, max_count, domein_over3, domein_count, all_urls, train_urls, test_urls
del removes, get_domain, get_count_url
gc.collect()

"""### One-Hot Encoding"""

month_train = train_df['general_firstup'].map(lambda x: np.eye(12)[int(x.split('-')[1])-1]).tolist()
month_test = test_df['general_firstup'].map(lambda x: np.eye(12)[int(x.split('-')[1])-1]).tolist()

hour_train = train_df['general_firstup'].map(lambda x: np.eye(24)[int(x.split()[1].split(":")[0])]).tolist()
hour_test = test_df['general_firstup'].map(lambda x: np.eye(24)[int(x.split()[1].split(":")[0])]).tolist()

day_of_week_train = train_df['general_firstup'].map(lambda x: np.eye(7)[dt.strptime(x, '%Y-%m-%d %H:%M:%S').isoweekday()-1]).tolist()
day_of_week_test = test_df['general_firstup'].map(lambda x: np.eye(7)[dt.strptime(x, '%Y-%m-%d %H:%M:%S').isoweekday()-1]).tolist()

cols = [f"month_ohe_{i:02}" for i in range(12)] + [f"hour_ohe_{i:02}" for i in range(24)] + [f"day_of_week_ohe_{i}" for i in range(7)]

date_ohe_train = pd.DataFrame(np.concatenate([np.stack(month_train), np.stack(hour_train), np.stack(day_of_week_train)], axis=1), columns=cols)
date_ohe_test = pd.DataFrame(np.concatenate([np.stack(month_test), np.stack(hour_test), np.stack(day_of_week_test)], axis=1), columns=cols)

date_ohe_train.to_feather(f"{GCF.FEATURES_PATH}/train_date_ohe.ftr")
date_ohe_test.to_feather(f"{GCF.FEATURES_PATH}/test_date_ohe.ftr")

del month_train, month_test, hour_train, hour_test, day_of_week_train, day_of_week_test, date_ohe_train, date_ohe_test, cols
gc.collect()

"""### UserID"""

# 投稿が多いからと言ってスコアが高いわけではなさそう
#lst = []
#for userid, df in train_df.groupby('userid'):
#    d = len(df), df['fav_novel_cnt_bin'].max(), df['fav_novel_cnt_bin'].min(), df['fav_novel_cnt_bin'].mean(), df['fav_novel_cnt_bin'].median()
#    lst.append(d)
#pd.DataFrame(lst).plot.scatter(x=0, y=4)

all_userid = test_df['userid'].tolist() + train_df['userid'].tolist()

dic = {}
idx = 0
for _id in set(all_userid):
    n = all_userid.count(_id)
    if n < 6:
        continue
    dic[_id] = idx
    idx += 1

def get_userid_idx(_id, dic):
    try:
        return dic[_id]
    except KeyError:
        return len(dic)

userid_train = train_df['userid'].map(lambda x: get_userid_idx(x, dic))
userid_test = test_df['userid'].map(lambda x: get_userid_idx(x, dic))

userid_train = pd.DataFrame(userid_train.tolist(), columns=['userid_over5'])
userid_test = pd.DataFrame(userid_test.tolist(), columns=['userid_over5'])

userid_train.to_feather(f"{GCF.FEATURES_PATH}/train_userid_over5.ftr")
userid_test.to_feather(f"{GCF.FEATURES_PATH}/test_userid_over5.ftr")

del userid_train, userid_test, dic, idx, n, all_userid
gc.collect()

"""### userごとの特徴"""

def get_date_diff(df):
    _date = df['general_firstup'].map(lambda x: dt.strptime(x, "%Y-%m-%d %H:%M:%S"))
    dt_diff_prev = _date - _date.shift(1)
    dt_diff_next =  _date.shift(-1) - _date
    dt_diff_prev = dt_diff_prev.map(lambda x: x.days)
    dt_diff_next = dt_diff_next.map(lambda x: x.days)
    dt_diff_init = _date - _date.iloc[0]

    dic = {}
    for i in range(12):
        d = (i+1)*30
        dic[f"prev_over_{d}"] = dt_diff_prev > d
        dic[f"next_over_{d}"] = dt_diff_next > d
    date_diff = pd.DataFrame(dic).astype(int)
    date_diff['init_diff'] = dt_diff_init.map(lambda x: x.days)
    return date_diff

genre_ohe_train = pd.read_feather(f"{GCF.FEATURES_PATH}/train_genre_ohe.ftr")
genre_ohe_test = pd.read_feather(f"{GCF.FEATURES_PATH}/test_genre_ohe.ftr")
all_df = pd.concat([
    pd.concat([train_df, genre_ohe_train], axis=1),
    pd.concat([test_df, genre_ohe_test], axis=1),
], axis=0).reset_index(drop=True)

org_bin_feat = ['isr15', 'isbl', 'isgl', 'iszankoku', 'istensei', 'istenni']
genre_feat = genre_ohe_train.columns.tolist()
del genre_ohe_train, genre_ohe_test

feats = np.zeros((len(all_df), len(org_bin_feat) + len(genre_feat)))
for userid, df in all_df.groupby('userid'):
    feats[df.index, :] = df[org_bin_feat+genre_feat].cumsum().values
cumsum_feat_df = pd.DataFrame(feats, columns=org_bin_feat+genre_feat)
del feats

post_count = pd.DataFrame(zip([1 for _ in range(len(all_df))], all_df['userid'].tolist())).groupby(1).cumsum(0)
post_count.columns = ['post_count']
cumsum_feat_norm_df = pd.DataFrame(cumsum_feat_df.values / post_count.values, columns=[f"cumsum_{c}_norm"for c in org_bin_feat+genre_feat])
post_count_norm = post_count/post_count.max()
post_count_norm.columns = ['post_count_norm']

cumsum_feat_norm_df = pd.concat([post_count_norm, cumsum_feat_norm_df], axis=1)
del post_count_norm, post_count

cumsum_feat_norm_df.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_cumsum_v1.ftr")
cumsum_feat_norm_df.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_cumsum_v1.ftr")

del all_df, org_bin_feat, genre_feat, cumsum_feat_norm_df, cumsum_feat_df
gc.collect()

"""### Target encoding"""

all_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)

"""#### UserID"""

def get_cumsum(target):
    res = []
    lst = []
    for i, v in enumerate(target):
        if v == -1:
            max_v, min_v, avg_v, med_v, std_v, var_v = -1, -1, -1, -1, -1, -1
        else:
            lst.append(v)
            max_v = max(lst) / 5
            min_v = min(lst) / 5
            avg_v = sum(lst)/len(lst)
            med_v = np.median(lst)
            std_v = np.std(lst)
            var_v = np.var(lst)
        res.append({
            'max_v': max_v,
            'min_v': min_v,
            'avg_v': avg_v,
            'med_v': med_v,
            'std_v': std_v,
            'var_v': var_v,
        })
    return pd.DataFrame(res)

target_feat = np.zeros((len(all_df), 9))
for userid, df in tqdm(all_df.groupby('userid'), total=len(all_df['userid'].unique())):
    _df = pd.concat([df['fav_novel_cnt_bin'].shift(1), df['fav_novel_cnt_bin'].shift(2), df['fav_novel_cnt_bin'].shift(3)], axis=1).fillna(-2)
    _df = _df.applymap(lambda x: None if x == -1 else x).fillna(method='ffill').applymap(lambda x: -1 if x == -2 else x).fillna(method='ffill')
    _df.columns = ['target_1', 'target_2', 'target_3']
    cumcum_df = get_cumsum(_df['target_1'])
    _feat = pd.concat([_df.reset_index(drop=True), cumcum_df], axis=1)
    target_feat[df.index, :] = _feat.values
user_target_feat_df = pd.DataFrame(target_feat, columns=_feat.columns)

user_target_feat_df.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_user_target_v1.ftr")
user_target_feat_df.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_user_target_v1.ftr")

del user_target_feat_df, target_feat
gc.collect()

"""#### カテゴリ"""

date_df = pd.DataFrame(all_df['general_firstup'].map(lambda x: x.split()[0].split('-')[:2]).tolist(), columns=['year', 'month'])
cate_df = pd.concat([date_df, all_df[['genre', 'fav_novel_cnt_bin']]], axis=1)
cate_df.loc[cate_df.query('year=="2021" and month=="09"').index, 'month'] = "08"
cate_df['yyyymm_genre'] = cate_df.apply(lambda x: f"{x.year}{x.month}_{x.genre}",axis=1)
cate_df['yyyy_genre'] = cate_df.apply(lambda x: f"{x.year}_{x.genre}",axis=1)
_cate_df = cate_df.query('fav_novel_cnt_bin!=-1')

dfs = []
for col in ['genre', 'yyyy_genre', 'yyyymm_genre']:
    dic = _cate_df.groupby(col).mean()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(cate_df[col].map(dic).values, columns=[f"{col}_mean"])
    dfs.append(_df)
    dic = _cate_df.groupby(col).std()['fav_novel_cnt_bin'].to_dict()
    _df = pd.DataFrame(cate_df[col].map(dic).values, columns=[f"{col}_std"])
    dfs.append(_df)
genre_target_df = pd.concat(dfs, axis=1).fillna(0)

genre_target_df.iloc[:len(train_df)].to_feather(f"{GCF.FEATURES_PATH}/train_genre_target_v1.ftr")
genre_target_df.iloc[len(train_df):].reset_index(drop=True).to_feather(f"{GCF.FEATURES_PATH}/test_genre_target_v1.ftr")

"""## ニューラルネット
- マルチモーダル

### Multimodal Model & Dataset
"""

class NishikaMultiDataset(Dataset):
    def __init__(self, X_cate, df):
        self.X_cate = X_cate
        self.title = df['title_genre_tag'].tolist()
        self.story = df["story"].tolist()
        self.fav_novel_cnt_bin = df["fav_novel_cnt_bin"].tolist()

    def __len__(self):
        return len(self.X_cate)
    
    def __getitem__(self, item):
        X_cate = self.X_cate.iloc[item]
        fav_novel_cnt_bin = self.fav_novel_cnt_bin[item]
        title = self.title[item]
        story = self.story[item]
        fav_novel_cnt_bin = self.fav_novel_cnt_bin[item]

        tok = GCF.TOKENIZER.encode_plus(
            title,
            story,
            truncation='only_second',
            max_length=GCF.MAX_LEN,
            padding='max_length'
        )

        d = {
            "X_cate": torch.tensor(X_cate, dtype=torch.float),
            "input_ids": torch.tensor(tok['input_ids'], dtype=torch.long),
            "attention_mask": torch.tensor(tok['attention_mask'], dtype=torch.long),
            #"token_type_ids": torch.tensor(tok['token_type_ids'], dtype=torch.long),
            "fav_novel_cnt_bin": torch.tensor(fav_novel_cnt_bin, dtype=torch.long),
        }
        return d

class NishikaMultiModel(nn.Module):
    
    def __init__(self, n_features, n_cate_dic):
        super(NishikaMultiModel, self).__init__()
        # BERT
        self.config = AutoConfig.from_pretrained(GCF.MODEL_NAME)
        self.config.attention_probs_dropout_prob = GCF.DROPOUT_RATIO
        self.config.hidden_dropout_prob = GCF.DROPOUT_RATIO
        self.config.output_hidden_states = True
        self.transformer_model = AutoModel.from_pretrained(
            GCF.MODEL_NAME, 
            config=self.config,
        )
        self.transformer_head = nn.Sequential(
            nn.Linear(self.config.hidden_size*GCF.N_USE_LAYER, self.config.hidden_size*GCF.N_USE_LAYER),
        )
        
        # MLP
        self.userid_emb = nn.Embedding(n_cate_dic['n_userid'], GCF.N_EMBED, padding_idx=0)
        self.mlp1 = nn.Sequential(
            nn.Linear(n_features+GCF.N_EMBED, GCF.MLP_HIDDEN),
            #nn.BatchNorm1d(GCF.MLP_HIDDEN),
            nn.Dropout(GCF.DROPOUT_RATIO),
            nn.ReLU(),
        )
        self.mlp2 = nn.Sequential(
            nn.Linear(GCF.MLP_HIDDEN, GCF.MLP_HIDDEN),
            #nn.BatchNorm1d(GCF.MLP_HIDDEN),
            nn.Dropout(GCF.DROPOUT_RATIO),
            nn.ReLU(),
        )

        # head
        self.classifier_1 = nn.Linear(self.config.hidden_size*GCF.N_USE_LAYER+GCF.MLP_HIDDEN, 5)
        self.classifier_2 = nn.Linear(self.config.hidden_size*GCF.N_USE_LAYER+GCF.MLP_HIDDEN, 5)
        self.dropouts_1 = nn.ModuleList([nn.Dropout(GCF.DROPOUT_RATIO) for _ in range(GCF.N_MSD)])
        self.dropouts_2 = nn.ModuleList([nn.Dropout(GCF.DROPOUT_RATIO) for _ in range(GCF.N_MSD)])

        self.transformer_model.resize_token_embeddings(len(GCF.TOKENIZER))
        self._init_weights(self.userid_emb)
        self._init_weights(self.mlp1)
        self._init_weights(self.mlp2)
        self._init_weights(self.transformer_head)
        self._init_weights(self.classifier_1)
        self._init_weights(self.classifier_2)

        # transformer freeze
        for param in self.transformer_model.parameters():
            param.requires_grad = False

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)

    def forward(self, X_cate, input_ids, attention_mask, token_type_ids=None, y=None):
        # BERT
        outputs = self.transformer_model(input_ids, attention_mask) #, token_type_ids)
        sequence_output = torch.cat([outputs["hidden_states"][-1*i].mean(1) for i in range(1, GCF.N_USE_LAYER+1)], dim=1)
        sequence_output = self.transformer_head(sequence_output)
        # MLP
        e = self.userid_emb(X_cate[:, -1].long())
        h = torch.cat([X_cate[:, :-1], e], dim=1)
        h = self.mlp1(h)
        h = self.mlp2(h)
        # head
        cat_h = torch.cat([sequence_output, h], dim=1)
        logits_1 = sum([self.classifier_1(dropout(cat_h)) for dropout in self.dropouts_1]) / GCF.N_MSD
        logits_2 = sum([self.classifier_2(dropout(cat_h)) for dropout in self.dropouts_2]) / GCF.N_MSD

        mask = X_cate[:, 8]
        logits =  logits_1, logits_2, mask
        if y is not None:
            loss = self.loss_fn(logits, y)
        else:
            loss = None
        dual_logits = logits_1 * (1 - mask).unsqueeze(1) + logits_2 * mask.unsqueeze(1)

        return dual_logits, loss

    def loss_fn(self, y_pred, y_true):
        y_pred_1, y_pred_2, mask = y_pred
        loss_1 = nn.CrossEntropyLoss(reduction='none')(y_pred_1, y_true)
        loss_2 = nn.CrossEntropyLoss(reduction='none')(y_pred_2, y_true)
        loss = loss_1 * (1 - mask) + loss_2 * mask
        return loss.mean()

"""### Main Processing"""

def set_seed(seed=GCF.SEED):
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def train_loop(model, train_loader, optimizer, scheduler, epoch, bar=None):
    n_step = 0
    losses = []
    predicts = []
    targets = []
    lrs = []
    model.train()
    if epoch == 3:
        for param in model.transformer_model.parameters():
            param.requires_grad = True
        #for param in model.userid_emb.parameters():
        #    param.requires_grad = False
        #for param in model.mlp1.parameters():
        #    param.requires_grad = False
        #for param in model.mlp2.parameters():
        #    param.requires_grad = False
    # else:
    #    model.transformer_model.eval()
    optimizer.zero_grad()
    for d in train_loader:    
        with torch.cuda.amp.autocast(): 
            logits, loss = model(
                d['X_cate'].to(device),
                d['input_ids'].to(device),
                d['attention_mask'].to(device),
                #d['token_type_ids'].to(device),
                y=d['fav_novel_cnt_bin'].to(device),
            )
            loss = loss / GCF.ACCUMULATE

        predicts.append(logits.detach())
        targets.append(d['fav_novel_cnt_bin'].numpy())
        losses.append(loss.item()*GCF.ACCUMULATE)
        lr = np.array([param_group["lr"] for param_group in optimizer.param_groups]).mean()
        lrs.append(lr)

        scaler.scale(loss).backward()

        if n_step % GCF.ACCUMULATE == 0:
            scaler.step(optimizer) 
            scaler.update() 
            optimizer.zero_grad()
            scheduler.step()   
        n_step += 1
        if bar is not None:
            bar.update(1)
    loss = np.array(losses).mean()
    predict = torch.vstack(predicts).cpu().softmax(1).numpy()
    targert = np.array([np.eye(5)[i] for i in np.hstack(targets)]).astype(int)
    score = log_loss(targert, predict)
    return loss, score, lrs

def valid_loop(model, valid_loader):
    losses = []
    predicts = []
    model.eval()
    for d in valid_loader:    
        with torch.no_grad():
            logits, loss = model(
                d['X_cate'].to(device),
                d['input_ids'].to(device),
                d['attention_mask'].to(device),
                #d['token_type_ids'].to(device),
                y=d['fav_novel_cnt_bin'].to(device),
            )

        predicts.append(logits)
        losses.append(loss.item())
    predict = torch.vstack(predicts).cpu().softmax(1).numpy()
    loss = np.array(losses).mean()
    return loss, predict

def test_loop(model, test_loader):
    predicts = []
    model.eval()
    for d in test_loader:    
        with torch.no_grad():
            logits, _ = model(
                d['X_cate'].to(device),
                d['input_ids'].to(device),
                d['attention_mask'].to(device),
                #d['token_type_ids'].to(device),
            )
        predicts.append(logits)
    predict = torch.vstack(predicts).cpu().softmax(1).numpy()
    return predict

def norm_url(text,):
    url_pattern = r"https?://[\w/:%#\$&\?\(\)~\.=\+\-… ]+"
    return re.sub(url_pattern, 'URL', text)

X_cate = pd.concat([pd.read_feather(f"{GCF.FEATURES_PATH}/train_{i}.ftr") for i in GCF.FEATURES], axis=1)
X_cate_test = pd.concat([pd.read_feather(f"{GCF.FEATURES_PATH}/test_{i}.ftr") for i in GCF.FEATURES], axis=1)

# year
X_cate['upload_year_norm'] = X_cate['upload_year'].map(lambda x: (x-2007)/14)
X_cate_test['upload_year_norm'] = X_cate_test['upload_year'].map(lambda x: (x-2007)/14)
# monthy
X_cate['month_sin'] = np.sin(2 * np.pi * X_cate['upload_month']/12)
X_cate['month_cos'] = np.cos(2 * np.pi * X_cate['upload_month']/12)
X_cate_test['month_sin'] = np.sin(2 * np.pi * X_cate_test['upload_month']/12)
X_cate_test['month_cos'] = np.cos(2 * np.pi * X_cate_test['upload_month']/12)
# hour
X_cate['hour_sin'] = np.sin(2 * np.pi * X_cate['hour']/24)
X_cate['hour_cos'] = np.cos(2 * np.pi * X_cate['hour']/24)
X_cate_test['hour_sin'] = np.sin(2 * np.pi * X_cate_test['hour']/24)
X_cate_test['hour_cos'] = np.cos(2 * np.pi * X_cate_test['hour']/24)
# day_of_week
X_cate['day_of_week_sin'] = np.sin(2 * np.pi * X_cate['day_of_week']/7)
X_cate['day_of_week_cos'] = np.cos(2 * np.pi * X_cate['day_of_week']/7)
X_cate_test['day_of_week_sin'] = np.sin(2 * np.pi * X_cate_test['day_of_week']/7)
X_cate_test['day_of_week_cos'] = np.cos(2 * np.pi * X_cate_test['day_of_week']/7)

# カテゴリ変数
# userid
userid_over5 = X_cate['userid_over5']
userid_over5_test = X_cate_test['userid_over5']
n_userid = len(set(userid_over5.tolist() + userid_over5_test.tolist()))
# カテゴリ数の辞書
n_cate_dic = {
    'n_userid': n_userid,
}

# 不要カラム削除
drop_features = ['upload_year', 'upload_month', 'userid_over5', 'hour', 'day_of_week'] + ['target_1', 'target_2', 'target_3'] #+ ['genre_9801', 'biggenre_98']
X_cate = X_cate.drop(drop_features, axis=1)
X_cate_test = X_cate_test.drop(drop_features, axis=1)
n_features = X_cate.shape[1]

X_cate['userid_over5'] = userid_over5
X_cate_test['userid_over5'] = userid_over5_test

# titleにgenreのタグを追加
genre_tag_dic = {g: f'[GENRE_{g:04}]' for g in train_df['genre'].unique()}
GCF.TOKENIZER.add_tokens(list(genre_tag_dic.values()))
print('add token:', GCF.TOKENIZER.added_tokens_encoder)
train_df['title_genre_tag'] = train_df.apply(lambda x: genre_tag_dic[x['genre']] + x['title'], axis=1)
test_df['title_genre_tag'] = test_df.apply(lambda x: genre_tag_dic[x['genre']] + x['title'], axis=1)

# テキストの正規化
train_df['story_norm'] = train_df['story'].map(norm_url)
test_df['story_norm'] = test_df['story'].map(norm_url)

fav_novel_cnt_bin = train_df['fav_novel_cnt_bin'].values

test_dset = NishikaMultiDataset(X_cate_test, test_df)
test_loader = DataLoader(test_dset, batch_size=GCF.BS,
                           pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())

os.makedirs(f'{GCF.MODELS_PATH}', exist_ok=True)
predicts = []
results = []
oof = np.zeros((len(train_df), 5))
skf = StratifiedKFold(n_splits=GCF.N_FOLDS, random_state=GCF.SEED, shuffle=True).split(train_df['ncode'].values, fav_novel_cnt_bin)
for fold, (train_index, valid_index) in enumerate(skf):
    train_dset = NishikaMultiDataset(X_cate.loc[train_index], train_df.loc[train_index])
    valid_dset = NishikaMultiDataset(X_cate.loc[valid_index], train_df.loc[valid_index])

    train_loader = DataLoader(train_dset, batch_size=GCF.BS,
                               pin_memory=True, shuffle=True, drop_last=True, num_workers=os.cpu_count(),
                               worker_init_fn=lambda x: set_seed())
    valid_loader = DataLoader(valid_dset, batch_size=GCF.BS,
                               pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())

    model = NishikaMultiModel(n_features, n_cate_dic)
    model.to(device)
    
    optimizer = AdamW(model.parameters(), lr=GCF.LR, weight_decay=GCF.WEIGHT_DECAY)

    max_train_steps = GCF.N_EPOCHS * len(train_loader) // GCF.ACCUMULATE
    warmup_steps = int(max_train_steps * GCF.WARM_UP_RATIO)
    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=max_train_steps
    )

    bar = tqdm(total=int(GCF.N_EPOCHS * len(train_loader)))
    bar.set_description(f'{GCF.EXP_NAME} Fold-{fold}')

    valid_targert = np.array([np.eye(5)[i] for i in valid_dset.fav_novel_cnt_bin]).astype(int)

    early_stop = 0
    valid_best = float('inf')
    set_seed()
    for epoch in range(GCF.N_EPOCHS):
        train_loss, train_score, lrs = train_loop(model, train_loader, optimizer, scheduler, epoch, bar)
        valid_loss, valid_predict = valid_loop(model, valid_loader)
        valid_score = log_loss(valid_targert, valid_predict)

        print(f'epoch {epoch}, train_loss={train_loss}, valid_loss={valid_loss}, train_score={train_score}, valid_score={valid_score}')

        if valid_best > valid_score:
            oof[valid_index, :] = valid_predict
            valid_best = valid_score
            print('    -> best score update!!')
            torch.save(model.state_dict(), f'{GCF.MODELS_PATH}/m_f{fold}.bin')
            early_stop = 0
        else:
            early_stop += 1

        if early_stop > GCF.PATIENT:
            print("### EARLY STOP ###")
            break
        results.append({
            'fold': fold,
            'train_loss': train_loss,
            'valid_loss': valid_loss,
            'train_score': train_score,
            'valid_score': valid_score,
        })
        pd.DataFrame(results).to_csv(f'{GCF.MODELS_PATH}/result.csv', index=None)

    model.load_state_dict(torch.load(f'{GCF.MODELS_PATH}/m_f{fold}.bin'))
    test_preds = test_loop(model, test_loader)
    predicts.append(test_preds)

    del model, optimizer, scheduler, valid_targert, bar
    torch.cuda.empty_cache()

oof_score = log_loss(np.stack([np.eye(5)[i] for i in fav_novel_cnt_bin]).astype(int), oof)
print(f"OOF score = {oof_score}")

predicts_avg = np.array(predicts).mean(0)
sub_df[["proba_0","proba_1","proba_2","proba_3","proba_4"]] = predicts_avg
sub_df.to_csv(f"{GCF.RESULT_PATH}/{GCF.EXP_NAME}.csv", index=None)

"""|exp|CV|LB|memo|
|--|--|--|--|
|073|0.7548|0.6998|mlp dropout|
|074|0.7623|0.7061|dropout04|
|075|0.7550||layer2, fold-2まで, 良さそうだけど本質的じゃなさそう<br>exp073がfold-2までで0.7542なので結局ダメそう？|
|076|0.7580|0.7004|no unfreeze|
|||||
|077|0.7537|0.6930|ジャンルの累計特徴＋自動投稿か否か|
|078|0.7551|0.6945|LDA|
|079|0.7681|0.7006|rinna RoBERTa|
|080|0.7180|0.6742|target encoding|

"""

!ls

GCF.TOKENIZER

