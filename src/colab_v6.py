# -*- coding: utf-8 -*-
"""なろう_学習.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ejGshCAaZRqdj_ZW_7dWAcaH7j1GUupU

# 小説家になろう ブクマ数予測 \~”伸びる”タイトルとは？\~

## Memo
- pseudo labeling
- ncodeを特徴量として利用
- binary特徴をタグとして入れる
- Adversal validation
- ほかのターゲットを使う（例えばジャンル）
- [Focal Loss](https://github.com/clcarwin/focal_loss_pytorch/blob/master/focalloss.py)
"""

!nvidia-smi

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers unidic-lite fugashi ipadic python-Levenshtein sentencepiece
!pip install -U torch

"""## 共通設定"""

import gc
import os
import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle

from datetime import datetime as dt
from tqdm.notebook import tqdm
from collections import defaultdict
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
import itertools
import Levenshtein

from gensim.corpora.dictionary import Dictionary
from gensim.models import LdaModel, TfidfModel, CoherenceModel
from collections import defaultdict

import re

# Transformer
import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader

from transformers import get_cosine_schedule_with_warmup
from transformers import AutoConfig
from transformers import AutoTokenizer
from transformers import AutoModel
from transformers import AdamW

device = torch.device("cuda")
scaler = torch.cuda.amp.GradScaler()

class GCF:
    EXP_NAME = 'exp105_1e4'

    INPUT_PATH = "/content/drive/MyDrive/Study/Nishika"
    FEATURES_PATH = f"{INPUT_PATH}/features"
    RESULT_PATH = f"{INPUT_PATH}/result"
    MODELS_PATH = f"{INPUT_PATH}/models/{EXP_NAME}"

    N_FOLDS = 5
    SEED = 0

    FEATURES = [
        "pc_or_k",
        "org_bin",
        "genre_ohe", "biggenre_ohe",
        #"kw_ohe_100",
        #"kw_ohe_50",
        "kw_ohe_50_norm",
        #"kw_lda_50_norm",
        "yyyymm",
        "date2",
        "userid_over5",
        "url_count_3",
        "kikaku_v1",
        "autopost",
        "cumsum_v1",
        "user_target_v1",
        #"user_target_v2_leak_fix",
        #"genre_target_v1",
        "genre_target_v2",
        #"lda_thr00",
        "bin_target",
        "date_target_v1",
        #"date_diff_v1",
        "kw_none",
        #"user_target_pseudo_v1",
        #"user_window_pseudo_v1",
    ]

    MODEL_NAME = "cl-tohoku/bert-base-japanese-whole-word-masking"
    TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME)
    MAX_LEN = 256

    BS = 16
    N_USE_LAYER = 1
    LR = 3e-4
    WEIGHT_DECAY = 1e-3
    N_EPOCHS = 4
    ACCUMULATE = 2
    WARM_UP_RATIO = 0.1
    MLP_HIDDEN = 1024
    PATIENT = 8
    N_MSD = 8
    N_EMBED = 128
    DROPOUT_RATIO = 0.2

train_df = pd.read_csv(f"{GCF.INPUT_PATH}/train.csv")
test_df = pd.read_csv(f"{GCF.INPUT_PATH}/test.csv")
sub_df = pd.read_csv(f"{GCF.INPUT_PATH}/sample_submission.csv")

test_df['fav_novel_cnt_bin'] = -1

"""## ニューラルネット
- マルチモーダル

### Multimodal Model & Dataset
"""

class NishikaMultiDataset(Dataset):
    def __init__(self, X_cate, df):
        self.X_cate = X_cate
        self.title = df['title_genre_tag'].tolist()
        self.story = df["story"].tolist()
        self.fav_novel_cnt_bin = df["fav_novel_cnt_bin"].tolist()

    def __len__(self):
        return len(self.X_cate)
    
    def __getitem__(self, item):
        X_cate = self.X_cate.iloc[item]
        fav_novel_cnt_bin = self.fav_novel_cnt_bin[item]
        title = self.title[item]
        story = self.story[item]
        fav_novel_cnt_bin = self.fav_novel_cnt_bin[item]

        tok = GCF.TOKENIZER.encode_plus(
            title,
            story,
            truncation='only_second',
            max_length=GCF.MAX_LEN,
            padding='max_length'
        )

        d = {
            "X_cate": torch.tensor(X_cate, dtype=torch.float),
            "input_ids": torch.tensor(tok['input_ids'], dtype=torch.long),
            "attention_mask": torch.tensor(tok['attention_mask'], dtype=torch.long),
            "token_type_ids": torch.tensor(tok['token_type_ids'], dtype=torch.long),
            "fav_novel_cnt_bin": torch.tensor(fav_novel_cnt_bin, dtype=torch.long),
        }
        return d

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable

class FocalLoss(nn.Module):
    def __init__(self, gamma=0, alpha=None, size_average=True):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha
        #if isinstance(alpha,(float,int,long)): self.alpha = torch.Tensor([alpha,1-alpha])
        #if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)
        #self.size_average = size_average

    def forward(self, input, target):
        if input.dim()>2:
            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W
            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C
            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C
        target = target.view(-1,1)

        logpt = F.log_softmax(input)
        logpt = logpt.gather(1,target)
        logpt = logpt.view(-1)
        pt = Variable(logpt.data.exp())

        if self.alpha is not None:
            if self.alpha.type()!=input.data.type():
                self.alpha = self.alpha.type_as(input.data)
            at = self.alpha.gather(0,target.data.view(-1))
            logpt = logpt * Variable(at)

        loss = -1 * (1-pt)**self.gamma * logpt
        #if self.size_average: return loss.mean()
        #else: return loss.sum()
        return loss

class NishikaMultiModel(nn.Module):
    
    def __init__(self, n_features, n_cate_dic):
        super(NishikaMultiModel, self).__init__()
        # BERT
        self.config = AutoConfig.from_pretrained(GCF.MODEL_NAME)
        self.config.attention_probs_dropout_prob = GCF.DROPOUT_RATIO
        self.config.hidden_dropout_prob = GCF.DROPOUT_RATIO
        self.config.output_hidden_states = True
        self.transformer_model = AutoModel.from_pretrained(
            GCF.MODEL_NAME, 
            config=self.config,
        )
        self.transformer_head = nn.Sequential(
            nn.Linear(self.config.hidden_size*GCF.N_USE_LAYER, self.config.hidden_size*GCF.N_USE_LAYER),
            #nn.Dropout(GCF.DROPOUT_RATIO),
            #nn.ReLU(),
            #nn.Linear(self.config.hidden_size*GCF.N_USE_LAYER, self.config.hidden_size*GCF.N_USE_LAYER),
            #nn.Dropout(GCF.DROPOUT_RATIO),
            #nn.ReLU(),
        )
        
        # MLP
        self.userid_emb = nn.Embedding(n_cate_dic['n_userid'], GCF.N_EMBED, padding_idx=0)
        self.mlp1 = nn.Sequential(
            nn.Linear(n_features+GCF.N_EMBED, GCF.MLP_HIDDEN),
            nn.Dropout(GCF.DROPOUT_RATIO),
            nn.ReLU(),
        )
        self.mlp2 = nn.Sequential(
            nn.Linear(GCF.MLP_HIDDEN, GCF.MLP_HIDDEN),
            nn.Dropout(GCF.DROPOUT_RATIO),
            nn.ReLU(),
        )
        #self.mlp3 = nn.Sequential(
        #    nn.Linear(GCF.MLP_HIDDEN, GCF.MLP_HIDDEN),
        #    nn.Dropout(GCF.DROPOUT_RATIO),
        #    nn.ReLU(),
        #)

        # head
        self.head_mlp = nn.Linear(self.config.hidden_size*GCF.N_USE_LAYER+GCF.MLP_HIDDEN, self.config.hidden_size*GCF.N_USE_LAYER+GCF.MLP_HIDDEN)
        self.classifier_1 = nn.Linear(self.config.hidden_size*GCF.N_USE_LAYER+GCF.MLP_HIDDEN, 5)
        self.classifier_2 = nn.Linear(self.config.hidden_size*GCF.N_USE_LAYER+GCF.MLP_HIDDEN, 5)
        self.dropouts_1 = nn.ModuleList([nn.Dropout(GCF.DROPOUT_RATIO) for _ in range(GCF.N_MSD)])
        self.dropouts_2 = nn.ModuleList([nn.Dropout(GCF.DROPOUT_RATIO) for _ in range(GCF.N_MSD)])

        self.transformer_model.resize_token_embeddings(len(GCF.TOKENIZER))
        self._init_weights(self.userid_emb)
        self._init_weights(self.mlp1[0])
        self._init_weights(self.mlp2[0])
        #self._init_weights(self.mlp3[0])
        self._init_weights(self.transformer_head[0])
        #self._init_weights(self.transformer_head[3]) 
        self._init_weights(self.head_mlp)
        self._init_weights(self.classifier_1)
        self._init_weights(self.classifier_2)

        # transformer freeze
        for param in self.transformer_model.parameters():
            param.requires_grad = False

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)

    def forward(self, X_cate, input_ids, attention_mask, token_type_ids=None, y=None, weights=None):
        # BERT
        outputs = self.transformer_model(input_ids, attention_mask, token_type_ids)
        sequence_output = torch.cat([outputs["hidden_states"][-1*i].mean(1) for i in range(1, GCF.N_USE_LAYER+1)], dim=1)
        sequence_output = self.transformer_head(sequence_output)
        # MLP
        e = self.userid_emb(X_cate[:, -1].long())
        h = torch.cat([X_cate[:, :-1], e], dim=1)
        h = self.mlp1(h)
        h = self.mlp2(h)
        #h = self.mlp3(h)
        # head
        cat_h = torch.cat([sequence_output, h], dim=1)
        cat_h = self.head_mlp(cat_h)
        logits_1 = sum([self.classifier_1(dropout(cat_h)) for dropout in self.dropouts_1]) / GCF.N_MSD
        logits_2 = sum([self.classifier_2(dropout(cat_h)) for dropout in self.dropouts_2]) / GCF.N_MSD

        mask = X_cate[:, 8]
        logits =  logits_1, logits_2, mask
        if y is not None:
            loss = self.loss_fn(logits, y, weights)
        else:
            loss = None
        dual_logits = logits_1 * (1 - mask).unsqueeze(1) + logits_2 * mask.unsqueeze(1)

        return dual_logits, loss

    def loss_fn(self, y_pred, y_true, weights=None):
        y_pred_1, y_pred_2, mask = y_pred
        #if weights is not None:
        #    loss_1 = nn.CrossEntropyLoss(reduction='none', weight=weights)(y_pred_1, y_true)
        #    loss_2 = nn.CrossEntropyLoss(reduction='none', weight=weights)(y_pred_2, y_true)
        #else:
        #    loss_1 = nn.CrossEntropyLoss(reduction='none', label_smoothing=0.0)(y_pred_1, y_true)
        #    loss_2 = nn.CrossEntropyLoss(reduction='none', label_smoothing=0.0)(y_pred_2, y_true)
        #loss_1 = FocalLoss(gamma=2.0)(y_pred_1, y_true)
        #loss_2 = FocalLoss(gamma=2.0)(y_pred_2, y_true)
        loss_1 = nn.CrossEntropyLoss(reduction='none')(y_pred_1, y_true)
        loss_2 = nn.CrossEntropyLoss(reduction='none')(y_pred_2, y_true)
        loss = loss_1 * (1 - mask) + loss_2 * mask
        return loss.mean()

"""### Main Processing"""

def set_seed(seed=GCF.SEED):
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def train_loop(model, train_loader, optimizer, scheduler, epoch, weights, bar=None):
    n_step = 0
    losses = []
    predicts = []
    targets = []
    lrs = []
    model.train()
    if epoch == 3:
        for param in model.transformer_model.parameters():
            param.requires_grad = True
    optimizer.zero_grad()
    for d in train_loader:    
        with torch.cuda.amp.autocast(): 
            logits, loss = model(
                d['X_cate'].to(device),
                d['input_ids'].to(device),
                d['attention_mask'].to(device),
                d['token_type_ids'].to(device),
                y=d['fav_novel_cnt_bin'].to(device),
                #weights=weights.to(device),
            )
            loss = loss / GCF.ACCUMULATE

        predicts.append(logits.detach())
        targets.append(d['fav_novel_cnt_bin'].numpy())
        losses.append(loss.item()*GCF.ACCUMULATE)
        lr = np.array([param_group["lr"] for param_group in optimizer.param_groups]).mean()
        lrs.append(lr)

        scaler.scale(loss).backward()

        if n_step % GCF.ACCUMULATE == 0:
            scaler.step(optimizer) 
            scaler.update() 
            optimizer.zero_grad()
            scheduler.step()   
        n_step += 1
        if bar is not None:
            bar.update(1)
    loss = np.array(losses).mean()
    predict = torch.vstack(predicts).cpu().softmax(1).numpy()
    targert = np.array([np.eye(5)[i] for i in np.hstack(targets)]).astype(int)
    score = log_loss(targert, predict)
    return loss, score, lrs

def valid_loop(model, valid_loader):
    losses = []
    predicts = []
    model.eval()
    for d in valid_loader:    
        with torch.no_grad():
            logits, loss = model(
                d['X_cate'].to(device),
                d['input_ids'].to(device),
                d['attention_mask'].to(device),
                d['token_type_ids'].to(device),
                y=d['fav_novel_cnt_bin'].to(device),
            )

        predicts.append(logits)
        losses.append(loss.item())
    predict = torch.vstack(predicts).cpu().softmax(1).numpy()
    loss = np.array(losses).mean()
    return loss, predict

def test_loop(model, test_loader):
    predicts = []
    model.eval()
    for d in test_loader:    
        with torch.no_grad():
            logits, _ = model(
                d['X_cate'].to(device),
                d['input_ids'].to(device),
                d['attention_mask'].to(device),
                d['token_type_ids'].to(device),
            )
        predicts.append(logits)
    predict = torch.vstack(predicts).cpu().softmax(1).numpy()
    return predict

def norm_url(text,):
    url_pattern = r"https?://[\w/:%#\$&\?\(\)~\.=\+\-… ]+"
    return re.sub(url_pattern, 'URL', text)

X_cate = pd.concat([pd.read_feather(f"{GCF.FEATURES_PATH}/train_{i}.ftr") for i in GCF.FEATURES], axis=1)
X_cate_test = pd.concat([pd.read_feather(f"{GCF.FEATURES_PATH}/test_{i}.ftr") for i in GCF.FEATURES], axis=1)

# year
X_cate['upload_year_norm'] = X_cate['upload_year'].map(lambda x: (x-2007)/14)
X_cate_test['upload_year_norm'] = X_cate_test['upload_year'].map(lambda x: (x-2007)/14)
# monthy
X_cate['month_sin'] = np.sin(2 * np.pi * X_cate['upload_month']/12)
X_cate['month_cos'] = np.cos(2 * np.pi * X_cate['upload_month']/12)
X_cate_test['month_sin'] = np.sin(2 * np.pi * X_cate_test['upload_month']/12)
X_cate_test['month_cos'] = np.cos(2 * np.pi * X_cate_test['upload_month']/12)
# hour
X_cate['hour_sin'] = np.sin(2 * np.pi * X_cate['hour']/24)
X_cate['hour_cos'] = np.cos(2 * np.pi * X_cate['hour']/24)
X_cate_test['hour_sin'] = np.sin(2 * np.pi * X_cate_test['hour']/24)
X_cate_test['hour_cos'] = np.cos(2 * np.pi * X_cate_test['hour']/24)
# day_of_week
X_cate['day_of_week_sin'] = np.sin(2 * np.pi * X_cate['day_of_week']/7)
X_cate['day_of_week_cos'] = np.cos(2 * np.pi * X_cate['day_of_week']/7)
X_cate_test['day_of_week_sin'] = np.sin(2 * np.pi * X_cate_test['day_of_week']/7)
X_cate_test['day_of_week_cos'] = np.cos(2 * np.pi * X_cate_test['day_of_week']/7)

# カテゴリ変数
# userid
userid_over5 = X_cate['userid_over5']
userid_over5_test = X_cate_test['userid_over5']
n_userid = len(set(userid_over5.tolist() + userid_over5_test.tolist()))
# カテゴリ数の辞書
n_cate_dic = {
    'n_userid': n_userid,
}

# 不要カラム削除
with open(f"{GCF.FEATURES_PATH}/unuse_feat_v1.pkl", "rb") as f:
    unuseful_feat_v1 = pickle.load(f)
drop_features = ['upload_year', 'upload_month', 'userid_over5', 'hour', 'day_of_week'] + ['target_1', 'target_2', 'target_3'] #+ unuseful_feat_v1
#drop_features += [f"prev_over_{(i+1)*30}" for i in range(5, 12)] + [f"next_over_{(i+1)*30}" for i in range(5, 12)] + ['init_diff]
X_cate = X_cate.drop(drop_features, axis=1)
X_cate_test = X_cate_test.drop(drop_features, axis=1)
n_features = X_cate.shape[1]

X_cate['userid_over5'] = userid_over5
X_cate_test['userid_over5'] = userid_over5_test

# titleにgenreのタグを追加
genre_tag_dic = {g: f'[GENRE_{g:04}]' for g in train_df['genre'].unique()}
GCF.TOKENIZER.add_tokens(list(genre_tag_dic.values()))
print('add token:', GCF.TOKENIZER.added_tokens_encoder)
train_df['title_genre_tag'] = train_df.apply(lambda x: genre_tag_dic[x['genre']] + x['title'], axis=1)
test_df['title_genre_tag'] = test_df.apply(lambda x: genre_tag_dic[x['genre']] + x['title'], axis=1)

# テキストの正規化
train_df['story_norm'] = train_df['story'].map(norm_url)
test_df['story_norm'] = test_df['story'].map(norm_url)

fav_novel_cnt_bin = train_df['fav_novel_cnt_bin'].values

test_dset = NishikaMultiDataset(X_cate_test, test_df)
test_loader = DataLoader(test_dset, batch_size=GCF.BS,
                           pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())

os.makedirs(f'{GCF.MODELS_PATH}', exist_ok=True)
predicts = []
results = []
oof = np.zeros((len(train_df), 5))
skf = StratifiedKFold(n_splits=GCF.N_FOLDS, random_state=GCF.SEED, shuffle=True).split(train_df['ncode'].values, fav_novel_cnt_bin)
for fold, (train_index, valid_index) in enumerate(skf):
    train_dset = NishikaMultiDataset(X_cate.loc[train_index], train_df.loc[train_index])
    valid_dset = NishikaMultiDataset(X_cate.loc[valid_index], train_df.loc[valid_index])

    train_loader = DataLoader(train_dset, batch_size=GCF.BS,
                               pin_memory=True, shuffle=True, drop_last=True, num_workers=os.cpu_count(),
                               worker_init_fn=lambda x: set_seed())
    valid_loader = DataLoader(valid_dset, batch_size=GCF.BS,
                               pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())

    model = NishikaMultiModel(n_features, n_cate_dic)
    model.to(device)
    
    optimizer = AdamW(model.parameters(), lr=GCF.LR, weight_decay=GCF.WEIGHT_DECAY)

    max_train_steps = GCF.N_EPOCHS * len(train_loader) // GCF.ACCUMULATE
    warmup_steps = int(max_train_steps * GCF.WARM_UP_RATIO)
    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=max_train_steps
    )

    bar = tqdm(total=int(GCF.N_EPOCHS * len(train_loader)))
    bar.set_description(f'{GCF.EXP_NAME} Fold-{fold}')

    valid_targert = np.array([np.eye(5)[i] for i in valid_dset.fav_novel_cnt_bin]).astype(int)

    #weights = (1/train_df.loc[train_index, 'fav_novel_cnt_bin'].value_counts()).values
    #weights /= weights.min()
    #weights = train_df.loc[train_index, 'fav_novel_cnt_bin'].value_counts().values
    #weights = weights-weights.min()+1
    #weights = weights[::-1].astype(float)
    #weights = torch.tensor(weights).float().to(device)
    #weights = torch.tensor([1, 1, 1, 2, 4]).float().to(device)
    weights = None

    early_stop = 0
    valid_best = float('inf')
    set_seed()
    for epoch in range(GCF.N_EPOCHS):
        train_loss, train_score, lrs = train_loop(model, train_loader, optimizer, scheduler, epoch, weights, bar)
        valid_loss, valid_predict = valid_loop(model, valid_loader)
        valid_score = log_loss(valid_targert, valid_predict)

        print(f'epoch {epoch}, train_loss={train_loss}, valid_loss={valid_loss}, train_score={train_score}, valid_score={valid_score}')

        if valid_best > valid_score:
            oof[valid_index, :] = valid_predict
            valid_best = valid_score
            print('    -> best score update!!')
            torch.save(model.state_dict(), f'{GCF.MODELS_PATH}/m_f{fold}.bin')
            early_stop = 0
        else:
            early_stop += 1

        if early_stop > GCF.PATIENT:
            print("### EARLY STOP ###")
            break
        results.append({
            'fold': fold,
            'train_loss': train_loss,
            'valid_loss': valid_loss,
            'train_score': train_score,
            'valid_score': valid_score,
        })
        pd.DataFrame(results).to_csv(f'{GCF.MODELS_PATH}/result.csv', index=None)

    model.load_state_dict(torch.load(f'{GCF.MODELS_PATH}/m_f{fold}.bin'))
    test_preds = test_loop(model, test_loader)
    predicts.append(test_preds)

    del model, optimizer, scheduler, valid_targert, bar
    torch.cuda.empty_cache()

oof_score = log_loss(np.stack([np.eye(5)[i] for i in fav_novel_cnt_bin]).astype(int), oof)
print(f"OOF score = {oof_score}")

predicts_avg = np.array(predicts).mean(0)
sub_df[["proba_0","proba_1","proba_2","proba_3","proba_4"]] = predicts_avg
sub_df.to_csv(f"{GCF.RESULT_PATH}/{GCF.EXP_NAME}.csv", index=None)

"""|exp|CV|LB|memo|
|--|--|--|--|
|077|0.7537|0.6930|ジャンルの累計特徴＋自動投稿か否か|
|080|0.7180|0.6742|target encoding|
|||||
|081|0.7186|0.6675|リークしてそうなshiftを削除|
|082|0.7141|0.6658|token_type_idを追加, genre target v2|
|083|0.7100||batch norm, fold-0まで|
|084|0.7252||LDA target, fold-2まで|
|085|0.7174|0.6644|bin feat target|
|086|X||user target feat除外, 良くない|
|087|0.7912||weight loss, fold-0のみ|
|088|0.7468|0.6858|LGBM|
|089|0.7827||weight loss, min, fold-0のみ|
|090|X||weight loss, count, 良くない|
|091|X||label_smoothing=0.1|
|092|0.7158|0.6635|mlp 1024|
|093|0.7373|0.6885|catboost|
|094|0.7165|0.6871|窓特徴|
|095|X||initを修正, そんなに変わらない|
|096|0.7145||lr=1e-3|
|097|0.7188||weight|
|098|0.7162|0.6649|seed=42|
|099|0.7143|0.6627|mlp2|
|100|0.7161|0.6642|BERTのheadを2層|
|101|0.7132||mlp3層, fold-2まで|
|102|X||特徴量追加|
|103|X||Focal gamma=2|
|104|0.71589|0.6689|target encode pseudo|
|105|0.7174||lr=1e-4, fold-0|
|106||||
|||||
||||CatBoostでimportanceが0の特徴量を削除|

### 後段用ベクトル抽出
"""

class NishikaMultiModel_vec(nn.Module):
    
    def __init__(self, n_features, n_cate_dic):
        super(NishikaMultiModel_vec, self).__init__()
        # BERT
        self.config = AutoConfig.from_pretrained(GCF.MODEL_NAME)
        self.config.attention_probs_dropout_prob = GCF.DROPOUT_RATIO
        self.config.hidden_dropout_prob = GCF.DROPOUT_RATIO
        self.config.output_hidden_states = True
        self.transformer_model = AutoModel.from_pretrained(
            GCF.MODEL_NAME, 
            config=self.config,
        )
        self.transformer_head = nn.Sequential(
            nn.Linear(self.config.hidden_size*GCF.N_USE_LAYER, self.config.hidden_size*GCF.N_USE_LAYER),
        )
        
        # MLP
        self.userid_emb = nn.Embedding(n_cate_dic['n_userid'], GCF.N_EMBED, padding_idx=0)
        self.mlp1 = nn.Sequential(
            nn.Linear(n_features+GCF.N_EMBED, GCF.MLP_HIDDEN),
            nn.Dropout(GCF.DROPOUT_RATIO),
            nn.ReLU(),
        )
        self.mlp2 = nn.Sequential(
            nn.Linear(GCF.MLP_HIDDEN, GCF.MLP_HIDDEN),
            nn.Dropout(GCF.DROPOUT_RATIO),
            nn.ReLU(),
        )

        # head
        self.classifier_1 = nn.Linear(self.config.hidden_size*GCF.N_USE_LAYER+GCF.MLP_HIDDEN, 5)
        self.classifier_2 = nn.Linear(self.config.hidden_size*GCF.N_USE_LAYER+GCF.MLP_HIDDEN, 5)
        self.dropouts_1 = nn.ModuleList([nn.Dropout(GCF.DROPOUT_RATIO) for _ in range(GCF.N_MSD)])
        self.dropouts_2 = nn.ModuleList([nn.Dropout(GCF.DROPOUT_RATIO) for _ in range(GCF.N_MSD)])
        self.transformer_model.resize_token_embeddings(len(GCF.TOKENIZER))


    def forward(self, X_cate, input_ids, attention_mask, token_type_ids):
        # BERT
        outputs = self.transformer_model(input_ids, attention_mask, token_type_ids)
        sequence_output = torch.cat([outputs["hidden_states"][-1*i].mean(1) for i in range(1, GCF.N_USE_LAYER+1)], dim=1)
        sequence_output = self.transformer_head(sequence_output)
        # MLP
        e = self.userid_emb(X_cate[:, -1].long())
        h = torch.cat([X_cate[:, :-1], e], dim=1)
        h = self.mlp1(h)
        h = self.mlp2(h)
        # head
        cat_h = torch.cat([sequence_output, h], dim=1)
        return cat_h

test_preds = []
vecs = np.zeros((len(train_df), 1792))
skf = StratifiedKFold(n_splits=GCF.N_FOLDS, random_state=GCF.SEED, shuffle=True).split(train_df['ncode'].values, fav_novel_cnt_bin)
for fold, (_, valid_index) in enumerate(skf):
    valid_dset = NishikaMultiDataset(X_cate.loc[valid_index], train_df.loc[valid_index])
    valid_loader = DataLoader(valid_dset, batch_size=GCF.BS,
                               pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())

    model = NishikaMultiModel_vec(n_features, n_cate_dic)
    model.to(device)
    model.load_state_dict(torch.load(f'{GCF.MODELS_PATH}/m_f{fold}.bin'))
    model.eval()

    predicts = []
    for d in valid_loader:    
        with torch.no_grad():
            cat_h = model(
                d['X_cate'].to(device),
                d['input_ids'].to(device),
                d['attention_mask'].to(device),
                d['token_type_ids'].to(device),
            )

        predicts.append(cat_h)
    vec = torch.vstack(predicts).cpu().numpy()
    vecs[valid_index, :] = vec

    predicts = []
    for d in test_loader:    
        with torch.no_grad():
            cat_h = model(
                d['X_cate'].to(device),
                d['input_ids'].to(device),
                d['attention_mask'].to(device),
                d['token_type_ids'].to(device),
            )
        predicts.append(cat_h)
    test_pred = torch.vstack(predicts).cpu().numpy()
    test_preds.append(test_pred)

np.save(f"{GCF.FEATURES_PATH}/nn_output_exp092_train", vecs)
np.save(f"{GCF.FEATURES_PATH}/nn_output_exp092_test", np.stack(test_preds).mean(0))

