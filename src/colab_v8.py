# -*- coding: utf-8 -*-
"""なろう_NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ejGshCAaZRqdj_ZW_7dWAcaH7j1GUupU

# 小説家になろう ブクマ数予測 \~”伸びる”タイトルとは？\~

## Memo
- pseudo labeling
- ncodeを特徴量として利用
- binary特徴をタグとして入れる
- Adversal validation
- ほかのターゲットを使う（例えばジャンル）
- medianを/5
- ROBerta, GPT-2, char bert
- autoencoderとか凝ったことやめる


### not work
- [Focal Loss](https://github.com/clcarwin/focal_loss_pytorch/blob/master/focalloss.py)
"""

!nvidia-smi

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers unidic-lite fugashi ipadic python-Levenshtein sentencepiece
!pip install -U torch

"""## 共通設定"""

import gc
import os
import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle

from datetime import datetime as dt
from tqdm.notebook import tqdm
from collections import defaultdict
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
from sklearn.preprocessing import StandardScaler, RobustScaler
import itertools
import Levenshtein

from gensim.corpora.dictionary import Dictionary
from gensim.models import LdaModel, TfidfModel, CoherenceModel
from collections import defaultdict

import re

# Transformer
import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader

from transformers import get_cosine_schedule_with_warmup
from transformers import AutoConfig
from transformers import AutoTokenizer
from transformers import AutoModel
from transformers import AdamW
from transformers import T5Tokenizer

device = torch.device("cuda")
scaler = torch.cuda.amp.GradScaler()

class GCF:
    EXP_NAME = 'exp151_user_cls'

    INPUT_PATH = "/content/drive/MyDrive/Study/Nishika"
    FEATURES_PATH = f"{INPUT_PATH}/features"
    RESULT_PATH = f"{INPUT_PATH}/result"
    MODELS_PATH = f"{INPUT_PATH}/models/{EXP_NAME}"

    N_FOLDS = 5
    SEED = 0

    FEATURES = [
        "pc_or_k",
        "org_bin",
        "genre_ohe", "biggenre_ohe",
        "kw_ohe_50_norm",
        "yyyymm",
        "date2",
        "userid_over5",
        "userid_only_test",
        "url_count_3",
        "kikaku_v1",
        "autopost",
        "cumsum_v1",
        "user_target_v1",
        "genre_target_v2",
        "bin_target",
        "date_target_v1",
        "kw_none",
        "story_feat_v1",
        #"user_target_v2_leak_fix",
        #"pca_kw_date",
        #"kmeans20",
        #"kmeans_bert20",
        #"onehot_target_v1",
        "kmeans5",
        "kmeans_bert3",
        "onehot_target_v2",
        "user_target_v3",
    ]

    MODEL_NAME = "cl-tohoku/bert-base-japanese-whole-word-masking"
    TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME)
    #MODEL_NAME = "rinna/japanese-roberta-base"
    #TOKENIZER = T5Tokenizer.from_pretrained(MODEL_NAME)
    MAX_LEN = 256

    BS = 16
    N_USE_LAYER = 1
    LR = 3e-4
    WEIGHT_DECAY = 1e-3
    N_EPOCHS = 4
    ACCUMULATE = 2
    WARM_UP_RATIO = 0.1
    MLP_HIDDEN = 1024
    PATIENT = 8
    N_MSD = 8
    N_EMBED = 128
    AE_HIDDEN = 256
    DROPOUT_RATIO = 0.2

train_df = pd.read_csv(f"{GCF.INPUT_PATH}/train.csv")
test_df = pd.read_csv(f"{GCF.INPUT_PATH}/test.csv")
sub_df = pd.read_csv(f"{GCF.INPUT_PATH}/sample_submission.csv")

test_df['fav_novel_cnt_bin'] = -1

"""## ニューラルネット
- マルチモーダル

### Multimodal Model & Dataset
"""

class NishikaMultiDataset(Dataset):
    def __init__(self, X_cate, df):
        self.X_cate = X_cate
        self.title = df['title_genre_tag'].tolist()
        self.story = df["story"].tolist()
        self.fav_novel_cnt_bin = df["fav_novel_cnt_bin"].tolist()

    def __len__(self):
        return len(self.X_cate)
    
    def __getitem__(self, item):
        X_cate = self.X_cate.iloc[item]
        fav_novel_cnt_bin = self.fav_novel_cnt_bin[item]
        title = self.title[item]
        story = self.story[item]
        fav_novel_cnt_bin = self.fav_novel_cnt_bin[item]

        tok = GCF.TOKENIZER.encode_plus(
            title,
            story,
            truncation='only_second',
            max_length=GCF.MAX_LEN,
            padding='max_length'
        )

        d = {
            "X_cate": torch.tensor(X_cate, dtype=torch.float),
            "input_ids": torch.tensor(tok['input_ids'], dtype=torch.long),
            "attention_mask": torch.tensor(tok['attention_mask'], dtype=torch.long),
            "token_type_ids": torch.tensor(tok['token_type_ids'], dtype=torch.long),
            "fav_novel_cnt_bin": torch.tensor(fav_novel_cnt_bin, dtype=torch.long),
        }
        return d

class NishikaMultiModel(nn.Module):
    
    def __init__(self, n_features, n_cate_dic):
        super(NishikaMultiModel, self).__init__()
        # BERT
        self.config = AutoConfig.from_pretrained(GCF.MODEL_NAME)
        self.config.attention_probs_dropout_prob = GCF.DROPOUT_RATIO
        self.config.hidden_dropout_prob = GCF.DROPOUT_RATIO
        self.config.output_hidden_states = True
        self.transformer_model = AutoModel.from_pretrained(
            GCF.MODEL_NAME, 
            config=self.config,
        )
        self.transformer_head = nn.Sequential(
            nn.Linear(self.config.hidden_size*GCF.N_USE_LAYER, self.config.hidden_size*GCF.N_USE_LAYER),
        )
        
        # MLP
        self.userid_emb_1 = nn.Embedding(n_cate_dic['n_userid_1'], GCF.N_EMBED, padding_idx=0)
        self.userid_emb_2 = nn.Embedding(n_cate_dic['n_userid_2'], GCF.N_EMBED, padding_idx=0)
        self.ae1 = nn.Linear(n_features, GCF.AE_HIDDEN)
        self.ae2 = nn.Linear(GCF.AE_HIDDEN, n_features)

        self.mlp1 = nn.Sequential(
            nn.Linear(GCF.AE_HIDDEN+GCF.N_EMBED*2, GCF.MLP_HIDDEN),
            #nn.Linear(n_features+GCF.N_EMBED*2, GCF.MLP_HIDDEN),
            nn.Dropout(GCF.DROPOUT_RATIO),
            nn.ReLU(),
        )
        self.mlp2 = nn.Sequential(
            nn.Linear(GCF.MLP_HIDDEN, GCF.MLP_HIDDEN),
            nn.Dropout(GCF.DROPOUT_RATIO),
            nn.ReLU(),
        )

        # head
        self.head_mlp = nn.Linear(self.config.hidden_size*GCF.N_USE_LAYER+GCF.MLP_HIDDEN, self.config.hidden_size*GCF.N_USE_LAYER+GCF.MLP_HIDDEN)
        self.classifier_1 = nn.Linear(self.config.hidden_size*GCF.N_USE_LAYER+GCF.MLP_HIDDEN, 5)
        self.classifier_2 = nn.Linear(self.config.hidden_size*GCF.N_USE_LAYER+GCF.MLP_HIDDEN, 5)
        self.dropouts_1 = nn.ModuleList([nn.Dropout(GCF.DROPOUT_RATIO) for _ in range(GCF.N_MSD)])
        self.dropouts_2 = nn.ModuleList([nn.Dropout(GCF.DROPOUT_RATIO) for _ in range(GCF.N_MSD)])

        self.transformer_model.resize_token_embeddings(len(GCF.TOKENIZER))
        self._init_weights(self.userid_emb_1)
        self._init_weights(self.userid_emb_2)
        self._init_weights(self.ae1)
        self._init_weights(self.ae2)
        self._init_weights(self.mlp1[0])
        self._init_weights(self.mlp2[0])
        self._init_weights(self.transformer_head[0])
        self._init_weights(self.head_mlp)
        self._init_weights(self.classifier_1)
        self._init_weights(self.classifier_2)

        # transformer freeze
        for param in self.transformer_model.parameters():
            param.requires_grad = False

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)

    def forward(self, X_cate, input_ids, attention_mask, token_type_ids=None, y=None):
        # BERT
        outputs = self.transformer_model(input_ids, attention_mask, token_type_ids)
        #sequence_output = torch.cat([outputs["hidden_states"][-1*i].mean(1) for i in range(1, GCF.N_USE_LAYER+1)], dim=1)
        sequence_output = torch.cat([outputs["hidden_states"][-1*i][:,0] for i in range(1, GCF.N_USE_LAYER+1)], dim=1)
        sequence_output = self.transformer_head(sequence_output)
        # MLP
        e1 = self.userid_emb_1(X_cate[:, -1].long())
        e2 = self.userid_emb_2(X_cate[:, -2].long())
        ae1 = self.ae1(X_cate[:, :-2])
        ae2 = self.ae2(ae1)
        h = torch.cat([ae1, e1, e2], dim=1)
        #h = torch.cat([X_cate[:, :-2], e1, e2], dim=1)

        h = self.mlp1(h)
        h = self.mlp2(h)
        # head
        cat_h = torch.cat([sequence_output, h], dim=1)
        cat_h = self.head_mlp(cat_h)
        logits_1 = sum([self.classifier_1(dropout(cat_h)) for dropout in self.dropouts_1]) / GCF.N_MSD
        logits_2 = sum([self.classifier_2(dropout(cat_h)) for dropout in self.dropouts_2]) / GCF.N_MSD

        mask = X_cate[:, 8]
        logits =  logits_1, logits_2, mask
        if y is not None:
            loss = self.loss_fn(logits, y)
            ae_loss = torch.nn.MSELoss()(ae2, X_cate[:, :-2])
            loss += ae_loss
        else:
            loss = None
        dual_logits = logits_1 * (1 - mask).unsqueeze(1) + logits_2 * mask.unsqueeze(1)

        return dual_logits, loss

    def loss_fn(self, y_pred, y_true):
        y_pred_1, y_pred_2, mask = y_pred
        loss_1 = nn.CrossEntropyLoss(reduction='none')(y_pred_1, y_true)
        loss_2 = nn.CrossEntropyLoss(reduction='none')(y_pred_2, y_true)
        loss = loss_1 * (1 - mask) + loss_2 * mask
        return loss.mean()

"""### Main Processing"""

def set_seed(seed=GCF.SEED):
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def norm_url(text,):
    url_pattern = r"https?://[\w/:%#\$&\?\(\)~\.=\+\-… ]+"
    return re.sub(url_pattern, 'URL', text)

def train_loop(model, train_loader, optimizer, scheduler, epoch, bar=None):
    n_step = 0
    losses = []
    predicts = []
    targets = []
    lrs = []
    model.train()
    if epoch == 3:
        for param in model.transformer_model.parameters():
            param.requires_grad = True
    optimizer.zero_grad()
    for d in train_loader:    
        with torch.cuda.amp.autocast(): 
            logits, loss = model(
                d['X_cate'].to(device),
                d['input_ids'].to(device),
                d['attention_mask'].to(device),
                d['token_type_ids'].to(device),
                y=d['fav_novel_cnt_bin'].to(device),
            )
            loss = loss / GCF.ACCUMULATE

        predicts.append(logits.detach())
        targets.append(d['fav_novel_cnt_bin'].numpy())
        losses.append(loss.item()*GCF.ACCUMULATE)
        lr = np.array([param_group["lr"] for param_group in optimizer.param_groups]).mean()
        lrs.append(lr)

        scaler.scale(loss).backward()

        if n_step % GCF.ACCUMULATE == 0:
            scaler.step(optimizer) 
            scaler.update() 
            optimizer.zero_grad()
            scheduler.step()   
        n_step += 1
        if bar is not None:
            bar.update(1)
    loss = np.array(losses).mean()
    predict = torch.vstack(predicts).cpu().softmax(1).numpy()
    targert = np.array([np.eye(5)[i] for i in np.hstack(targets)]).astype(int)
    score = log_loss(targert, predict)
    return loss, score, lrs

def valid_loop(model, valid_loader):
    losses = []
    predicts = []
    model.eval()
    for d in valid_loader:    
        with torch.no_grad():
            logits, loss = model(
                d['X_cate'].to(device),
                d['input_ids'].to(device),
                d['attention_mask'].to(device),
                d['token_type_ids'].to(device),
                y=d['fav_novel_cnt_bin'].to(device),
            )

        predicts.append(logits)
        losses.append(loss.item())
    predict = torch.vstack(predicts).cpu().softmax(1).numpy()
    loss = np.array(losses).mean()
    return loss, predict

def test_loop(model, test_loader):
    predicts = []
    model.eval()
    for d in test_loader:    
        with torch.no_grad():
            logits, _ = model(
                d['X_cate'].to(device),
                d['input_ids'].to(device),
                d['attention_mask'].to(device),
                d['token_type_ids'].to(device),
            )
        predicts.append(logits)
    predict = torch.vstack(predicts).cpu().softmax(1).numpy()
    return predict

#os.environ['CUDA_LAUNCH_BLOCKING'] = "1"

X_cate = pd.concat([pd.read_feather(f"{GCF.FEATURES_PATH}/train_{i}.ftr") for i in GCF.FEATURES], axis=1)
X_cate_test = pd.concat([pd.read_feather(f"{GCF.FEATURES_PATH}/test_{i}.ftr") for i in GCF.FEATURES], axis=1)

# year
X_cate['upload_year_norm'] = X_cate['upload_year'].map(lambda x: (x-2007)/14)
X_cate_test['upload_year_norm'] = X_cate_test['upload_year'].map(lambda x: (x-2007)/14)
# monthy
X_cate['month_sin'] = np.sin(2 * np.pi * X_cate['upload_month']/12)
X_cate['month_cos'] = np.cos(2 * np.pi * X_cate['upload_month']/12)
X_cate_test['month_sin'] = np.sin(2 * np.pi * X_cate_test['upload_month']/12)
X_cate_test['month_cos'] = np.cos(2 * np.pi * X_cate_test['upload_month']/12)
# hour
X_cate['hour_sin'] = np.sin(2 * np.pi * X_cate['hour']/24)
X_cate['hour_cos'] = np.cos(2 * np.pi * X_cate['hour']/24)
X_cate_test['hour_sin'] = np.sin(2 * np.pi * X_cate_test['hour']/24)
X_cate_test['hour_cos'] = np.cos(2 * np.pi * X_cate_test['hour']/24)
# day_of_week
X_cate['day_of_week_sin'] = np.sin(2 * np.pi * X_cate['day_of_week']/7)
X_cate['day_of_week_cos'] = np.cos(2 * np.pi * X_cate['day_of_week']/7)
X_cate_test['day_of_week_sin'] = np.sin(2 * np.pi * X_cate_test['day_of_week']/7)
X_cate_test['day_of_week_cos'] = np.cos(2 * np.pi * X_cate_test['day_of_week']/7)

# カテゴリ変数
# userid
userid_columns_1 = 'userid_over5'
userid_columns_2 = 'userid_only_test'

userid_train_1 = X_cate[userid_columns_1]
userid_test_1 = X_cate_test[userid_columns_1]
n_userid_1 = len(set(userid_train_1.tolist() + userid_test_1.tolist()))
userid_train_2 = X_cate[userid_columns_2]
userid_test_2 = X_cate_test[userid_columns_2]
n_userid_2 = len(set(userid_train_2.tolist() + userid_test_2.tolist()))
# カテゴリ数の辞書
n_cate_dic = {
    'n_userid_1': n_userid_1,
    'n_userid_2': n_userid_2,
}

# 不要カラム削除
drop_features = ['upload_year', 'upload_month', 'hour', 'day_of_week', userid_columns_1, userid_columns_2] + ['target_1', 'target_2', 'target_3'] 
X_cate = X_cate.drop(drop_features, axis=1)
X_cate_test = X_cate_test.drop(drop_features, axis=1)
n_features = X_cate.shape[1]

# userid
X_cate[userid_columns_2] = userid_train_2
X_cate_test[userid_columns_2] = userid_test_2
X_cate[userid_columns_1] = userid_train_1
X_cate_test[userid_columns_1] = userid_test_1

# titleにgenreのタグを追加
genre_tag_dic = {g: f'[GENRE_{g:04}]' for g in train_df['genre'].unique()}
GCF.TOKENIZER.add_tokens(list(genre_tag_dic.values()))
print('add token:', GCF.TOKENIZER.added_tokens_encoder)
train_df['title_genre_tag'] = train_df.apply(lambda x: genre_tag_dic[x['genre']] + x['title'], axis=1)
test_df['title_genre_tag'] = test_df.apply(lambda x: genre_tag_dic[x['genre']] + x['title'], axis=1)

# テキストの正規化
train_df['story_norm'] = train_df['story'].map(norm_url)
test_df['story_norm'] = test_df['story'].map(norm_url)

fav_novel_cnt_bin = train_df['fav_novel_cnt_bin'].values

test_dset = NishikaMultiDataset(X_cate_test, test_df)
test_loader = DataLoader(test_dset, batch_size=GCF.BS,
                           pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())

os.makedirs(f'{GCF.MODELS_PATH}', exist_ok=True)
predicts = []
results = []
oof = np.zeros((len(train_df), 5))
skf = StratifiedKFold(n_splits=GCF.N_FOLDS, random_state=GCF.SEED, shuffle=True).split(train_df['ncode'].values, fav_novel_cnt_bin)
for fold, (train_index, valid_index) in enumerate(skf):
    train_dset = NishikaMultiDataset(X_cate.loc[train_index], train_df.loc[train_index])
    valid_dset = NishikaMultiDataset(X_cate.loc[valid_index], train_df.loc[valid_index])

    train_loader = DataLoader(train_dset, batch_size=GCF.BS,
                               pin_memory=True, shuffle=True, drop_last=True, num_workers=os.cpu_count(),
                               worker_init_fn=lambda x: set_seed())
    valid_loader = DataLoader(valid_dset, batch_size=GCF.BS,
                               pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())

    model = NishikaMultiModel(n_features, n_cate_dic)
    model.to(device)
    
    optimizer = AdamW(model.parameters(), lr=GCF.LR, weight_decay=GCF.WEIGHT_DECAY)

    max_train_steps = GCF.N_EPOCHS * len(train_loader) // GCF.ACCUMULATE
    warmup_steps = int(max_train_steps * GCF.WARM_UP_RATIO)
    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=max_train_steps
    )

    bar = tqdm(total=int(GCF.N_EPOCHS * len(train_loader)))
    bar.set_description(f'{GCF.EXP_NAME} Fold-{fold}')

    valid_targert = np.array([np.eye(5)[i] for i in valid_dset.fav_novel_cnt_bin]).astype(int)

    early_stop = 0
    valid_best = float('inf')
    set_seed()
    for epoch in range(GCF.N_EPOCHS):
        train_loss, train_score, lrs = train_loop(model, train_loader, optimizer, scheduler, epoch, bar)
        valid_loss, valid_predict = valid_loop(model, valid_loader)
        valid_score = log_loss(valid_targert, valid_predict)

        print(f'epoch {epoch}, train_loss={train_loss}, valid_loss={valid_loss}, train_score={train_score}, valid_score={valid_score}')

        if valid_best > valid_score:
            oof[valid_index, :] = valid_predict
            valid_best = valid_score
            print('    -> best score update!!')
            torch.save(model.state_dict(), f'{GCF.MODELS_PATH}/m_f{fold}.bin')
            early_stop = 0
        else:
            early_stop += 1

        if early_stop > GCF.PATIENT:
            print("### EARLY STOP ###")
            break
        results.append({
            'fold': fold,
            'train_loss': train_loss,
            'valid_loss': valid_loss,
            'train_score': train_score,
            'valid_score': valid_score,
        })
        pd.DataFrame(results).to_csv(f'{GCF.MODELS_PATH}/result.csv', index=None)

    model.load_state_dict(torch.load(f'{GCF.MODELS_PATH}/m_f{fold}.bin'))
    test_preds = test_loop(model, test_loader)
    predicts.append(test_preds)

    del model, optimizer, scheduler, valid_targert, bar
    torch.cuda.empty_cache()

oof_score = log_loss(np.stack([np.eye(5)[i] for i in fav_novel_cnt_bin]).astype(int), oof)
print(f"OOF score = {oof_score}")

predicts_avg = np.array(predicts).mean(0)
sub_df[["proba_0","proba_1","proba_2","proba_3","proba_4"]] = predicts_avg
sub_df.to_csv(f"{GCF.RESULT_PATH}/{GCF.EXP_NAME}.csv", index=None)

np.save(f"{GCF.FEATURES_PATH}/nn_output_{GCF.EXP_NAME}_train", oof)
np.save(f"{GCF.FEATURES_PATH}/nn_output_{GCF.EXP_NAME}_test", predicts_avg)

"""|exp|CV|LB|memo|
|--|--|--|--|
|099|0.7143|0.6627|mlp2|
|||||
|106|0.7145|0.6700|shift追加|
|107|0.7239|0.6647|useridをtestのみに|
|108|0.7431||LGBM|
|109|0.7438||CatBoost|
|112|0.7224|0.6648|auto encoderっぽく(113 cls tokenになっちゃってる)|
|114|0.7211|0.6658|cls token|
|115|0.7236|0.6664|2 layer|
|115|0.7544||XGBoost|
|||||
|123|X||key word target|
|124|0.7142|0.7582|storyの長さや改行を特徴量に入れる|
|125|0.7336||LGBM, add bert feat|
|126|0.7445||CatBoost, add bert feat|
|127|0.7233|0.6649|PCAキーワード|
|128|0.7188|0.6961|window feat|
|129|0.7139|0.6649|userid x 2|
|130|||kmeans target|
|131|0.7244||LGBM, kmeans target|
|132|0.7280||CatBoost, kmeans target|
|133|0.6945|0.6844|LGBM, kmeans bert target|
|134|0.7028||CatBoost, kmeans bert target|
|135|0.6666|0.6704|all new feat|
|136|||LGBM, onehot target|
|137|0.6687|0.6930|CatBoost, onehot target|
|138|0.6970||not ohe target|
|139|0.7028||LGBM, leak fix|
|140|0.7282||CatBoost, leak fix|
|141|0.7025|0.6621|leakの修正|
|142|0.7025||not AutoEncoder|
|143|0.7174||rinna RoBERTa|
|144|0.7210||CatBoost, not BERT|
|145|0.7210||CatBoost, not BERT, LR=1e-3|
|146|0.7191||rinna cls token|
|14７|0.7168||CatBoost id 2|
|148|0.7014||LGBM id 2|
|149|0.7147||CatBoost, userid cls target|
|150|0.7045||LGBM, userid cls target|
|151|0.7029|0.6640|BERT, userid cls target|
|||||
||||fix target with pseudo|
"""



